{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Определение стоимости автомобилей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сервис по продаже автомобилей с пробегом «Не бит, не крашен» разрабатывает приложение для привлечения новых клиентов. В нём можно быстро узнать рыночную стоимость своего автомобиля. В вашем распоряжении исторические данные: технические характеристики, комплектации и цены автомобилей. Вам нужно построить модель для определения стоимости. \n",
    "\n",
    "Заказчику важны:\n",
    "\n",
    "- качество предсказания;\n",
    "- скорость предсказания;\n",
    "- время обучения.\n",
    "\n",
    "\n",
    "Предоставленны следующие архивные данные:\n",
    "\n",
    "**Признаки**\n",
    "- DateCrawled — дата скачивания анкеты из базы\n",
    "- VehicleType — тип автомобильного кузова\n",
    "- RegistrationYear — год регистрации автомобиля\n",
    "- Gearbox — тип коробки передач\n",
    "- Power — мощность (л. с.)\n",
    "- Model — модель автомобиля\n",
    "- Kilometer — пробег (км)\n",
    "- RegistrationMonth — месяц регистрации автомобиля\n",
    "- FuelType — тип топлива\n",
    "- Brand — марка автомобиля\n",
    "- Repaired — была машина в ремонте или нет\n",
    "- DateCreated — дата создания анкеты\n",
    "- NumberOfPictures — количество фотографий автомобиля\n",
    "- PostalCode — почтовый индекс владельца анкеты (пользователя)\n",
    "- LastSeen — дата последней активности пользователя\n",
    "\n",
    "**Целевой признак:**\n",
    "Price — цена (евро)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import OrdinalEncoder, StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from time import time\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим и рассмотрим данные:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DateCrawled</th>\n",
       "      <th>Price</th>\n",
       "      <th>VehicleType</th>\n",
       "      <th>RegistrationYear</th>\n",
       "      <th>Gearbox</th>\n",
       "      <th>Power</th>\n",
       "      <th>Model</th>\n",
       "      <th>Kilometer</th>\n",
       "      <th>RegistrationMonth</th>\n",
       "      <th>FuelType</th>\n",
       "      <th>Brand</th>\n",
       "      <th>Repaired</th>\n",
       "      <th>DateCreated</th>\n",
       "      <th>NumberOfPictures</th>\n",
       "      <th>PostalCode</th>\n",
       "      <th>LastSeen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-03-24 11:52:17</td>\n",
       "      <td>480</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1993</td>\n",
       "      <td>manual</td>\n",
       "      <td>0</td>\n",
       "      <td>golf</td>\n",
       "      <td>150000</td>\n",
       "      <td>0</td>\n",
       "      <td>petrol</td>\n",
       "      <td>volkswagen</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-03-24 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>70435</td>\n",
       "      <td>2016-04-07 03:16:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-03-24 10:58:45</td>\n",
       "      <td>18300</td>\n",
       "      <td>coupe</td>\n",
       "      <td>2011</td>\n",
       "      <td>manual</td>\n",
       "      <td>190</td>\n",
       "      <td>NaN</td>\n",
       "      <td>125000</td>\n",
       "      <td>5</td>\n",
       "      <td>gasoline</td>\n",
       "      <td>audi</td>\n",
       "      <td>yes</td>\n",
       "      <td>2016-03-24 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>66954</td>\n",
       "      <td>2016-04-07 01:46:50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-03-14 12:52:21</td>\n",
       "      <td>9800</td>\n",
       "      <td>suv</td>\n",
       "      <td>2004</td>\n",
       "      <td>auto</td>\n",
       "      <td>163</td>\n",
       "      <td>grand</td>\n",
       "      <td>125000</td>\n",
       "      <td>8</td>\n",
       "      <td>gasoline</td>\n",
       "      <td>jeep</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-03-14 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>90480</td>\n",
       "      <td>2016-04-05 12:47:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-03-17 16:54:04</td>\n",
       "      <td>1500</td>\n",
       "      <td>small</td>\n",
       "      <td>2001</td>\n",
       "      <td>manual</td>\n",
       "      <td>75</td>\n",
       "      <td>golf</td>\n",
       "      <td>150000</td>\n",
       "      <td>6</td>\n",
       "      <td>petrol</td>\n",
       "      <td>volkswagen</td>\n",
       "      <td>no</td>\n",
       "      <td>2016-03-17 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>91074</td>\n",
       "      <td>2016-03-17 17:40:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-03-31 17:25:20</td>\n",
       "      <td>3600</td>\n",
       "      <td>small</td>\n",
       "      <td>2008</td>\n",
       "      <td>manual</td>\n",
       "      <td>69</td>\n",
       "      <td>fabia</td>\n",
       "      <td>90000</td>\n",
       "      <td>7</td>\n",
       "      <td>gasoline</td>\n",
       "      <td>skoda</td>\n",
       "      <td>no</td>\n",
       "      <td>2016-03-31 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>60437</td>\n",
       "      <td>2016-04-06 10:17:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2016-04-04 17:36:23</td>\n",
       "      <td>650</td>\n",
       "      <td>sedan</td>\n",
       "      <td>1995</td>\n",
       "      <td>manual</td>\n",
       "      <td>102</td>\n",
       "      <td>3er</td>\n",
       "      <td>150000</td>\n",
       "      <td>10</td>\n",
       "      <td>petrol</td>\n",
       "      <td>bmw</td>\n",
       "      <td>yes</td>\n",
       "      <td>2016-04-04 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>33775</td>\n",
       "      <td>2016-04-06 19:17:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2016-04-01 20:48:51</td>\n",
       "      <td>2200</td>\n",
       "      <td>convertible</td>\n",
       "      <td>2004</td>\n",
       "      <td>manual</td>\n",
       "      <td>109</td>\n",
       "      <td>2_reihe</td>\n",
       "      <td>150000</td>\n",
       "      <td>8</td>\n",
       "      <td>petrol</td>\n",
       "      <td>peugeot</td>\n",
       "      <td>no</td>\n",
       "      <td>2016-04-01 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>67112</td>\n",
       "      <td>2016-04-05 18:18:39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2016-03-21 18:54:38</td>\n",
       "      <td>0</td>\n",
       "      <td>sedan</td>\n",
       "      <td>1980</td>\n",
       "      <td>manual</td>\n",
       "      <td>50</td>\n",
       "      <td>other</td>\n",
       "      <td>40000</td>\n",
       "      <td>7</td>\n",
       "      <td>petrol</td>\n",
       "      <td>volkswagen</td>\n",
       "      <td>no</td>\n",
       "      <td>2016-03-21 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>19348</td>\n",
       "      <td>2016-03-25 16:47:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2016-04-04 23:42:13</td>\n",
       "      <td>14500</td>\n",
       "      <td>bus</td>\n",
       "      <td>2014</td>\n",
       "      <td>manual</td>\n",
       "      <td>125</td>\n",
       "      <td>c_max</td>\n",
       "      <td>30000</td>\n",
       "      <td>8</td>\n",
       "      <td>petrol</td>\n",
       "      <td>ford</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-04-04 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>94505</td>\n",
       "      <td>2016-04-04 23:42:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2016-03-17 10:53:50</td>\n",
       "      <td>999</td>\n",
       "      <td>small</td>\n",
       "      <td>1998</td>\n",
       "      <td>manual</td>\n",
       "      <td>101</td>\n",
       "      <td>golf</td>\n",
       "      <td>150000</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>volkswagen</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-03-17 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>27472</td>\n",
       "      <td>2016-03-31 17:17:06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           DateCrawled  Price  VehicleType  RegistrationYear Gearbox  Power  \\\n",
       "0  2016-03-24 11:52:17    480          NaN              1993  manual      0   \n",
       "1  2016-03-24 10:58:45  18300        coupe              2011  manual    190   \n",
       "2  2016-03-14 12:52:21   9800          suv              2004    auto    163   \n",
       "3  2016-03-17 16:54:04   1500        small              2001  manual     75   \n",
       "4  2016-03-31 17:25:20   3600        small              2008  manual     69   \n",
       "5  2016-04-04 17:36:23    650        sedan              1995  manual    102   \n",
       "6  2016-04-01 20:48:51   2200  convertible              2004  manual    109   \n",
       "7  2016-03-21 18:54:38      0        sedan              1980  manual     50   \n",
       "8  2016-04-04 23:42:13  14500          bus              2014  manual    125   \n",
       "9  2016-03-17 10:53:50    999        small              1998  manual    101   \n",
       "\n",
       "     Model  Kilometer  RegistrationMonth  FuelType       Brand Repaired  \\\n",
       "0     golf     150000                  0    petrol  volkswagen      NaN   \n",
       "1      NaN     125000                  5  gasoline        audi      yes   \n",
       "2    grand     125000                  8  gasoline        jeep      NaN   \n",
       "3     golf     150000                  6    petrol  volkswagen       no   \n",
       "4    fabia      90000                  7  gasoline       skoda       no   \n",
       "5      3er     150000                 10    petrol         bmw      yes   \n",
       "6  2_reihe     150000                  8    petrol     peugeot       no   \n",
       "7    other      40000                  7    petrol  volkswagen       no   \n",
       "8    c_max      30000                  8    petrol        ford      NaN   \n",
       "9     golf     150000                  0       NaN  volkswagen      NaN   \n",
       "\n",
       "           DateCreated  NumberOfPictures  PostalCode             LastSeen  \n",
       "0  2016-03-24 00:00:00                 0       70435  2016-04-07 03:16:57  \n",
       "1  2016-03-24 00:00:00                 0       66954  2016-04-07 01:46:50  \n",
       "2  2016-03-14 00:00:00                 0       90480  2016-04-05 12:47:46  \n",
       "3  2016-03-17 00:00:00                 0       91074  2016-03-17 17:40:17  \n",
       "4  2016-03-31 00:00:00                 0       60437  2016-04-06 10:17:21  \n",
       "5  2016-04-04 00:00:00                 0       33775  2016-04-06 19:17:07  \n",
       "6  2016-04-01 00:00:00                 0       67112  2016-04-05 18:18:39  \n",
       "7  2016-03-21 00:00:00                 0       19348  2016-03-25 16:47:58  \n",
       "8  2016-04-04 00:00:00                 0       94505  2016-04-04 23:42:13  \n",
       "9  2016-03-17 00:00:00                 0       27472  2016-03-31 17:17:06  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 354369 entries, 0 to 354368\n",
      "Data columns (total 16 columns):\n",
      " #   Column             Non-Null Count   Dtype \n",
      "---  ------             --------------   ----- \n",
      " 0   DateCrawled        354369 non-null  object\n",
      " 1   Price              354369 non-null  int64 \n",
      " 2   VehicleType        316879 non-null  object\n",
      " 3   RegistrationYear   354369 non-null  int64 \n",
      " 4   Gearbox            334536 non-null  object\n",
      " 5   Power              354369 non-null  int64 \n",
      " 6   Model              334664 non-null  object\n",
      " 7   Kilometer          354369 non-null  int64 \n",
      " 8   RegistrationMonth  354369 non-null  int64 \n",
      " 9   FuelType           321474 non-null  object\n",
      " 10  Brand              354369 non-null  object\n",
      " 11  Repaired           283215 non-null  object\n",
      " 12  DateCreated        354369 non-null  object\n",
      " 13  NumberOfPictures   354369 non-null  int64 \n",
      " 14  PostalCode         354369 non-null  int64 \n",
      " 15  LastSeen           354369 non-null  object\n",
      "dtypes: int64(7), object(9)\n",
      "memory usage: 43.3+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "try:\n",
    "    df = pd.read_csv('/datasets/autos.csv')\n",
    "except:\n",
    "    df = pd.read_csv(\"C:/Users/chern/autos.csv\")\n",
    "    \n",
    "display(df.head(10))\n",
    "display(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим данные на наличие дубликатов и удалим их при необходимости:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Приведем столбцы Gearbox и Repaired к булевому виду:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         True\n",
       "1         True\n",
       "2         True\n",
       "3         True\n",
       "4         True\n",
       "          ... \n",
       "354364    True\n",
       "354365    True\n",
       "354366    True\n",
       "354367    True\n",
       "354368    True\n",
       "Name: Gearbox, Length: 354365, dtype: bool"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0         True\n",
       "1         True\n",
       "2         True\n",
       "3         True\n",
       "4         True\n",
       "          ... \n",
       "354364    True\n",
       "354365    True\n",
       "354366    True\n",
       "354367    True\n",
       "354368    True\n",
       "Name: Repaired, Length: 354365, dtype: bool"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(df['Gearbox'].astype(bool))\n",
    "df['Repaired'].astype(bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оставим в датафрейме только те столбцы, которые важны, для определения стоимости автомобиля:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub = df[['Price', 'VehicleType', 'RegistrationYear', 'Gearbox', 'Power', 'Model', 'Kilometer', 'FuelType', 'Brand', 'Repaired']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Price</th>\n",
       "      <th>VehicleType</th>\n",
       "      <th>RegistrationYear</th>\n",
       "      <th>Gearbox</th>\n",
       "      <th>Power</th>\n",
       "      <th>Model</th>\n",
       "      <th>Kilometer</th>\n",
       "      <th>FuelType</th>\n",
       "      <th>Brand</th>\n",
       "      <th>Repaired</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>480</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1993</td>\n",
       "      <td>manual</td>\n",
       "      <td>0</td>\n",
       "      <td>golf</td>\n",
       "      <td>150000</td>\n",
       "      <td>petrol</td>\n",
       "      <td>volkswagen</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18300</td>\n",
       "      <td>coupe</td>\n",
       "      <td>2011</td>\n",
       "      <td>manual</td>\n",
       "      <td>190</td>\n",
       "      <td>NaN</td>\n",
       "      <td>125000</td>\n",
       "      <td>gasoline</td>\n",
       "      <td>audi</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9800</td>\n",
       "      <td>suv</td>\n",
       "      <td>2004</td>\n",
       "      <td>auto</td>\n",
       "      <td>163</td>\n",
       "      <td>grand</td>\n",
       "      <td>125000</td>\n",
       "      <td>gasoline</td>\n",
       "      <td>jeep</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1500</td>\n",
       "      <td>small</td>\n",
       "      <td>2001</td>\n",
       "      <td>manual</td>\n",
       "      <td>75</td>\n",
       "      <td>golf</td>\n",
       "      <td>150000</td>\n",
       "      <td>petrol</td>\n",
       "      <td>volkswagen</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3600</td>\n",
       "      <td>small</td>\n",
       "      <td>2008</td>\n",
       "      <td>manual</td>\n",
       "      <td>69</td>\n",
       "      <td>fabia</td>\n",
       "      <td>90000</td>\n",
       "      <td>gasoline</td>\n",
       "      <td>skoda</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Price VehicleType  RegistrationYear Gearbox  Power  Model  Kilometer  \\\n",
       "0    480         NaN              1993  manual      0   golf     150000   \n",
       "1  18300       coupe              2011  manual    190    NaN     125000   \n",
       "2   9800         suv              2004    auto    163  grand     125000   \n",
       "3   1500       small              2001  manual     75   golf     150000   \n",
       "4   3600       small              2008  manual     69  fabia      90000   \n",
       "\n",
       "   FuelType       Brand Repaired  \n",
       "0    petrol  volkswagen      NaN  \n",
       "1  gasoline        audi      yes  \n",
       "2  gasoline        jeep      NaN  \n",
       "3    petrol  volkswagen       no  \n",
       "4  gasoline       skoda       no  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sub = df_sub.reset_index(drop=True)\n",
    "df_sub.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим данные на наличие пропусков:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Price                   0\n",
       "VehicleType         37490\n",
       "RegistrationYear        0\n",
       "Gearbox             19833\n",
       "Power                   0\n",
       "Model               19705\n",
       "Kilometer               0\n",
       "FuelType            32895\n",
       "Brand                   0\n",
       "Repaired            71154\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sub.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим какая доля данных пропущена:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.57948725184485"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "5.596771690206426"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "5.560650741467131"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "9.282801631086592"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "20.07929677027923"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_sub['VehicleType'].isna().sum() / len(df_sub['VehicleType'])*100)\n",
    "display(df_sub['Gearbox'].isna().sum() / len(df_sub['VehicleType'])*100)\n",
    "display(df_sub['Model'].isna().sum() / len(df_sub['Model'])*100)\n",
    "display(df_sub['FuelType'].isna().sum() / len(df_sub['FuelType'])*100)\n",
    "display(df_sub['Repaired'].isna().sum() / len(df_sub['FuelType'])*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пропущена небольшая доля от всех данных по каждому столбцу, кроме столбца Repaired, но тем не менее суммарная доля пропсуков слишком высокая. Используем заглушки в виде новых значений категорий по всем столбцам с пропусками"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Price               0\n",
       "VehicleType         0\n",
       "RegistrationYear    0\n",
       "Gearbox             0\n",
       "Power               0\n",
       "Model               0\n",
       "Kilometer           0\n",
       "FuelType            0\n",
       "Brand               0\n",
       "Repaired            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_to_replace = ['VehicleType', 'Gearbox', 'Model', 'FuelType', 'Repaired']\n",
    "\n",
    "for column in columns_to_replace:\n",
    "    df_sub[column] = df_sub[column].fillna(value='unknown')\n",
    "\n",
    "df_sub.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим численные признаки на наличие аномалий:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x1500 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_sub.hist(figsize=(15, 15));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0,     1,     2, ..., 19998, 19999, 20000], dtype=int64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sub['Price'].sort_values().unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 700x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_sub['Price'].plot(y='age', kind='box', title='Цена автомобилей в евро', figsize=(7, 7)); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим нулевую цену и другие аномально низкие цены, например, 1-2 евро. Удалим автомобили с ценой ниже 15 евро: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub = df_sub.query('Price > 500')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1000, 1001, 1039, 1234, 1400, 1500, 1600, 1800, 1910, 1923, 1925,\n",
       "       1927, 1928, 1929, 1930, 1931, 1932, 1933, 1934, 1935, 1936, 1937,\n",
       "       1938, 1940, 1941, 1942, 1943, 1944, 1945, 1946, 1947, 1948, 1949,\n",
       "       1950, 1951, 1952, 1953, 1954, 1955, 1956, 1957, 1958, 1959, 1960,\n",
       "       1961, 1962, 1963, 1964, 1965, 1966, 1967, 1968, 1969, 1970, 1971,\n",
       "       1972, 1973, 1974, 1975, 1976, 1977, 1978, 1979, 1980, 1981, 1982,\n",
       "       1983, 1984, 1985, 1986, 1987, 1988, 1989, 1990, 1991, 1992, 1993,\n",
       "       1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004,\n",
       "       2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015,\n",
       "       2016, 2017, 2018, 2019, 2066, 2290, 2500, 2800, 2900, 3000, 3200,\n",
       "       3700, 4000, 4100, 4500, 5000, 5300, 5555, 5911, 6000, 7000, 7100,\n",
       "       7800, 8500, 8888, 9000, 9450, 9999], dtype=int64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sub['RegistrationYear'].sort_values().unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 700x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_sub['RegistrationYear'].plot(y='age', kind='box', title='Год регистрации автомобиля', figsize=(7, 7)); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также видим странные значения года. Удалим аномальные значения года и совсем старые автомобили, старше 50 лет:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub = df_sub.query('RegistrationYear >= 1970 & RegistrationYear < 2017')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0,     1,     2,     3,     4,     5,     6,     7,     8,\n",
       "           9,    10,    11,    12,    13,    14,    15,    16,    17,\n",
       "          18,    19,    20,    21,    22,    23,    24,    25,    26,\n",
       "          27,    28,    29,    30,    31,    33,    34,    35,    36,\n",
       "          37,    38,    39,    40,    41,    42,    43,    44,    45,\n",
       "          46,    47,    48,    49,    50,    51,    52,    53,    54,\n",
       "          55,    56,    57,    58,    59,    60,    61,    62,    63,\n",
       "          64,    65,    66,    67,    68,    69,    70,    71,    72,\n",
       "          73,    74,    75,    76,    77,    78,    79,    80,    81,\n",
       "          82,    83,    84,    85,    86,    87,    88,    89,    90,\n",
       "          91,    92,    93,    94,    95,    96,    97,    98,    99,\n",
       "         100,   101,   102,   103,   104,   105,   106,   107,   108,\n",
       "         109,   110,   111,   112,   113,   114,   115,   116,   117,\n",
       "         118,   119,   120,   121,   122,   123,   124,   125,   126,\n",
       "         127,   128,   129,   130,   131,   132,   133,   134,   135,\n",
       "         136,   137,   138,   139,   140,   141,   142,   143,   144,\n",
       "         145,   146,   147,   148,   149,   150,   151,   152,   153,\n",
       "         154,   155,   156,   157,   158,   159,   160,   161,   162,\n",
       "         163,   164,   165,   166,   167,   168,   169,   170,   171,\n",
       "         172,   173,   174,   175,   176,   177,   178,   179,   180,\n",
       "         181,   182,   183,   184,   185,   186,   187,   188,   189,\n",
       "         190,   191,   192,   193,   194,   195,   196,   197,   198,\n",
       "         199,   200,   201,   202,   203,   204,   205,   206,   207,\n",
       "         208,   209,   210,   211,   212,   213,   214,   215,   216,\n",
       "         217,   218,   219,   220,   221,   222,   223,   224,   225,\n",
       "         226,   227,   228,   229,   230,   231,   232,   233,   234,\n",
       "         235,   236,   237,   238,   239,   240,   241,   242,   243,\n",
       "         244,   245,   246,   247,   248,   249,   250,   251,   252,\n",
       "         253,   254,   255,   256,   257,   258,   259,   260,   261,\n",
       "         262,   264,   265,   266,   267,   268,   269,   270,   271,\n",
       "         272,   273,   274,   275,   276,   277,   278,   279,   280,\n",
       "         281,   282,   283,   284,   285,   286,   287,   288,   289,\n",
       "         290,   292,   293,   294,   295,   296,   297,   298,   299,\n",
       "         300,   301,   303,   304,   305,   306,   307,   308,   309,\n",
       "         310,   311,   313,   314,   315,   316,   317,   318,   320,\n",
       "         321,   322,   323,   324,   325,   326,   328,   329,   330,\n",
       "         331,   332,   333,   334,   335,   336,   337,   338,   339,\n",
       "         340,   341,   343,   344,   345,   346,   347,   348,   349,\n",
       "         350,   352,   353,   354,   355,   356,   357,   358,   360,\n",
       "         361,   362,   363,   364,   365,   367,   368,   370,   371,\n",
       "         374,   376,   377,   379,   380,   381,   382,   385,   386,\n",
       "         387,   388,   390,   392,   394,   396,   398,   399,   400,\n",
       "         401,   405,   408,   409,   411,   416,   420,   421,   426,\n",
       "         428,   430,   431,   435,   440,   442,   445,   449,   450,\n",
       "         454,   457,   460,   475,   476,   487,   489,   490,   500,\n",
       "         504,   505,   507,   508,   510,   514,   515,   517,   519,\n",
       "         520,   521,   525,   540,   544,   550,   551,   560,   572,\n",
       "         579,   585,   600,   601,   602,   603,   604,   606,   612,\n",
       "         620,   640,   645,   650,   651,   671,   682,   685,   696,\n",
       "         700,   702,   703,   732,   743,   750,   751,   771,   776,\n",
       "         800,   805,   850,   871,   900,   903,   907,   909,   923,\n",
       "         950,   952,   953,   999,  1000,  1001,  1003,  1005,  1011,\n",
       "        1012,  1016,  1017,  1021,  1024,  1054,  1055,  1056,  1062,\n",
       "        1079,  1082,  1090,  1100,  1105,  1115,  1149,  1151,  1160,\n",
       "        1162,  1164,  1199,  1202,  1221,  1223,  1240,  1250,  1275,\n",
       "        1288,  1299,  1300,  1317,  1339,  1360,  1362,  1363,  1367,\n",
       "        1390,  1398,  1399,  1400,  1401,  1403,  1405,  1416,  1432,\n",
       "        1433,  1436,  1500,  1501,  1502,  1503,  1506,  1521,  1548,\n",
       "        1595,  1596,  1597,  1598,  1600,  1631,  1653,  1659,  1689,\n",
       "        1700,  1701,  1704,  1707,  1753,  1771,  1779,  1780,  1781,\n",
       "        1783,  1793,  1796,  1800,  1801,  1870,  1895,  1896,  1900,\n",
       "        1920,  1922,  1933,  1937,  1968,  1992,  1993,  1995,  1998,\n",
       "        2000,  2004,  2005,  2007,  2009,  2016,  2017,  2018,  2172,\n",
       "        2331,  2340,  2389,  2402,  2461,  2598,  2729,  2789,  2792,\n",
       "        2799,  3199,  3454,  3500,  4400,  5411,  5420,  5809,  5867,\n",
       "        6006,  6010,  6011,  6012,  6018,  6045,  6062,  6226,  6512,\n",
       "        6920,  7508,  7511,  7512,  7515,  7518,  7529,  7544,  8259,\n",
       "        8404,  8500,  9007,  9012,  9013, 10000, 10110, 10218, 10311,\n",
       "       10520, 10522, 10710, 10910, 10912, 11011, 11025, 11509, 11530,\n",
       "       11635, 12012, 12510, 12512, 13636, 14009, 15001, 15017, 15020,\n",
       "       15033, 16311, 16312, 17011, 17019, 17410, 17700, 17932, 19208,\n",
       "       19211, 19312, 20000], dtype=int64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sub['Power'].sort_values().unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmgAAAJbCAYAAABQAWhKAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAABhaElEQVR4nO3df1zV9f3///sBOYgIJxH5lSYk6Sowy0qlkZBNM1FJ2Wo4pltZm2lvDbVs71bbt2mLrLXKSme1lek2IVtUTC0tTDSzWGK/iLf4I0GN8ICI/Di8vn/44TWPkOcQR88Rb9fL5VwunOfzcc55vk6O3Xm+Xs/ny2IYhiEAAAD4DD9vDwAAAADOCGgAAAA+hoAGAADgYwhoAAAAPoaABgAA4GMIaAAAAD6GgAYAAOBjCGgAAAA+hoAGAADgYwhoAAAAPoaABpxBL774oiwWiywWizZu3Nim3zAMxcfHy2KxKCUl5YyPDwDgGwhogBeEhIRo+fLlbdrfffddlZWVKSQkxAujAgD4CgIa4AU333yzcnNzVVNT49S+fPlyjRgxQhdccIGXRgYA8AUENMALfvrTn0qSVq5cabbZ7Xbl5ubql7/8Zbuv+fbbbzVjxgydf/75slqtuvDCC/Wb3/xGDQ0NTnUWi0UzZ850aktLS1NsbKxT2+rVq9ucat24caMsFotWr17d5vN79uypadOmObWVlJRo4sSJ6tWrl7p3764hQ4bor3/9a5vXHj58WNnZ2brwwgsVGBioiIgI3Xjjjfr8889VXl5unvb9rkfr57aeIv7www/b/Y7c8eCDD7b7GSefUk5JSXHq7927t0aPHq1t27Y51cXGxrb5Xl566SVZLBan7/xUx/niiy9Kko4dO6bs7GwNGTJENptNYWFhGjFihF577TWn93f1fZ14LDU1NZo7d67i4uJktVp1/vnna/bs2aqrq2vz3Zx4Cv5U383atWt17bXXqnfv3qesO9nJ30FAQIAuuOACzZo1q93xtMfdMbanoaFBv//973XxxRere/fu6t27t1JTU7V582a3Phs4k7p5ewDAuSg0NFQZGRl6/vnndccdd0g6Htb8/Px08803609/+pNT/bFjx5SamqqysjL97ne/0+DBg1VYWKhFixapuLhYb7zxxhk/hi+++EJJSUmKiIjQn//8Z/Xu3Vsvv/yypk2bpgMHDmj+/PmSpNraWv3whz9UeXm57rnnHg0bNkxHjhzRe++9p4qKCiUlJamoqMh837/85S9avny5U1ufPn08Pv6CggLZbDZJ0pQpU9qtufzyy7VkyRIZhqFdu3bpN7/5jcaMGaP9+/ere/fu7b6mpqZG8+fPl7+/f7v9s2bNUmZmplPbgAEDJB0PEN9++63mzp2r888/X42NjVq/fr0mTZqkF154QT//+c8lyem7eeONN/TQQw8pLy9P0dHRko7/+5Kko0ePauTIkdq3b5/uu+8+DR48WDt37tRvf/tb7dixQ+vXr5fFYmkzxhPfa8aMGU595eXlmjBhgoYMGaLnn39ekZGRp/wO2/O///u/GjdunHl8Dz30kBwOh5YsWeL2e5xqjO1pbm7W2LFjVVhYqNmzZ+u6665Tc3OztmzZoj179igpKcntzwbOBAIa4CW//OUvlZqaqp07d+rSSy/V888/rx//+MftXn/217/+VZ988on+8Y9/6Mc//rEk6Uc/+pF69uype+65R+vWrdOPfvSjMzr+Bx98UI2NjdqwYYP69esnSbrxxht1+PBh/e53v9Mdd9whm82mP/3pT9q5c6fWrVun66+/3nz9pEmTzJ+HDx9u/lxQUNCmzZNaZxyvvvpq9erVS5IUFBTUbm1oaKg5jhEjRujQoUOaPXu2du3apYsvvrjd1zzwwAPy9/dXenp6uzN9F1xwwXcem81m0wsvvGA+dzgcGjVqlKqrq/WnP/3JDGgnvv7zzz+XdDxMnjxL+uc//1mffPKJtm7dqiuvvFKSNGrUKJ1//vnKyMhQQUGBxo4da9Y3NjZKkq666ir17dvX/A5O9OGHH6qhoUELFy7UddddZ7Z/13fYngEDBpjHcO211+r111/X9u3b3XqtO2Nsz8qVK7VhwwYtW7ZMt912m9k+fvx4t8cNnEmc4gS8ZOTIkRowYICef/557dixQ9u2bfvO05vvvPOOgoODlZGR4dTeemrt7bff9ujYWlpa1Nzc7PRob0yjRo0yw9mJYzp69Kg5y/PWW29p4MCBTuGssxwOh5qbm2UYRodfe+TIEUlSjx49XNYahqHm5mY1NTXpyy+/1N///nfFxsbqwgsvbLe+pKRETz31lBYvXqyePXt2eGyS9M9//lPXXHONevbsqW7duikgIEDLly/XZ5991uH3ys/PV0JCgoYMGeL033LMmDHtriSur6+XpO+cHZT+O9u3fPlyff3119/57+NUWv99HT16VP/617/0+eefa9SoUW691p0xtuett95S9+7dv/N/Y4CvIaABXmKxWPSLX/xCL7/8sp599lkNHDhQycnJ7dZWVVUpKiqqzemoiIgIdevWTVVVVR4d280336yAgACnx8nXCFVVVZmnmE4UExNj9kvSoUOHzJkOTxk+fLg5rvPPP1+3336729/B119/rbCwMAUGBrqsfe+99xQQECCr1apBgwZp7969WrFixXe+9s4771RycrJuvvnmDh1Pq7y8PP3kJz/R+eefr5dffllFRUVmcD927FiH3+/AgQP65JNP2vy3DAkJkWEY+uabb5zqv/nmG/n5+Zkzi+25/PLL9cQTT2jt2rXq27ev+Z47d+50e1y33nqrAgICFBwcrIkTJ2rUqFG6//773XqtO2Nsz6FDhxQTEyM/P/5vD2cHTnECXjRt2jT99re/1bPPPqs//OEP31nXu3dvbd26VYZhOIW0gwcPqrm5WeHh4R4d1x//+Een01fS8VNRJ4+poqKizWv3798vSeaY+vTpo3379nl0fH/729908cUXq6mpSdu3b9c999yjgwcPas2aNS5f+5///EeJiYlufc4VV1yh5557TtLxRRwvvviirr/+ehUWFmro0KFOtStWrFBRUZGKi4s7ejiml19+WXFxcfr73//u9N/55IUg7goPD1dQUJCef/757+w/UWlpqeLi4r7z+rlWd911l2pra81r3/r06aNbbrnF7XE98MADSktLU0tLi3bt2qX7779f1113nTZt2uTys90d48n69OmjTZs2qaWlhZCGswIBDfCi888/X/PmzdPnn3+uqVOnfmfdqFGj9I9//ENr1qzRTTfdZLb/7W9/M/s96cILLzSvWWp18v+pjRo1Sq+++qr2799vzpq1jqlHjx7mNUZjx47Vb3/7W73zzjttQt/3dfHFF5vjGzFihN5++21t3brV5et27typ//u//3PronLp+H51J34PQ4cO1csvv6zc3FyngFZbW6t58+bpf/7nf3TJJZd08Gj+y2KxyGq1OoWzysrKNqs43ZWWlqaFCxeqd+/eiouLO2Wt3W7Xhg0bNG7cOJfv++GHH+p3v/udHn30UfMato6ccoyNjTW/16uvvloVFRWaM2eOysrKNHDgQI+M8WRjx47VypUr9eKLL3KaE2cFAhrgZQ8//LDLmp///Od6+umnNXXqVJWXlysxMVGbNm3SwoULdeONN7a5vuvw4cPmxeOSVFdXp6amJqe21pmuPXv26JtvvunwLNwDDzyg/Px8paam6re//a3CwsK0YsUKvfHGG3rkkUfMFZKzZ8/W3//+d02cOFH33nuvrr76atXX1+vdd99VWlqaUlNTO/S5krR792717NlTTU1NKi4udiv8bd26VbNmzZLValVCQoK2bNli9tXX16umpkYff/yxLr/8crO9pqbGrGudQZPkVCNJr732miIjI/XAAw90+FhOlJaWpry8PM2YMUMZGRnau3ev/r//7/9TdHS0SktLO/x+s2fPVm5urq699lrNmTNHgwcPVktLi/bs2aO1a9cqOztbw4YN05o1a7Rw4ULZ7XbNmTPnlO959OhRTZkyRampqZo1a9b3Os6ysjJt2bJFLS0tKi8v11NPPaWwsDD179//O1/TkTHu3r1bAwYM0NSpU80NoX/605/qhRde0K9+9St98cUXSk1NVUtLi7Zu3aqLL764QzOAwBlhADhjXnjhBUOSsW3btlPWXXrppcbIkSOd2qqqqoxf/epXRnR0tNGtWzejf//+xoIFC4xjx4451Unq8OOBBx4wDMMwNmzYYEgy/vnPf7YZU3BwsDF16lSnth07dhjjx483bDabYbVajcsuu8x44YUX2ry2urra+J//+R/jggsuMAICAoyIiAhj3Lhxxueff96m9oEHHjC+61dT6/fX+ggICDD69etn3H777UZVVdV3f6GGYfTv39/l99C/f3+zfuTIkU59ISEhxpAhQ4xnn3223fdduXKlU/vUqVOd3m/Xrl2GJCMnJ+eU43z44YeN2NhYIzAw0Lj44ouNZcuWufWd7Nq1q93+I0eOGP/7v/9rDBo0yLBarYbNZjMSExONOXPmGJWVlYZhGMaVV15pjB8/vt1/lyNHjnT6t3j77bcbvXv3Nvbv3+9U196/2ZO1fgetDz8/PyMiIsIYP3688fHHH5/ytR0ZY+vnnPzvtb6+3vjtb39rXHTRRYbVajV69+5tXHfddcbmzZtP+dmAN1gM43ssgwLQZaSkpCglJUUPPvigt4dyWsXGxurBBx9ss6lsq40bN2ratGkqLy8/o+MCgPZwpSRwjrvgggs8vsjAF11++eWn3PA2NDS0zalLAPAWZtAAAAB8DDNoAAAAPoaABgAA4GMIaAAAAD7mnN4HraWlRfv371dISEibW+gAAAB4mmEYqq2tdXnrsXM6oO3fv7/NjZ4BAABOt717957yPsXndEALCQmRdPxLCg0N9fJoAABAV1dTU6N+/fqZGeS7nNMBrfW0ZmhoKAENAACcMa4urWKRAAAAgI8hoAEAAPgYAhoAAICPIaABAAD4GAIaAACAjyGgAQAA+BgCGgAAgI8hoAEAAPgYAhoAAICPIaABAAD4GAIaAACAjyGgAQAA+BgCGgAAgI8hoAEAAPgYAhoAAICPIaABAAD4GAIaAACAj+lQQFu0aJGuuuoqhYSEKCIiQunp6friiy+cagzD0IMPPqiYmBgFBQUpJSVFO3fudKppaGjQrFmzFB4eruDgYE2YMEH79u1zqqmurlZWVpZsNptsNpuysrJ0+PBhp5o9e/Zo/PjxCg4OVnh4uO666y41NjZ25JAAoEMcDoc2btyolStXauPGjXI4HN4eEoAuqEMB7d1339Wdd96pLVu2aN26dWpubtbo0aNVV1dn1jzyyCN67LHH9NRTT2nbtm2KiorSj370I9XW1po1s2fP1quvvqpVq1Zp06ZNOnLkiNLS0px+0WVmZqq4uFgFBQUqKChQcXGxsrKyzH6Hw6Fx48aprq5OmzZt0qpVq5Sbm6vs7OzOfB8A8J3y8vIUHx+v1NRUZWZmKjU1VfHx8crLy/P20AB0NUYnHDx40JBkvPvuu4ZhGEZLS4sRFRVlPPzww2bNsWPHDJvNZjz77LOGYRjG4cOHjYCAAGPVqlVmzddff234+fkZBQUFhmEYxqeffmpIMrZs2WLWFBUVGZKMzz//3DAMw3jzzTcNPz8/4+uvvzZrVq5caQQGBhp2u92t8dvtdkOS2/UAzl25ubmGxWIxxo8fbxQVFRm1tbVGUVGRMX78eMNisRi5ubneHiKAs4C72aNT16DZ7XZJUlhYmCRp165dqqys1OjRo82awMBAjRw5Ups3b5Ykbd++XU1NTU41MTExSkhIMGuKiopks9k0bNgws2b48OGy2WxONQkJCYqJiTFrxowZo4aGBm3fvr0zhwUAThwOh7Kzs5WWlqY1a9Zo+PDh6tmzp4YPH641a9YoLS1Nc+fO5XQnAI/53gHNMAzdfffd+uEPf6iEhARJUmVlpSQpMjLSqTYyMtLsq6yslNVqVa9evU5ZExER0eYzIyIinGpO/pxevXrJarWaNSdraGhQTU2N0wMAXCksLFR5ebnuu+8++fk5/9r08/PTggULtGvXLhUWFnpphAC6mu8d0GbOnKlPPvlEK1eubNNnsVicnhuG0abtZCfXtFf/fWpOtGjRInPRgc1mU79+/U45JgCQpIqKCkky/xg9WWt7ax0AdNb3CmizZs3Sv/71L23YsEF9+/Y126OioiSpzQzWwYMHzdmuqKgoNTY2qrq6+pQ1Bw4caPO5hw4dcqo5+XOqq6vV1NTUZmat1YIFC2S3283H3r17O3LYAM5R0dHRkqSSkpJ2+1vbW+sAoLM6FNAMw9DMmTOVl5end955R3FxcU79cXFxioqK0rp168y2xsZGvfvuu0pKSpIkDR06VAEBAU41FRUVKikpMWtGjBghu92uDz74wKzZunWr7Ha7U01JSYnTX6xr165VYGCghg4d2u74AwMDFRoa6vQAAFeSk5MVGxurhQsXqqWlxamvpaVFixYtUlxcnJKTk700QgBdTkdWHvz61782bDabsXHjRqOiosJ8HD161Kx5+OGHDZvNZuTl5Rk7duwwfvrTnxrR0dFGTU2NWfOrX/3K6Nu3r7F+/Xrjo48+Mq677jrjsssuM5qbm82aG264wRg8eLBRVFRkFBUVGYmJiUZaWprZ39zcbCQkJBijRo0yPvroI2P9+vVG3759jZkzZ7p9PKziBOCuE1dxbt682aipqTE2b97MKk4AHeJu9uhQQJPU7uOFF14wa1paWowHHnjAiIqKMgIDA41rr73W2LFjh9P71NfXGzNnzjTCwsKMoKAgIy0tzdizZ49TTVVVlTFlyhQjJCTECAkJMaZMmWJUV1c71ezevdsYN26cERQUZISFhRkzZ840jh075vbxENAAdERubq4RGxvr9PsvLi6OcAbAbe5mD4thGIZXpu58QE1NjWw2m+x2O6c7AbjF4XCosLBQFRUVio6OVnJysvz9/b09LABnCXezR7czOCYAOOv5+/srJSXF28MA0MVxs3QAAAAfQ0ADAADwMQQ0AAAAH0NAAwAA8DEENAAAAB/DKk4A6AC22QBwJjCDBgBuysvLU3x8vFJTU5WZmanU1FTFx8crLy/P20MD0MUwgwYAbsjLy1NGRoZuvPFGTZw4UfX19QoKCtJXX32ljIwMrV69WpMmTfL2MAF0EdxJgDsJAHDB4XAoPj5e/v7+Ki8vl8PhMPv8/f0VGxurlpYWlZaWcroTwCm5mz04xQkALhQWFqq8vFxlZWUKDw/XsmXLVFFRoWXLlik8PFxlZWXatWuXCgsLvT1UAF0EpzgBwIW9e/dKkvr06aN9+/apW7fjvzpvu+02TZs2TTExMTp06JBZBwCdxQwaALiwdetWSdKtt95qhrNW3bp10y9+8QunOgDoLAIaALjQeqnuRx99pJaWFqe+lpYWffzxx051ANBZBDQAcOGiiy6SJK1bt07p6ekqKipSbW2tioqKlJ6ervXr1zvVAUBnsYqTVZwAXGhsbFRwcLCCg4PVq1cvlZeXm31xcXH69ttvVVdXp7q6OlmtVu8NFIDPYxUnAHiI1WrVnDlzZLfbdfToUd199916+umndffdd6uurk52u11z5swhnAHwGFZxAoAbHnnkEUnS448/rscee8xs79atm+bNm2f2A4AncIqTU5wAOqCxsVFLlixRWVmZBgwYoBkzZjBzBsBt7mYPAhoBDQAAnCFcgwYAAHCWIqABAAD4GAIaAACAjyGgAQAA+BgCGgAAgI8hoAEAAPgYAhoAAICPIaABAAD4GAIaAACAjyGgAQAA+BgCGgAAgI8hoAEAAPgYAhoAAICPIaABAAD4GAIaAACAjyGgAQAA+BgCGgAAgI8hoAEAAPgYAhoAAICP6ebtAQDA2cThcKiwsFAVFRWKjo5WcnKy/P39vT0sAF0MM2gA4Ka8vDzFx8crNTVVmZmZSk1NVXx8vPLy8rw9NABdDAENANyQl5enjIwMJSYmqqioSLW1tSoqKlJiYqIyMjIIaQA8ymIYhuHtQXhLTU2NbDab7Ha7QkNDvT0cAD7K4XAoPj5eiYmJWrNmjfz8/vu3bUtLi9LT01VSUqLS0lJOdwI4JXezBzNoAOBCYWGhysvLdd999zmFM0ny8/PTggULtGvXLhUWFnpphAC6GgIaALhQUVEhSUpISGi3v7W9tQ4AOouABgAuREdHS5JKSkrkcDi0ceNGrVy5Uhs3bpTD4VBJSYlTHQB0FtegcQ0aABdar0ELDw/XN998o/LycrMvNjZW4eHhqqqq4ho0AC5xDRoAeIi/v79+/OMf68MPP1R9fb2WLl2q/fv3a+nSpaqvr9eHH36ojIwMwhkAj2EGjRk0AC6cOIN26NAh7d692+xjBg1AR7ibPbiTAAC40LqKc+XKlbrqqqva3Enggw8+UFJSkgoLC5WSkuLt4QLoAghoAODCias4/f3924QwVnEC8LQOX4P23nvvafz48YqJiZHFYtGaNWuc+i0WS7uPnJwcsyYlJaVN/y233OL0PtXV1crKypLNZpPNZlNWVpYOHz7sVLNnzx6NHz9ewcHBCg8P11133aXGxsaOHhIAnNKJqzjbwypOAJ7W4YBWV1enyy67TE899VS7/RUVFU6P559/XhaLRZMnT3aqmz59ulPdc88959SfmZmp4uJiFRQUqKCgQMXFxcrKyjL7HQ6Hxo0bp7q6Om3atEmrVq1Sbm6usrOzO3pIAHBKycnJio2N1cKFC9XU1OS0zUZTU5MWLVqkuLg4JScne3uoALqIDp/iHDt2rMaOHfud/VFRUU7PX3vtNaWmpurCCy90au/Ro0eb2lafffaZCgoKtGXLFg0bNkyStGzZMo0YMUJffPGFBg0apLVr1+rTTz/V3r17FRMTI0lavHixpk2bpj/84Q9c9A/AY/z9/bV48WJlZGTIZrOpvr7e7AsKCtKxY8e0evVqFggA8JjTus3GgQMH9MYbb+jWW29t07dixQqFh4fr0ksv1dy5c1VbW2v2FRUVyWazmeFMkoYPHy6bzabNmzebNQkJCWY4k6QxY8aooaFB27dvb3c8DQ0NqqmpcXoAgLvaW/RusVjabQeAzjitAe2vf/2rQkJCNGnSJKf2KVOmmKcH7r//fuXm5jrVVFZWKiIios37RUREqLKy0qyJjIx06u/Vq5esVqtZc7JFixaZ17TZbDb169evs4cI4BzgcDiUnZ2t8ePHy263a8OGDXrllVe0YcMGHT58WOPHj9fcuXPlcDi8PVQAXcRpXcX5/PPPa8qUKerevbtT+/Tp082fExISdNFFF+nKK6/URx99pCuuuELS8b9KT2YYhlO7OzUnWrBgge6++27zeU1NDSENgEsnbrMREBDQZhXnggUL2GYDgEedthm0wsJCffHFF7rttttc1l5xxRUKCAhQaWmppOPXsR04cKBN3aFDh8xZs6ioqDYzZdXV1Wpqamozs9YqMDBQoaGhTg8AcIWbpQM4005bQFu+fLmGDh2qyy67zGXtzp071dTUZC5RHzFihOx2uz744AOzZuvWrbLb7UpKSjJrSkpKnH4hrl27VoGBgRo6dKiHjwbAuYxtNgCcaR2+1dORI0f01VdfSZIuv/xyPfbYY0pNTVVYWJguuOACScdPHUZHR2vx4sX61a9+5fT6srIyrVixQjfeeKPCw8P16aefKjs7W0FBQdq2bZu5Cmrs2LHav3+/uf3G7bffrv79++v111+XdPyakCFDhigyMlI5OTn69ttvNW3aNKWnp+vJJ59061i41RMAd7Te6ikxMVFr1qyRn99//7ZtaWlRenq6SkpKuNUTAJfczh5GB23YsMGQ1OYxdepUs+a5554zgoKCjMOHD7d5/Z49e4xrr73WCAsLM6xWqzFgwADjrrvuMqqqqpzqqqqqjClTphghISFGSEiIMWXKFKO6utqpZvfu3ca4ceOMoKAgIywszJg5c6Zx7Ngxt4/Fbrcbkgy73d6h7wDAuSc3N9ewWCzG+PHjjc2bNxs1NTXG5s2bjfHjxxsWi8XIzc319hABnAXczR7cLJ0ZNABuysvLU3Z2tsrLy822uLg4Pfroo21WqwNAe9zNHgQ0AhqADqivr9e8efNUWlqqiy66SDk5OQoKCvL2sACcJdzNHqd1HzQA6Ermz5+v0NBQPf3001q7dq2efvpphYaGav78+d4eGoAuhoAGAG6YP3++cnJy1Lt3by1btkwVFRVatmyZevfurZycHEIaAI/iFCenOAG40NjYqODgYPXu3Vv79u1Tt27/3eO7ublZffv2VVVVlerq6mS1Wr04UgC+jlOcAOAhS5YsUXNzsx566CGncCZJ3bp10+9//3s1NzdryZIlXhohgK6GgAYALpSVlUmS0tLS2u1vbW+tA4DOIqABgAsDBgyQJOXn57fb39reWgcAncU1aFyDBsAFrkED4ClcgwYAHmK1WjVnzhwdOHBAffv21dKlS7V//34tXbpUffv21YEDBzRnzhzCGQCP6ea6BADwyCOPSJIef/xx3XHHHWZ7t27dNG/ePLMfADyBU5yc4gTQAY2NjVqyZInKyso0YMAAzZgxg5kzAG7jVk9uIKABAIAziWvQAAAAzlIENAAAAB9DQAMAAPAxBDQAAAAfQ0ADAADwMeyDBgAdwDYbAM4EAhoAuGn+/Pl6/PHH1dzcbLbNmzdPc+bMYaNaAB7FKU4AcMP8+fOVk5Oj3r17a9myZaqoqNCyZcvUu3dv5eTkaP78+d4eIoAuhI1q2agWgAvcLB2Ap7BRLQB4yJIlS9Tc3KyHHnrIKZxJx+/F+fvf/17Nzc1asmSJl0YIoKshoAGAC2VlZZKktLS0dvtb21vrAKCzCGgA4MKAAQMkSfn5+e32t7a31gFAZ3ENGtegAXCBa9AAeArXoAGAh1itVs2ZM0cHDhxQ3759tXTpUu3fv19Lly5V3759deDAAc2ZM4dwBsBj2AcNANzQus/Z448/rjvuuMNs79atm+bNm8c+aAA8ilOcnOIE0AHcSQBAZ7ibPQhoBDQAAHCGcA0aAADAWYqABgAA4GMIaAAAAD6GgAYAAOBjCGgAAAA+hoAGAADgYwhoAAAAPoaABgAA4GMIaAAAAD6GgAYAAOBjCGgAAAA+hoAGAADgYwhoAAAAPoaABgAA4GMIaAAAAD6GgAYAAOBjCGgAAAA+hoAGAADgYwhoAAAAPqabtwcAAGcTh8OhwsJCVVRUKDo6WsnJyfL39/f2sAB0MQQ0AHBTXl6e5syZoz179phtF1xwgR5//HFNmjTJiyMD0NV0+BTne++9p/HjxysmJkYWi0Vr1qxx6p82bZosFovTY/jw4U41DQ0NmjVrlsLDwxUcHKwJEyZo3759TjXV1dXKysqSzWaTzWZTVlaWDh8+7FSzZ88ejR8/XsHBwQoPD9ddd92lxsbGjh4SALiUl5enyZMnO4Uz6fjvocmTJysvL89LIwPQFXU4oNXV1emyyy7TU0899Z01N9xwgyoqKszHm2++6dQ/e/Zsvfrqq1q1apU2bdqkI0eOKC0tTQ6Hw6zJzMxUcXGxCgoKVFBQoOLiYmVlZZn9DodD48aNU11dnTZt2qRVq1YpNzdX2dnZHT0kADglh8Nh/v6xWq2699579dVXX+nee++V1WqVJGVlZTn9DgOATjE6QZLx6quvOrVNnTrVmDhx4ne+5vDhw0ZAQICxatUqs+3rr782/Pz8jIKCAsMwDOPTTz81JBlbtmwxa4qKigxJxueff24YhmG8+eabhp+fn/H111+bNStXrjQCAwMNu93u1vjtdrshye16AOemt956y5BkBAQEGA0NDU59DQ0NRkBAgCHJeOutt7w0QgBnC3ezx2lZxblx40ZFRERo4MCBmj59ug4ePGj2bd++XU1NTRo9erTZFhMTo4SEBG3evFmSVFRUJJvNpmHDhpk1w4cPl81mc6pJSEhQTEyMWTNmzBg1NDRo+/bt7Y6roaFBNTU1Tg8AcOWxxx6TJM2aNcucMWtltVp15513OtUBQGd5PKCNHTtWK1as0DvvvKPFixdr27Ztuu6669TQ0CBJqqyslNVqVa9evZxeFxkZqcrKSrMmIiKizXtHREQ41URGRjr19+rVS1ar1aw52aJFi8xr2mw2m/r169fp4wXQ9VVXV0uSRowY0W5/6x+TrXUA0FkeD2g333yzxo0bp4SEBI0fP15vvfWWvvzyS73xxhunfJ1hGLJYLObzE3/uTM2JFixYILvdbj727t3r7mEBOIddddVVko7/DmlpaXHqa2lp0f/+7/861QFAZ532jWqjo6PVv39/lZaWSpKioqLU2NjY5i/NgwcPmjNiUVFROnDgQJv3OnTokFPNyTNl1dXVampqajOz1iowMFChoaFODwBwZfHixZKkr776ShMmTFBRUZFqa2tVVFSkCRMmqKyszKkOADrrtAe0qqoq7d27V9HR0ZKkoUOHKiAgQOvWrTNrKioqVFJSoqSkJEnHTyPY7XZ98MEHZs3WrVtlt9udakpKSlRRUWHWrF27VoGBgRo6dOjpPiwA55CgoCBNnDhRkvTGG28oKSlJoaGhSkpKMs8OTJw4UUFBQd4cJoAuxGIYhtGRFxw5ckRfffWVJOnyyy/XY489ptTUVIWFhSksLEwPPvigJk+erOjoaJWXl+u+++7Tnj179NlnnykkJESS9Otf/1r5+fl68cUXFRYWprlz56qqqkrbt283d+QeO3as9u/fr+eee06SdPvtt6t///56/fXXJR1f9j5kyBBFRkYqJydH3377raZNm6b09HQ9+eSTbh1LTU2NbDab7HY7s2kAXEpPT9drr73Wpn3ixIlt9oQEgPa4nT06ujx0w4YNhqQ2j6lTpxpHjx41Ro8ebfTp08cICAgwLrjgAmPq1KnGnj17nN6jvr7emDlzphEWFmYEBQUZaWlpbWqqqqqMKVOmGCEhIUZISIgxZcoUo7q62qlm9+7dxrhx44ygoCAjLCzMmDlzpnHs2DG3j4VtNgB01NGjR40777zTGD16tHHnnXcaR48e9faQAJxF3M0eHZ5B60qYQQMAAGeSu9njtF+DBgAAgI4hoAEAAPgYAhoAAICPIaABAAD4GAIaAACAjyGgAQAA+BgCGgAAgI8hoAEAAPgYAhoAAICPIaABAAD4GAIaAACAjyGgAQAA+BgCGgAAgI/p5u0BAMDZxOFwqLCwUBUVFYqOjlZycrL8/f29PSwAXQwzaADgpry8PMXHxys1NVWZmZlKTU1VfHy88vLyvD00AF0MAQ0A3JCXl6eMjAwlJiaqqKhItbW1KioqUmJiojIyMghpADzKYhiG4e1BeEtNTY1sNpvsdrtCQ0O9PRwAPsrhcCg+Pl6JiYlas2aN/Pz++7dtS0uL0tPTVVJSotLSUk53Ajgld7MHM2gA4EJhYaHKy8t13333OYUzSfLz89OCBQu0a9cuFRYWemmEALoaAhoAuFBRUSFJSkhIaLe/tb21DgA6i4AGAC5ER0dLkkpKStrtb21vrQOAziKgAYALycnJio2N1cKFC9XS0uLU19LSokWLFikuLk7JycleGiGAroaABgAu+Pv7a/HixcrPz1d6errTKs709HTl5+fr0UcfZYEAAI9ho1oAcMOkSZO0evVqZWdnKykpyWyPi4vT6tWrNWnSJC+ODkBXwwwaAHTAyTsTnXzKEwA8gYAGAG5o3ah28ODBTqc4Bw8ezEa1ADyOjWrZqBaAC2xUC8BT2KgWADyEjWoBnGkENABwgY1qAZxpBDQAcIGNagGcaQQ0AHCBjWoBnGkENABwgY1qAZxpbFQLAG5go1oAZxLbbLDNBoAOcDgcKiwsVEVFhaKjo5WcnMzMGQC3uZs9mEEDgA7w9/dXSkqKt4cBoIvjGjQAAAAfQ0ADAADwMQQ0AAAAH0NAAwAA8DEENAAAAB9DQAMAAPAxBDQAAAAfQ0ADAADwMQQ0AAAAH0NAAwAA8DEENAAAAB9DQAMAAPAxBDQAAAAfQ0ADAADwMQQ0AAAAH0NAA4AOOHLkiG666SYNHjxYN910k44cOeLtIQHogjoc0N577z2NHz9eMTExslgsWrNmjdnX1NSke+65R4mJiQoODlZMTIx+/vOfa//+/U7vkZKSIovF4vS45ZZbnGqqq6uVlZUlm80mm82mrKwsHT582Klmz549Gj9+vIKDgxUeHq677rpLjY2NHT0kAHDL1VdfrZCQEK1Zs0Y7duzQmjVrFBISoquvvtrbQwPQxXQ4oNXV1emyyy7TU0891abv6NGj+uijj3T//ffro48+Ul5enr788ktNmDChTe306dNVUVFhPp577jmn/szMTBUXF6ugoEAFBQUqLi5WVlaW2e9wODRu3DjV1dVp06ZNWrVqlXJzc5Wdnd3RQwIAl66++mpt27ZNFotFWVlZ+s9//qOsrCxZLBZt27aNkAbAoyyGYRjf+8UWi1599VWlp6d/Z03rL67du3frggsukHR8Bm3IkCH605/+1O5rPvvsM11yySXasmWLhg0bJknasmWLRowYoc8//1yDBg3SW2+9pbS0NO3du1cxMTGSpFWrVmnatGk6ePCgQkNDXY6/pqZGNptNdrvdrXoA56YjR44oJCREFotFR48eVffu3c2+Y8eOqUePHjIMQ7W1terZs6cXRwrA17mbPU77NWh2u10Wi0XnnXeeU/uKFSsUHh6uSy+9VHPnzlVtba3ZV1RUJJvNZoYzSRo+fLhsNps2b95s1iQkJJjhTJLGjBmjhoYGbd++/fQeFIBzSuvs/c9+9jOncCZJ3bt3V2ZmplMdAHRWt9P55seOHdO9996rzMxMp5Q4ZcoUxcXFKSoqSiUlJVqwYIH+85//aN26dZKkyspKRUREtHm/iIgIVVZWmjWRkZFO/b169ZLVajVrTtbQ0KCGhgbzeU1NTaePEUDXV1ZWJkmaO3duu/133323VqxYYdYBQGedthm0pqYm3XLLLWppadGSJUuc+qZPn67rr79eCQkJuuWWW7R69WqtX79eH330kVljsVjavKdhGE7t7tScaNGiReaiA5vNpn79+n3fwwNwDhkwYIAk6dFHH223/7HHHnOqA4DOOi0BrampST/5yU+0a9curVu3zuX1XVdccYUCAgJUWloqSYqKitKBAwfa1B06dMicNYuKimozU1ZdXa2mpqY2M2utFixYILvdbj727t37fQ4PwDnmpZdekiS9/PLLOnbsmFPfsWPH9MorrzjVAUBneTygtYaz0tJSrV+/Xr1793b5mp07d6qpqUnR0dGSpBEjRshut+uDDz4wa7Zu3Sq73a6kpCSzpqSkRBUVFWbN2rVrFRgYqKFDh7b7OYGBgQoNDXV6AIArPXv21FVXXSXDMNSjRw/97Gc/00cffaSf/exn5gKBq666igUCADymw6s4jxw5oq+++kqSdPnll+uxxx5TamqqwsLCFBMTo8mTJ+ujjz5Sfn6+00xWWFiYrFarysrKtGLFCt14440KDw/Xp59+quzsbAUFBWnbtm3y9/eXJI0dO1b79+83t9+4/fbb1b9/f73++uuSjm+zMWTIEEVGRionJ0fffvutpk2bpvT0dD355JNuHQurOAF0ROtWGye76qqrnP6gBIDv4nb2MDpow4YNhqQ2j6lTpxq7du1qt0+SsWHDBsMwDGPPnj3Gtddea4SFhRlWq9UYMGCAcddddxlVVVVOn1NVVWVMmTLFCAkJMUJCQowpU6YY1dXVTjW7d+82xo0bZwQFBRlhYWHGzJkzjWPHjrl9LHa73ZBk2O32jn4NAM5RtbW1Rnp6upGYmGikp6cbtbW13h4SgLOIu9mjU/ugne2YQQMAAGeSu9njtG6zAQBdjcPhUGFhoSoqKhQdHa3k5GTz0gwA8BRulg4AbsrLy1N8fLxSU1OVmZmp1NRUxcfHKy8vz9tDA9DFENAAwA15eXnKyMhQYmKiioqKVFtbq6KiIiUmJiojI4OQBsCjuAaNa9AAuOBwOBQfH6/ExEStWbNGfn7//du2paVF6enpKikpUWlpKac7AZySz9yLEwDOdoWFhSovL9d9993nFM4kyc/PTwsWLNCuXbtUWFjopREC6GoIaADgQuuG2AkJCe32t7afuHE2AHQGAQ0AXGi9y0lJSUm7/a3trXUA0FkENABwITk5WbGxsVq4cKFaWlqc+lpaWrRo0SLFxcUpOTnZSyME0NUQ0ADABX9/fy1evFj5+flKT093WsWZnp6u/Px8PfrooywQAOAxbFQLAG6YNGmSVq9erezsbCUlJZntcXFxWr16tSZNmuTF0QHoathmg202AHRAY2OjlixZorKyMg0YMEAzZsyQ1Wr19rAAnCW41RMAeFheXp6ys7NVXl5utj3xxBNavHgxM2gAPIqABgBuaL2TwLhx4zRv3jwFBQWpvr5eb731ljIyMjjNCcCjOMXJKU4ALrTeSSA8PFwHDx7Unj17zL4LLrhAERERqqqq4k4CAFziFCcAeEjrnQTKy8sVFBTk1Hfo0CEzsBUWFiolJcULIwTQ1bDNBgC48PXXX5s/jxo1ymmbjVGjRrVbBwCdwQwaALhQWVkpSRo8eLBee+01836cw4cP12uvvabLL79cn3zyiVkHAJ3FDBoAuPDtt99KkoKDg9vt79Gjh1MdAHQWAQ0AXGidMduyZUu7dxLYunWrUx0AdBa/TQDAhdYL/wcNGqQdO3YoKSlJoaGhSkpKUklJiQYNGuRUBwCdxTYbbLMBwAWHw6GYmBgdPHhQ48aN09ixY532QXvjjTcUERGh/fv3s80GgFNimw0A8BB/f38988wzysjI0DvvvKM33njD7OvRo4csFoueeeYZwhkAj+EUJwC4ofVm6ZGRkU7tkZGR3EUAgMdxipNTnAA6wOFwqLCwUBUVFYqOjlZycjIzZwDcxilOADgN/P39WQwA4LQjoAFABzQ2NmrJkiUqKyvTgAEDNGPGDFmtVm8PC0AXQ0ADADfNnz9fjz/+uJqbm822efPmac6cOXrkkUe8ODIAXQ0BDQDcMH/+fOXk5CgiIkIpKSkKDg5WXV2dNm7cqJycHEkipAHwGBYJsEgAgAuNjY0KDg6W1WrVsWPH1NLSYvb5+fmpe/fuamxsVF1dHac7AZySu9mDbTYAwIUlS5aoublZR48eVZ8+fbRs2TJVVFRo2bJl6tOnj44eParm5mYtWbLE20MF0EVwihMAXPjyyy8lSeHh4dq3b5+6dTv+q/O2227TtGnTFB0drW+++casA4DOYgYNAFyoqKiQJI0dO9YMZ626deumMWPGONUBQGcxgwYALkRFRUmS3nrrLTU0NKioqMjcqHbEiBH697//7VQHAJ1FQAMAFwYNGiRJ+uabb9SjR482iwRan7fWAUBnsYqTVZwAXGhsbFRQUJBaWlpksVh04q/N1ud+fn6qr69nFSeAU+JWTwDgIf7+/urZs6dqamoUHh7eZh+0Q4cOqWfPntyTE4DHsEgAAFwoLCxUTU2NpkyZourqav3zn//Uiy++qH/+85+qrq5WZmamampqVFhY6O2hAugiCGgA4ELr6sxnn31W33zzja655hr169dP11xzjb755hs9++yzTnUA0Fmc4gQAF6KjoyVJN954o9Ms2d69e3XeeecpOTnZqQ4AOosZNABwITk5WUFBQSosLJTVatW9996rr776Svfee6+sVqsKCwsVFBRkBjUA6CwCGgC40NjYqPr6eknS9ddfrwkTJigiIkITJkzQ9ddfL0mqr69XY2OjN4cJoAshoAGAC/PmzZMk3XTTTfr000+VlJSk0NBQJSUl6bPPPlN6erpTHQB0FtegAYALpaWlkqScnBz169dPS5YsUVlZmQYMGKAZM2aovLxca9asMesAoLMIaADgwkUXXaS1a9dq3rx5+vjjj1VeXm72PfHEE7rsssvMOgDwBO4kwJ0EALhQX1+vHj16SDq+knPcuHEKCgpSfX293njjDb355puSpKNHjyooKMibQwXg49zNHgQ0AhoAFxwOh0JDQ3X06NHvrOnRo4dqamq4mwCAU3I3e7BIAABcKCwsPGU4k47PnnEnAQCeQkADABe+/vprSdLYsWNlt9uVnp6uxMREpaeny263a+zYsU51ANBZLBIAABcOHTokSYqNjdVll11mLhLYsWOHLrvsMo0ZM8apDgA6ixk0AHChT58+kqRnnnlGCQkJKioqUm1trYqKipSQkKDnnnvOqQ4AOqvDAe29997T+PHjFRMTI4vFojVr1jj1G4ahBx98UDExMQoKClJKSop27tzpVNPQ0KBZs2YpPDxcwcHBmjBhgvbt2+dUU11draysLNlsNtlsNmVlZenw4cNONXv27NH48eMVHBys8PBw3XXXXezkDcDjoqKizJ8Nw9D27dv1j3/8Q9u3b9eJ66xOrAOAzuhwQKurq9Nll12mp556qt3+Rx55RI899pieeuopbdu2TVFRUfrRj36k2tpas2b27Nl69dVXtWrVKm3atElHjhxRWlqaHA6HWZOZmani4mIVFBSooKBAxcXFysrKMvsdDofGjRunuro6bdq0SatWrVJubq6ys7M7ekgA4Jbzzz9f//73vzVz5kzdeuutmjlzpv7973/r/PPP9/bQAHQ1RidIMl599VXzeUtLixEVFWU8/PDDZtuxY8cMm81mPPvss4ZhGMbhw4eNgIAAY9WqVWbN119/bfj5+RkFBQWGYRjGp59+akgytmzZYtYUFRUZkozPP//cMAzDePPNNw0/Pz/j66+/NmtWrlxpBAYGGna73a3x2+12Q5Lb9QDOTa+88oohyZBkREREGNnZ2cbTTz9tZGdnGxEREWbfK6+84u2hAvBx7mYPj16DtmvXLlVWVmr06NFmW2BgoEaOHKnNmzdLkrZv366mpianmpiYGCUkJJg1RUVFstlsGjZsmFkzfPhw2Ww2p5qEhATFxMSYNWPGjFFDQ4O2b9/uycMCcI6LiIiQJP3gBz9QUFCQFi9erDvvvFOLFy9Wjx499IMf/MCpDgA6y6OrOCsrKyVJkZGRTu2RkZHavXu3WWO1WtWrV682Na2vr6ysbPcXXUREhFPNyZ/Tq1cvWa1Ws+ZkDQ0NamhoMJ/X1NR05PAAnOPCw8P1zjvv6P3331dFRYWio6N1zTXX6LrrrvP20AB0MadlFafFYnF6bhhGm7aTnVzTXv33qTnRokWLzEUHNptN/fr1O+WYAECSDh48KEl6//33NXnyZAUGBiotLU2BgYGaPHmy3n//fac6AOgsjwa01hVMJ89gHTx40JztioqKUmNjo6qrq09Zc+DAgTbvf+jQIaeakz+nurpaTU1NbWbWWi1YsEB2u9187N2793scJYBzTXR0tCRp4cKF2rFjh5KSkhQaGqqkpCSVlJToD3/4g1MdAHSWRwNaXFycoqKitG7dOrOtsbFR7777rpKSkiRJQ4cOVUBAgFNNRUWFSkpKzJoRI0bIbrfrgw8+MGu2bt0qu93uVFNSUqKKigqzZu3atQoMDNTQoUPbHV9gYKBCQ0OdHgDgSnJysmJjY7V582Z9+eWX2rBhg1555RVt2LBBX3zxhYqKihQXF6fk5GRvDxVAF9HhgHbkyBEVFxeruLhY0vGFAcXFxdqzZ48sFotmz56thQsX6tVXX1VJSYmmTZumHj16KDMzU5Jks9l06623Kjs7W2+//bY+/vhj/exnP1NiYqKuv/56SdLFF1+sG264QdOnT9eWLVu0ZcsWTZ8+XWlpaRo0aJAkafTo0brkkkuUlZWljz/+WG+//bbmzp2r6dOnE7wAeJS/v78WL16s/Pz8dk9x5ufn69FHH+VG6QA8p6PLQzds2GAuKT/xMXXqVMMwjm+18cADDxhRUVFGYGCgce211xo7duxweo/6+npj5syZRlhYmBEUFGSkpaUZe/bscaqpqqoypkyZYoSEhBghISHGlClTjOrqaqea3bt3G+PGjTOCgoKMsLAwY+bMmcaxY8fcPha22QDQEbm5uUZsbKzT7764uDgjNzfX20MDcJZwN3tYDOOEbbDPMTU1NbLZbLLb7cy6AXCLw+FQYWGhuYozOTmZmTMAbnM3e3CzdADoAH9/f6WkpHh7GAC6OG6WDgAA4GMIaAAAAD6GgAYAAOBjCGgAAAA+hoAGAADgYwhoAAAAPoaABgAA4GMIaAAAAD6GgAYAAOBjCGgAAAA+hoAGAADgYwhoAAAAPoaABgAdUF9fr5kzZ2rMmDGaOXOm6uvrvT0kAF2QxTAMw9uD8JaamhrZbDbZ7XaFhoZ6ezgAfFx6erpee+21Nu0TJ07UmjVrzvyAAJx13M0ezKABgBtaw5nVatW9996rr776Svfee6+sVqtee+01paene3uIALoQZtCYQQPgQn19vXr06CGr1ara2lpZrVazr7GxUSEhIWpsbNTRo0cVFBTkxZEC8HXMoAGAh8ybN0+SdPfddzuFM0myWq2aPXu2Ux0AdBYBDQBcKC0tlSTddttt7fbfeuutTnUA0FkENABw4aKLLpIk/eUvf2m3f/ny5U51ANBZXIPGNWgAXOAaNACewjVoAOAhQUFBmjhxohnG7rnnHn355Ze65557zHA2ceJEwhkAj2EGjRk0AG5iHzQAneVu9uh2BscEAGe1NWvWqL6+XvPmzVNpaakuuugi5eTkMHMGwOMIaADQAUFBQXrqqae8PQwAXRzXoAEAAPgYZtAAoAMcDocKCwtVUVGh6OhoJScny9/f39vDAtDFMIMGAG7Ky8tTfHy8UlNTlZmZqdTUVMXHxysvL8/bQwPQxTCDBgBuyMvLU0ZGhsaNG6d58+YpKChI9fX1euutt5SRkaHVq1dr0qRJ3h4mgC6CbTbYZgOACw6HQ/Hx8QoPD9ehQ4e0e/dus69///7q06ePqqqqVFpayulOAKfERrUA4CGFhYUqLy/Xhx9+qMGDB6uoqEi1tbUqKirS4MGD9eGHH2rXrl0qLCz09lABdBEENABw4euvv5YkjR07Vrm5uTp27Jhef/11HTt2TLm5uRo7dqxTHQB0FtegAYALhw4dkiTFxsZq4MCBKi8vN/tiY2N1ww03ONUBQGcxgwYALvTp00eS9MwzzyghIcHpFGdCQoKeffZZpzoA6CwCGgC4EBUV5fTcMAzzcao6APi+OMUJAG76wQ9+oJKSEiUlJZltcXFx+sEPfqDPP//ciyMD0NUQ0ADAhYMHD0qSPv/8c6WlpWnu3LnmPmgFBQXKz893qgOAziKgAYAL0dHRkqRFixbpueeeMwOZdHwGbeHChbrvvvvMOgDoLAIaALiQnJys2NhYbd68WV9++aXef/99816c11xzjSZPnqy4uDglJyd7e6gAuggWCQCAC/7+/lq8eLHy8/M1efJkBQYGKi0tTYGBgZo8ebLy8/P16KOPchcBAB7DDBoAuGHSpElavXq1srOz2ywS4D6cADyNe3FyL04AHeBwOFRYWGie4kxOTmbmDIDb3M0ezKABQAf4+/srJSXF28MA0MVxDRoAAICPIaABAAD4GAIaAACAjyGgAQAA+BgCGgAAgI8hoAEAAPgYttkAgA5obGzUkiVLVFZWpgEDBmjGjBmyWq3eHhaALoaABgBumj9/vhYvXqyWlhazLTs7W9nZ2XrkkUe8ODIAXY3HT3HGxsbKYrG0edx5552SpGnTprXpGz58uNN7NDQ0aNasWQoPD1dwcLAmTJigffv2OdVUV1crKytLNptNNptNWVlZOnz4sKcPBwAkHQ9nOTk5TuFMklpaWpSTk6P58+d7aWQAuiKPB7Rt27apoqLCfKxbt06S9OMf/9isueGGG5xq3nzzTaf3mD17tl599VWtWrVKmzZt0pEjR5SWliaHw2HWZGZmqri4WAUFBSooKFBxcbGysrI8fTgAoMbGRuXk5JjPhw8frrffftvpj8ucnBw1NjZ6Y3gAuiCPn+Ls06eP0/OHH35YAwYM0MiRI822wMBARUVFtft6u92u5cuX66WXXtL1118vSXr55ZfVr18/rV+/XmPGjNFnn32mgoICbdmyRcOGDZMkLVu2TCNGjNAXX3yhQYMGefqwAJzD/vjHP5o/19bWqmfPnpKkoqIiHTlyRCEhIWbd/fff75UxAuhaTusqzsbGRr388sv65S9/KYvFYrZv3LhRERERGjhwoKZPn66DBw+afdu3b1dTU5NGjx5ttsXExCghIUGbN2+WdPyXos1mM8OZdPwvWpvNZta0p6GhQTU1NU4PAHDl8ccflyRdfvnlZjhr1bNnTw0ZMsSpDgA667QGtDVr1ujw4cOaNm2a2TZ27FitWLFC77zzjhYvXqxt27bpuuuuU0NDgySpsrJSVqtVvXr1cnqvyMhIVVZWmjURERFtPi8iIsKsac+iRYvMa9ZsNpv69evngaME0NUdO3ZMkpSamtpuf3JyslMdAHTWaQ1oy5cv19ixYxUTE2O23XzzzRo3bpwSEhI0fvx4vfXWW/ryyy/1xhtvnPK9DMNwmoU78efvqjnZggULZLfbzcfevXu/x1EBONfExcVJkv785z+rubnZqa+5uVnPPPOMUx0AdNZpC2i7d+/W+vXrddttt52yLjo6Wv3791dpaakkKSoqSo2NjaqurnaqO3jwoCIjI82aAwcOtHmvQ4cOmTXtCQwMVGhoqNMDAFwpLCyUdDyMxcTEaOnSpdq/f7+WLl2qmJgYM7S11gFAZ522gPbCCy8oIiJC48aNO2VdVVWV9u7dq+joaEnS0KFDFRAQYK7+lKSKigqVlJQoKSlJkjRixAjZ7XZ98MEHZs3WrVtlt9vNGgDwlLCwMPOPv0OHDumOO+7Q+eefrzvuuEOHDh2SdPwyjLCwMG8OE0AXYjEMw/D0m7a0tCguLk4//elP9fDDD5vtR44c0YMPPqjJkycrOjpa5eXluu+++7Rnzx599tln5kqoX//618rPz9eLL76osLAwzZ07V1VVVdq+fbv8/f0lHb+Wbf/+/XruueckSbfffrv69++v119/3e1x1tTUyGazyW63M5sGwKXvmr0/8RpZADgVd7PHaZlBW79+vfbs2aNf/vKXTu3+/v7asWOHJk6cqIEDB2rq1KkaOHCgioqKzHAmHV8JlZ6erp/85Ce65ppr1KNHD73++utmOJOkFStWKDExUaNHj9bo0aM1ePBgvfTSS6fjcABA0vEFSlVVVUpISFBYWJgSEhJUVVVFOAPgcadlBu1swQwaAAA4k7w6gwYAAIDvj4AGAADgYzx+qycA6MocDocKCwtVUVGh6OhoJScnO10fCwCewAwaALgpLy9P8fHxSk1NVWZmplJTUxUfH6+8vDxvDw1AF0NAAwA35OXlKSMjQ4mJiSoqKlJtba2KioqUmJiojIwMQhoAj2IVJ6s4AbjgcDgUHx+vxMRErVmzRn5+//3btqWlRenp6SopKVFpaSmnOwGcEqs4AcBDCgsLzY21TwxnkuTn56cFCxZo165d3OoJgMcQ0ADAhYqKCklSQkJCu/2t7a11ANBZBDQAcKH1XsElJSXt9re2t9YBQGcR0ADAheTkZMXGxmrhwoVqaWlx6mtpadGiRYsUFxen5ORkL40QQFdDQAMAF/z9/bV48WLl5+crPT3daRVnenq68vPz9eijj7JAAIDHsFEtALhh0qRJWr16tbKzs5WUlGS2x8XFafXq1Zo0aZIXRwegq2GbDbbZANABjY2NWrJkicrKyjRgwADNmDFDVqvV28MCcJZwN3swgwYAbsrLy1N2drbKy8vNtieeeEKLFy9mBg2AR3ENGgC4gTsJADiTOMXJKU4ALnAnAQCewp0EAMBDuJMAgDONgAYALnAnAQBnGgENAFzgTgIAzjSuQeMaNAAunHgNWm5urt5//31VVFQoOjpa11xzjSZPnsw1aADcwjYbAOAhrXcSyMjIkM1mU319vdkXFBSkY8eOafXq1YQzAB7DKU4AcFN7JxwsFku77QDQGZzi5BQnABc4xQnAU9hmAwA8hG02AJxpXIMGAC60bp9RVlamW265Rbt37zb7+vfvrz/84Q9OdQDQWQQ0AHChdfuMn/3sZ236du/ebbazzQYAT+EUJwC4kJSUJIvFcsoai8WipKSkMzQiAF0dAQ0AXNiwYYPTSs2srCwVFxcrKyvLbDMMQxs2bPDG8AB0QaziZBUnABeuu+46M3z17dtX+/btM/v69eunvXv3SpJSU1P1zjvveGWMAM4OrOIEAA/ZsWOHJOmqq65qs42Gn5+frrzySqc6AOgsFgkAgAuBgYGSpG3btql79+5OfQcOHDBXdbbWAUBnMYMGAC7ccMMN5s+hoaFaunSp9u/fr6VLlzqdojixDgA6gxk0AHDhpptu0vLlyyVJBw8e1O233/6ddQDgCcygAYALW7Zs8WgdALhCQAMAN33XPmfsfwbA0whoAOBCSkqKpOMrNo8cOaI777xTo0eP1p133qkjR46Y9+dsrQOAzuIaNABwISUlRX369NGmTZt0yy236L777lNCQoJKSkp0yy23aNOmTYqIiCCgAfAYAhoAuODv769nn31WkydP1ttvv638/Hyzr0ePHpKkZ555ps0eaQDwfXGKEwDcMGnSJOXm5ioiIsKpPSIiQrm5uZo0aZKXRgagK+JWT9zqCUAH1NfXa968eSotLdVFF12knJwcBQUFeXtYAM4S3OoJADxs/vz5Cg0N1dNPP621a9fq6aefVmhoqObPn+/toQHoYghoAOCG+fPnKycnR71799ayZctUUVGhZcuWqXfv3srJySGkAfAoTnFyihOAC42NjQoODlbv3r21b98+dev23/VVzc3N6tu3r6qqqlRXVyer1erFkQLwdZziBAAPWbJkiZqbm/XQQw85hTNJ6tatm37/+9+rublZS5Ys8dIIAXQ1BDQAcKGsrEySlJaW1m5/a3trHQB0FgENAFwYMGCAJDntf3ai1vbWOgDoLK5B4xo0AC5wDRoAT+EaNADwEKvVqjlz5ujAgQPq27evli5dqv3792vp0qXq27evDhw4oDlz5hDOAHgMt3oCADc88sgjkqTHH39cd9xxh9nerVs3zZs3z+wHAE/gFCenOAF0QGNjo5YsWaKysjINGDBAM2bMYOYMgNu8dorzwQcflMVicXpERUWZ/YZh6MEHH1RMTIyCgoKUkpKinTt3Or1HQ0ODZs2apfDwcAUHB2vChAnat2+fU011dbWysrJks9lks9mUlZWlw4cPe/pwAMCJ1WrV7Nmz9eSTT2r27NmEMwCnxWm5Bu3SSy9VRUWF+dixY4fZ98gjj+ixxx7TU089pW3btikqKko/+tGPVFtba9bMnj1br776qlatWqVNmzbpyJEjSktLk8PhMGsyMzNVXFysgoICFRQUqLi4WFlZWafjcAAAAM6o03INWrdu3ZxmzVoZhqE//elP+s1vfqNJkyZJkv76178qMjJSr7zyiu644w7Z7XYtX75cL730kq6//npJ0ssvv6x+/fpp/fr1GjNmjD777DMVFBRoy5YtGjZsmCRp2bJlGjFihL744gsNGjTodBwWAADAGXFaZtBKS0sVExOjuLg43XLLLfq///s/SdKuXbtUWVmp0aNHm7WBgYEaOXKkNm/eLEnavn27mpqanGpiYmKUkJBg1hQVFclms5nhTJKGDx8um81m1rSnoaFBNTU1Tg8AAABf4/GANmzYMP3tb3/Tv//9by1btkyVlZVKSkpSVVWVKisrJUmRkZFOr4mMjDT7KisrZbVa1atXr1PWREREtPnsiIgIs6Y9ixYtMq9Zs9ls6tevX6eOFQAA4HTweEAbO3asJk+erMTERF1//fV64403JB0/ldnKYrE4vcYwjDZtJzu5pr16V++zYMEC2e1287F37163jgkAAOBMOu0b1QYHBysxMVGlpaXmdWknz3IdPHjQnFWLiopSY2OjqqurT1lz4MCBNp916NChNrNzJwoMDFRoaKjTAwAAwNec9oDW0NCgzz77TNHR0YqLi1NUVJTWrVtn9jc2Nurdd99VUlKSJGno0KEKCAhwqqmoqFBJSYlZM2LECNntdn3wwQdmzdatW2W3280aAACAs5XHV3HOnTtX48eP1wUXXKCDBw/qoYceUk1NjaZOnSqLxaLZs2dr4cKFuuiii3TRRRdp4cKF6tGjhzIzMyVJNptNt956q7Kzs9W7d2+FhYVp7ty55ilTSbr44ot1ww03aPr06XruueckSbfffrvS0tJYwQkAAM56Hg9o+/bt009/+lN988036tOnj4YPH64tW7aof//+kqT58+ervr5eM2bMUHV1tYYNG6a1a9cqJCTEfI/HH39c3bp1009+8hPV19dr1KhRevHFF+Xv72/WrFixQnfddZe52nPChAl66qmnPH04AAAAZxy3euJWTwAA4Azx2q2eAAAA0DkENAAAAB9DQAMAAPAxp+VenADQVTkcDhUWFqqiokLR0dFKTk52WsAEAJ7ADBoAuCkvL0/x8fFKTU1VZmamUlNTFR8fr7y8PG8PDUAXQ0ADADfk5eUpIyNDiYmJKioqUm1trYqKipSYmKiMjAxCGgCPYpsNttkA4ILD4VB8fLwSExO1Zs0a+fn992/blpYWpaenq6SkRKWlpZzuBHBKbLMBAB5SWFio8vJy3XfffU7hTJL8/Py0YMEC7dq1S4WFhV4aIYCuhoAGAC5UVFRIkhISEtrtb21vrQOAziKgAYAL0dHRkqSSkpJ2+1vbW+sAoLMIaADgQnJysmJjY7Vw4UK1tLQ49bW0tGjRokWKi4tTcnKyl0YIoKshoAGAC/7+/lq8eLHy8/OVnp7utIozPT1d+fn5evTRR1kgAMBj2KgWANwwadIkrV69WtnZ2UpKSjLb4+LitHr1ak2aNMmLowPQ1bDNBttsAOgA7iQAoDPczR7MoAFAB/j7+yslJcXbwwDQxXENGgAAgI8hoAEAAPgYAhoAAICPIaABAAD4GAIaAACAjyGgAQAA+BgCGgAAgI8hoAEAAPgYAhoAAICPIaABAAD4GG71BAAd0NjYqCVLlqisrEwDBgzQjBkzZLVavT0sAF0MAQ0A3DR//nw9/vjjam5uNtvmzZunOXPm6JFHHvHiyAB0NZziBAA3zJ8/Xzk5OWppaXFqb2lpUU5OjubPn++lkQHoighoAOBCY2OjFi9eLEkaO3asioqKVFtbq6KiIo0dO1aStHjxYjU2NnpzmAC6EAIaALjw5JNPqqWlRYMHD9a//vUvDR8+XD179tTw4cP1r3/9S4mJiWppadGTTz7p7aEC6CIIaADgwqZNmyRJCxculJ+f869NPz8/PfTQQ051ANBZBDQAcKFnz56SpF27drXbX15e7lQHAJ1lMQzD8PYgvKWmpkY2m012u12hoaHeHg4AH7V27VqNGTNGYWFh2r9/v4qKilRRUaHo6GiNGDFCMTEx+vbbb/Xvf/9bo0eP9vZwAfgwd7MHAY2ABsAFh8OhsLAw1dTUyM/Pz2klZ+vz0NBQffvtt/L39/fiSAH4OnezB6c4AcAFf39/3XHHHZLU7jYbknTHHXcQzgB4DDNozKABcMHhcCg+Pl7h4eE6ePCg9uzZY/b1799fffr0UVVVlUpLSwlpAE7J3ezBnQQAwIXCwkKVl5dr5cqVuuqqq1RYWGheg5acnKwPPvhASUlJKiwsVEpKireHC6ALIKABgAsVFRWSpISEBPn7+7cJYQkJCU51ANBZXIMGAC5ER0dLkkpKStrtb21vrQOAziKgAYALycnJio2N1cKFC9tdJLBo0SLFxcUpOTnZSyME0NUQ0ADABX9/fy1evFj5+flKT093uhdnenq68vPz9eijj7JAAIDHcA0aALhh0qRJWr16tbKzs5WUlGS2x8XFafXq1Zo0aZIXRwegq2GbDbbZANABR44cUVZWlsrKyjRgwAC99NJL3OIJgNvYZgMAPCw9PV2vvfaa+XzHjh0KCQnRxIkTtWbNGu8NDECXwzVoAOCG1nBmtVqVmZmpxx57TJmZmbJarXrttdeUnp7u7SEC6EI4xckpTgAu1NfXq0ePHurWrZuio6O1d+9es69fv36qqKhQc3Ozjh49qqCgIC+OFICv416cAOAh8+bNkyQ1NzdryJAhTqs4hwwZoubmZqc6AOgsAhoAuPDll19KklJTU5Wbm6tjx47p9ddf17Fjx5Sbm6vU1FSnOgDoLBYJAIALwcHBko5vSjtw4ECVl5ebfbGxserXr59THQB0FjNoAOBC6wKAd999V5deeqnTKc5LL71UhYWFTnUA0FkeD2iLFi3SVVddpZCQEEVERCg9PV1ffPGFU820adNksVicHsOHD3eqaWho0KxZsxQeHq7g4GBNmDBB+/btc6qprq5WVlaWbDabbDabsrKydPjwYU8fEoBzXN++fc2f165dqyeeeELPPfecnnjiCa1du7bdOgDoDI8HtHfffVd33nmntmzZonXr1qm5uVmjR49WXV2dU90NN9ygiooK8/Hmm2869c+ePVuvvvqqVq1apU2bNunIkSNKS0uTw+EwazIzM1VcXKyCggIVFBSouLhYWVlZnj4kAJB0/BRmU1OTVq1apblz52rVqlVqamri1CYAj/P4NWgFBQVOz1944QVFRERo+/btuvbaa832wMBARUVFtfsedrtdy5cv10svvaTrr79ekvTyyy+rX79+Wr9+vcaMGaPPPvtMBQUF2rJli4YNGyZJWrZsmUaMGKEvvvhCgwYN8vShAThHHTx4UJJUV1en3r1767zzzlN9fb2CgoJ0+PBhVVVVOdUBQGed9mvQ7Ha7JCksLMypfePGjYqIiNDAgQM1ffp0p19s27dvV1NTk0aPHm22xcTEKCEhQZs3b5YkFRUVyWazmeFMkoYPHy6bzWbWAIAnRERESJJ69eqlqqoqlZWVaf/+/SorK1NVVZV69erlVAcAnXVaV3EahqG7775bP/zhD5WQkGC2jx07Vj/+8Y/Vv39/7dq1S/fff7+uu+46bd++XYGBgaqsrJTVajV/6bWKjIxUZWWlJKmysrLdX4YRERFmzckaGhrU0NBgPq+pqfHEYQI4R1RXV8tisejE/b0tFouqq6u9OCoAXdFpDWgzZ87UJ598ok2bNjm133zzzebPCQkJuvLKK9W/f3+98cYbmjRp0ne+n2EYslgs5vMTf/6umhMtWrRIv/vd7zp6GADOcfv37zd/bi+gtT4/sQ4AOuO0neKcNWuW/vWvf2nDhg0uVzZFR0erf//+Ki0tlSRFRUWpsbGxzV+lBw8eVGRkpFlz4MCBNu916NAhs+ZkCxYskN1uNx8n3q4FAL7LiZdNBAYGOvV179693ToA6AyPBzTDMDRz5kzl5eXpnXfeUVxcnMvXVFVVae/evYqOjpYkDR06VAEBAVq3bp1ZU1FRoZKSEiUlJUmSRowYIbvdrg8++MCs2bp1q+x2u1lzssDAQIWGhjo9AMCV1pmx8847T9XV1dqwYYNeeeUVbdiwQd9++63OO+88pzoA6CyPB7Q777xTL7/8sl555RWFhISosrJSlZWVqq+vlyQdOXJEc+fOVVFRkcrLy7Vx40aNHz9e4eHhuummmyRJNptNt956q7Kzs/X222/r448/1s9+9jMlJiaaqzovvvhi3XDDDZo+fbq2bNmiLVu2aPr06UpLS2MFJwCPOnr0qCTp8OHDysjI0M6dO1VfX6+dO3cqIyPD3H+xtQ4AOsvj16A988wzkqSUlBSn9hdeeEHTpk2Tv7+/duzYob/97W86fPiwoqOjlZqaqr///e8KCQkx6x9//HF169ZNP/nJT1RfX69Ro0bpxRdflL+/v1mzYsUK3XXXXeZqzwkTJuipp57y9CEBOMddeeWVWr9+vQIDA/Xmm28qPz/f7PPz81NgYKAaGhp05ZVXenGUALoSi3Hi1a7nmJqaGtlsNtntdk53AvhOb7/9tjl7b7VaNXnyZF155ZX68MMPlZubq8bGRknS+vXrNWrUKG8OFYCPczd7ENAIaABcaGxsVFBQkFpaWr6zxs/PT/X19bJarWdwZADONu5mD26WDgAubN68+ZThTJJaWlpYxQnAYwhoAOBCRUWFR+sAwBUCGgC4cOJeZ56oAwBXCGgA4EJ2drZH6wDAFQIaALiwa9cuj9YBgCsENAAAAB9DQAMAAPAxBDQAAAAfQ0ADABd69erl0ToAcIWABgAuVFdXe7QOAFwhoAEAAPgYAhoAAICPIaABgAvdunXzaB0AuEJAAwAXevTo4dE6AHCFgAYALtTU1Hi0DgBcIaABAAD4GAIaAACAjyGgAQAA+BgCGgAAgI8hoAEAAPgYAhoAAICPIaABAAD4GAIaALhgsVg8WgcArhDQAAAAfAwBDQBcYAYNwJlGQAMAFwhoAM40AhoAuEBAA3CmEdAAwIXm5maP1gGAKwQ0AAAAH0NAAwAA8DEENAAAAB9DQAMAAPAxBDQAcIFVnADONAIaALgQEBDg0ToAcIWABgAuENAAnGkENABwobGx0aN1AOAKAQ0AXGhqavJoHQC4QkADAADwMQQ0AAAAH0NAAwAA8DEENAAAAB9DQAMAAPAxBDQAAAAfQ0ADAADwMQQ0AAAAH0NAAwAA8DHdvD0AADjd6hsdKjt05Ix8VsnX9k69fkCfngqy+ntoNADOVgQ0AF1e2aEjSnty0xn5rM5+Tv6sHyrhfJuHRgPgbGUxDMPw9iC8paamRjabTXa7XaGhod4eDoDTpLMzaIl9z3O7dse+w9/7cyRm0ICuzt3scdbPoC1ZskQ5OTmqqKjQpZdeqj/96U9KTk729rAAdNKub+pU19Ds7WGccZ48FRsc2E1x4cEeez8AZ85ZPYP297//XVlZWVqyZImuueYaPffcc/rLX/6iTz/9VBdccIHL1zODBvimzw9U6cZncr09DCcVL/6Py5roaU+cgZF0zJu/nqwfRPb29jAA/D/uZo+zOqANGzZMV1xxhZ555hmz7eKLL1Z6eroWLVrk8vUENMA3rd6xRb/7aLq3h9ElLLx6ucZffLW3hwHg/+nypzgbGxu1fft23XvvvU7to0eP1ubNm700KgCeUF8Xprpds7w9jHa1N5PmizNnrS66YYC3hwDgezhrA9o333wjh8OhyMhIp/bIyEhVVla2+5qGhgY1NDSYz2tqak7rGAF8P+MSYxXgd4MGRPRUUEDnL5g/1uTQvup6D4xM0sRx2vvtUS1e96WyfzRQ/cJ6eOZ9/5++vYLU3QPHLHENGnA2O2sDWiuLxeL03DCMNm2tFi1apN/97ndnYlgAOiEs2KpbrnZ9Ham7Sr62a/bfiz32fq0Wr/vS4+/JNhsApLM4oIWHh8vf37/NbNnBgwfbzKq1WrBgge6++27zeU1Njfr163daxwnA+wb06an8WT/02Pu1zsh5crar1YA+PT36fgDOTmdtQLNarRo6dKjWrVunm266yWxft26dJk6c2O5rAgMDFRgYeKaGCMBHBFn9PT4rdWWsR98OAJyctQFNku6++25lZWXpyiuv1IgRI7R06VLt2bNHv/rVr7w9NAAAgO/trA5oN998s6qqqvT73/9eFRUVSkhI0Jtvvqn+/ft7e2gAAADf21m9D1pnsQ8aAAA4k9zNHn5ncEwAAABwAwENAADAxxDQAAAAfAwBDQAAwMcQ0AAAAHwMAQ0AAMDHENAAAAB8DAENAADAxxDQAAAAfAwBDQAAwMcQ0AAAAHwMAQ0AAMDHENAAAAB8DAENAADAxxDQAAAAfAwBDQAAwMcQ0AAAAHxMN28PwJsMw5Ak1dTUeHkkAADgXNCaOVozyHc5pwNabW2tJKlfv35eHgkAADiX1NbWymazfWe/xXAV4bqwlpYW7d+/XyEhIbJYLN4eDoCzRE1Njfr166e9e/cqNDTU28MBcBYxDEO1tbWKiYmRn993X2l2Tgc0APg+ampqZLPZZLfbCWgATgsWCQAAAPgYAhoAAICPIaABQAcFBgbqgQceUGBgoLeHAqCL4ho0AAAAH8MMGgAAgI8hoAEAAPgYAhoAAICPIaABAAD4GAIagHPGtGnTZLFYZLFYFBAQoAsvvFBz585VXV2dt4cGAE7O6XtxAjj33HDDDXrhhRfU1NSkwsJC3Xbbbaqrq9Mzzzzj1XE5HA5ZLJZT3voFwLmD3wQAzimBgYGKiopSv379lJmZqSlTpmjNmjVqaGjQXXfdpYiICHXv3l0//OEPtW3bNvN1Q4cO1eLFi83n6enp6tatm2pqaiRJlZWVslgs+uKLLyRJjY2Nmj9/vs4//3wFBwdr2LBh2rhxo/n6F198Ueedd57y8/N1ySWXKDAwULt37z4zXwIAn0dAA3BOCwoKUlNTk+bPn6/c3Fz99a9/1UcffaT4+HiNGTNG3377rSQpJSXFDFiGYaiwsFC9evXSpk2bJEkbNmxQVFSUBg0aJEn6xS9+offff1+rVq3SJ598oh//+Me64YYbVFpaan720aNHtWjRIv3lL3/Rzp07FRERcWYPHoDPIqABOGd98MEHeuWVV5SamqpnnnlGOTk5Gjt2rC655BItW7ZMQUFBWr58uaTjAa2wsFAtLS365JNP5O/vr6ysLDO0bdy4USNHjpQklZWVaeXKlfrnP/+p5ORkDRgwQHPnztUPf/hDvfDCC+bnNzU1acmSJUpKStKgQYMUHBx8xr8DAL6JgAbgnJKfn6+ePXuqe/fuGjFihK699lrNmjVLTU1Nuuaaa8y6gIAAXX311frss88kSddee61qa2v18ccf691339XIkSOVmpqqd999V5JzQPvoo49kGIYGDhyonj17mo93331XZWVl5mdYrVYNHjz4DB49gLMFiwQAnFNaZ8sCAgIUExOjgIAA/ec//5EkWSwWp1rDMMw2m82mIUOGaOPGjdq8ebOuu+46JScnq7i4WKWlpfryyy+VkpIiSWppaZG/v7+2b98uf39/p/fs2bOn+XNQUFCbzwQAiRk0AOeY4OBgxcfHq3///goICJAkxcfHy2q1mteTScdPP3744Ye6+OKLzbaUlBRt2LBB7733nlJSUnTeeefpkksu0UMPPaSIiAiz9vLLL5fD4dDBgwcVHx/v9IiKijqzBwzgrERAA3DOCw4O1q9//WvNmzdPBQUF+vTTTzV9+nQdPXpUt956q1mXkpKigoICWSwWXXLJJWbbihUrzNObkjRw4EBNmTJFP//5z5WXl6ddu3Zp27Zt+uMf/6g333zzjB8fgLMPAQ0AJD388MOaPHmysrKydMUVV+irr77Sv//9b/Xq1cusufbaayVJI0eONE9Njhw5Ug6HwymgSdILL7ygn//858rOztagQYM0YcIEbd26Vf369TtzBwXgrGUxDMPw9iAAAADwX8ygAQAA+BgCGgAAgI8hoAEAAPgYAhoAAICPIaABAAD4GAIaAACAjyGgAQAA+BgCGgAAgI8hoAEAAPgYAhoAAICPIaABAAD4GAIaAACAj/n/Aeafh3oV9J37AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 700x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_sub['Power'].plot(y='age', kind='box', title='Мощность двигателя в л.с', figsize=(7, 7)); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Удалим мощность равную нулю и аномально высокую мощность:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub = df_sub.query('Power >= 4 & Power < 2000')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данные столбца Kilometer на гистограмме выглядят нормально."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    274467.000000\n",
       "mean     127575.136537\n",
       "std       36984.569540\n",
       "min        5000.000000\n",
       "25%      125000.000000\n",
       "50%      150000.000000\n",
       "75%      150000.000000\n",
       "max      150000.000000\n",
       "Name: Kilometer, dtype: float64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sub.Kilometer.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим категориальные данные:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['coupe', 'suv', 'small', 'sedan', 'convertible', 'bus', 'wagon',\n",
       "       'unknown', 'other'], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array(['manual', 'auto', 'unknown'], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array(['unknown', 'grand', 'golf', 'fabia', '3er', '2_reihe', 'c_max',\n",
       "       '3_reihe', 'passat', 'navara', 'twingo', 'a_klasse', 'scirocco',\n",
       "       '5er', 'civic', 'transporter', 'punto', 'e_klasse', 'clio',\n",
       "       'kadett', 'other', 'one', 'fortwo', '1er', 'b_klasse', 'a8',\n",
       "       'jetta', 'fiesta', 'c_klasse', 'micra', 'vito', 'sprinter',\n",
       "       'escort', 'forester', 'xc_reihe', 'scenic', 'a1', 'focus', 'a4',\n",
       "       'tt', 'astra', 'a6', 'jazz', 'polo', 'slk', '7er', 'combo', '80',\n",
       "       '147', 'z_reihe', 'sorento', 'ibiza', 'mustang', 'eos', 'touran',\n",
       "       'getz', 'insignia', 'ka', 'almera', 'megane', 'a3', 'lupo',\n",
       "       'caddy', 'mondeo', 'cordoba', 'colt', 'impreza', 'vectra',\n",
       "       'berlingo', 'tiguan', '6_reihe', 'c4', 'panda', 'up', 'i_reihe',\n",
       "       'ceed', 'kangoo', '5_reihe', 'yeti', 'octavia', 'zafira', 'mii',\n",
       "       'rx_reihe', 'corsa', '6er', 'modus', 'fox', 'matiz', 'beetle',\n",
       "       'rio', 'touareg', 'logan', 'spider', 'omega', 'cuore', 's_max',\n",
       "       'a2', 'galaxy', 'c3', 'viano', 's_klasse', '1_reihe', 'sharan',\n",
       "       'avensis', 'roomster', 'sl', 'kaefer', 'santa', 'leon', 'cooper',\n",
       "       '4_reihe', 'a5', 'sportage', 'laguna', 'ptcruiser', 'clk',\n",
       "       'primera', 'espace', 'exeo', '159', 'transit', 'juke', 'x_reihe',\n",
       "       'v40', 'carisma', 'accord', 'corolla', 'lanos', 'phaeton', 'verso',\n",
       "       'swift', 'rav', 'qashqai', 'picanto', 'boxster', 'superb', 'stilo',\n",
       "       'alhambra', 'roadster', 'galant', '90', 'signum', 'crossfire',\n",
       "       'agila', 'duster', 'v50', 'mx_reihe', '500', 'meriva', 'c_reihe',\n",
       "       'v_klasse', 'm_klasse', 'yaris', 'c5', 'aygo', 'cc', 'carnival',\n",
       "       '911', 'bora', 'forfour', 'cl', 'tigra', '156', '300c', '100',\n",
       "       'cr_reihe', 'spark', 'kuga', 'x_type', 'ducato', 's_type',\n",
       "       'x_trail', 'toledo', 'altea', 'arosa', 'voyager', 'v70', 'tucson',\n",
       "       'c1', 'citigo', 'jimny', 'cayenne', 'wrangler', 'lybra', 'lancer',\n",
       "       'captiva', 'fusion', 'discovery', 'freelander', 'sandero', 'q7',\n",
       "       'note', 'seicento', 'antara', 'sirion', '900', 'cherokee',\n",
       "       'clubman', 'defender', 'cx_reihe', 'legacy', 'pajero', 'auris',\n",
       "       'c2', 'niva', 's60', 'm_reihe', 'nubira', 'vivaro', 'g_klasse',\n",
       "       'justy', 'lodgy', 'range_rover', '601', '850', 'ypsilon', 'q3',\n",
       "       'serie_2', 'calibra', 'glk', 'charade', 'croma', 'outlander',\n",
       "       'doblo', 'musa', '9000', 'kalos', 'v60', 'bravo', 'r19', '200',\n",
       "       '145', 'b_max', 'range_rover_sport', 'aveo', 'move', 'rangerover',\n",
       "       'q5', 'range_rover_evoque', 'materia', 'terios', 'gl', 'kalina',\n",
       "       'elefantino', 'delta', 'i3', 'samara', 'amarok', 'kappa',\n",
       "       'serie_3'], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array(['gasoline', 'petrol', 'unknown', 'lpg', 'other', 'hybrid', 'cng',\n",
       "       'electric'], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array(['audi', 'jeep', 'volkswagen', 'skoda', 'bmw', 'peugeot', 'ford',\n",
       "       'mazda', 'nissan', 'renault', 'mercedes_benz', 'honda', 'fiat',\n",
       "       'opel', 'mini', 'smart', 'hyundai', 'subaru', 'volvo',\n",
       "       'mitsubishi', 'alfa_romeo', 'kia', 'seat', 'suzuki', 'lancia',\n",
       "       'citroen', 'toyota', 'chevrolet', 'dacia', 'sonstige_autos',\n",
       "       'daihatsu', 'chrysler', 'jaguar', 'daewoo', 'rover', 'porsche',\n",
       "       'saab', 'trabant', 'land_rover', 'lada'], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array(['yes', 'unknown', 'no'], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_sub.VehicleType.unique())\n",
    "display(df_sub.Gearbox.unique())\n",
    "display(df_sub.Model.unique())\n",
    "display(df_sub.FuelType.unique())\n",
    "display(df_sub.Brand.unique())\n",
    "display(df_sub.Repaired.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Еще раз проверим данные на наличие дубликатов и удалим их в случае необходимости:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36688"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sub.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sub = df_sub.drop_duplicates()\n",
    "df_sub.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вывод:**\n",
    "\n",
    "Данные состоят из 16 столбцов и 354369 строк. В нескольких столбцах содержались пропуски, общая доля которых довольно высока, заменили пропуски отдельной категорией unknown. \n",
    "Удалили 4  дубликата из данных.\n",
    "Подготовили данные к обучению, удалив неважные для обучения столбцы. Получили 9 столбцов с признаками и один с целевым признаком.\n",
    "Обработали анамольные значения в числовых данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение моделей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разделим данные на признаки и целевой признак:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df_sub.drop(['Price'], axis=1)\n",
    "target = df_sub['Price']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разделим данные на обучающую и тестовую выборки в соотношении 75:25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "R_STATE = 12345"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VehicleType</th>\n",
       "      <th>RegistrationYear</th>\n",
       "      <th>Gearbox</th>\n",
       "      <th>Power</th>\n",
       "      <th>Model</th>\n",
       "      <th>Kilometer</th>\n",
       "      <th>FuelType</th>\n",
       "      <th>Brand</th>\n",
       "      <th>Repaired</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>164326</th>\n",
       "      <td>small</td>\n",
       "      <td>2010</td>\n",
       "      <td>manual</td>\n",
       "      <td>60</td>\n",
       "      <td>fabia</td>\n",
       "      <td>50000</td>\n",
       "      <td>petrol</td>\n",
       "      <td>skoda</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135606</th>\n",
       "      <td>unknown</td>\n",
       "      <td>2016</td>\n",
       "      <td>manual</td>\n",
       "      <td>101</td>\n",
       "      <td>a4</td>\n",
       "      <td>150000</td>\n",
       "      <td>petrol</td>\n",
       "      <td>audi</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198806</th>\n",
       "      <td>sedan</td>\n",
       "      <td>2003</td>\n",
       "      <td>manual</td>\n",
       "      <td>179</td>\n",
       "      <td>golf</td>\n",
       "      <td>150000</td>\n",
       "      <td>petrol</td>\n",
       "      <td>volkswagen</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259323</th>\n",
       "      <td>small</td>\n",
       "      <td>1997</td>\n",
       "      <td>manual</td>\n",
       "      <td>75</td>\n",
       "      <td>civic</td>\n",
       "      <td>125000</td>\n",
       "      <td>petrol</td>\n",
       "      <td>honda</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26633</th>\n",
       "      <td>sedan</td>\n",
       "      <td>1998</td>\n",
       "      <td>manual</td>\n",
       "      <td>98</td>\n",
       "      <td>other</td>\n",
       "      <td>125000</td>\n",
       "      <td>petrol</td>\n",
       "      <td>chrysler</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116973</th>\n",
       "      <td>small</td>\n",
       "      <td>2008</td>\n",
       "      <td>manual</td>\n",
       "      <td>75</td>\n",
       "      <td>unknown</td>\n",
       "      <td>60000</td>\n",
       "      <td>petrol</td>\n",
       "      <td>chevrolet</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187470</th>\n",
       "      <td>sedan</td>\n",
       "      <td>1999</td>\n",
       "      <td>manual</td>\n",
       "      <td>116</td>\n",
       "      <td>focus</td>\n",
       "      <td>150000</td>\n",
       "      <td>petrol</td>\n",
       "      <td>ford</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182985</th>\n",
       "      <td>sedan</td>\n",
       "      <td>2005</td>\n",
       "      <td>auto</td>\n",
       "      <td>231</td>\n",
       "      <td>5er</td>\n",
       "      <td>150000</td>\n",
       "      <td>gasoline</td>\n",
       "      <td>bmw</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105210</th>\n",
       "      <td>sedan</td>\n",
       "      <td>2008</td>\n",
       "      <td>auto</td>\n",
       "      <td>200</td>\n",
       "      <td>golf</td>\n",
       "      <td>150000</td>\n",
       "      <td>petrol</td>\n",
       "      <td>volkswagen</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320656</th>\n",
       "      <td>small</td>\n",
       "      <td>2012</td>\n",
       "      <td>manual</td>\n",
       "      <td>75</td>\n",
       "      <td>polo</td>\n",
       "      <td>5000</td>\n",
       "      <td>gasoline</td>\n",
       "      <td>volkswagen</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>178334 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       VehicleType  RegistrationYear Gearbox  Power    Model  Kilometer  \\\n",
       "164326       small              2010  manual     60    fabia      50000   \n",
       "135606     unknown              2016  manual    101       a4     150000   \n",
       "198806       sedan              2003  manual    179     golf     150000   \n",
       "259323       small              1997  manual     75    civic     125000   \n",
       "26633        sedan              1998  manual     98    other     125000   \n",
       "...            ...               ...     ...    ...      ...        ...   \n",
       "116973       small              2008  manual     75  unknown      60000   \n",
       "187470       sedan              1999  manual    116    focus     150000   \n",
       "182985       sedan              2005    auto    231      5er     150000   \n",
       "105210       sedan              2008    auto    200     golf     150000   \n",
       "320656       small              2012  manual     75     polo       5000   \n",
       "\n",
       "        FuelType       Brand Repaired  \n",
       "164326    petrol       skoda       no  \n",
       "135606    petrol        audi       no  \n",
       "198806    petrol  volkswagen       no  \n",
       "259323    petrol       honda      yes  \n",
       "26633     petrol    chrysler       no  \n",
       "...          ...         ...      ...  \n",
       "116973    petrol   chevrolet       no  \n",
       "187470    petrol        ford       no  \n",
       "182985  gasoline         bmw      yes  \n",
       "105210    petrol  volkswagen       no  \n",
       "320656  gasoline  volkswagen       no  \n",
       "\n",
       "[178334 rows x 9 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VehicleType</th>\n",
       "      <th>RegistrationYear</th>\n",
       "      <th>Gearbox</th>\n",
       "      <th>Power</th>\n",
       "      <th>Model</th>\n",
       "      <th>Kilometer</th>\n",
       "      <th>FuelType</th>\n",
       "      <th>Brand</th>\n",
       "      <th>Repaired</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>43157</th>\n",
       "      <td>sedan</td>\n",
       "      <td>2000</td>\n",
       "      <td>auto</td>\n",
       "      <td>224</td>\n",
       "      <td>s_klasse</td>\n",
       "      <td>150000</td>\n",
       "      <td>petrol</td>\n",
       "      <td>mercedes_benz</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271088</th>\n",
       "      <td>wagon</td>\n",
       "      <td>2008</td>\n",
       "      <td>manual</td>\n",
       "      <td>87</td>\n",
       "      <td>logan</td>\n",
       "      <td>70000</td>\n",
       "      <td>petrol</td>\n",
       "      <td>dacia</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321102</th>\n",
       "      <td>small</td>\n",
       "      <td>2010</td>\n",
       "      <td>manual</td>\n",
       "      <td>95</td>\n",
       "      <td>colt</td>\n",
       "      <td>90000</td>\n",
       "      <td>unknown</td>\n",
       "      <td>mitsubishi</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225254</th>\n",
       "      <td>unknown</td>\n",
       "      <td>2000</td>\n",
       "      <td>manual</td>\n",
       "      <td>101</td>\n",
       "      <td>a3</td>\n",
       "      <td>150000</td>\n",
       "      <td>unknown</td>\n",
       "      <td>audi</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289901</th>\n",
       "      <td>wagon</td>\n",
       "      <td>1998</td>\n",
       "      <td>auto</td>\n",
       "      <td>110</td>\n",
       "      <td>sharan</td>\n",
       "      <td>5000</td>\n",
       "      <td>gasoline</td>\n",
       "      <td>volkswagen</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130106</th>\n",
       "      <td>bus</td>\n",
       "      <td>2011</td>\n",
       "      <td>manual</td>\n",
       "      <td>105</td>\n",
       "      <td>c_max</td>\n",
       "      <td>60000</td>\n",
       "      <td>petrol</td>\n",
       "      <td>ford</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87433</th>\n",
       "      <td>sedan</td>\n",
       "      <td>2006</td>\n",
       "      <td>manual</td>\n",
       "      <td>140</td>\n",
       "      <td>passat</td>\n",
       "      <td>150000</td>\n",
       "      <td>gasoline</td>\n",
       "      <td>volkswagen</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162068</th>\n",
       "      <td>bus</td>\n",
       "      <td>2001</td>\n",
       "      <td>manual</td>\n",
       "      <td>116</td>\n",
       "      <td>sharan</td>\n",
       "      <td>150000</td>\n",
       "      <td>gasoline</td>\n",
       "      <td>volkswagen</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168306</th>\n",
       "      <td>sedan</td>\n",
       "      <td>2006</td>\n",
       "      <td>manual</td>\n",
       "      <td>218</td>\n",
       "      <td>5er</td>\n",
       "      <td>150000</td>\n",
       "      <td>petrol</td>\n",
       "      <td>bmw</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259601</th>\n",
       "      <td>wagon</td>\n",
       "      <td>2008</td>\n",
       "      <td>manual</td>\n",
       "      <td>110</td>\n",
       "      <td>astra</td>\n",
       "      <td>125000</td>\n",
       "      <td>gasoline</td>\n",
       "      <td>opel</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>59445 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       VehicleType  RegistrationYear Gearbox  Power     Model  Kilometer  \\\n",
       "43157        sedan              2000    auto    224  s_klasse     150000   \n",
       "271088       wagon              2008  manual     87     logan      70000   \n",
       "321102       small              2010  manual     95      colt      90000   \n",
       "225254     unknown              2000  manual    101        a3     150000   \n",
       "289901       wagon              1998    auto    110    sharan       5000   \n",
       "...            ...               ...     ...    ...       ...        ...   \n",
       "130106         bus              2011  manual    105     c_max      60000   \n",
       "87433        sedan              2006  manual    140    passat     150000   \n",
       "162068         bus              2001  manual    116    sharan     150000   \n",
       "168306       sedan              2006  manual    218       5er     150000   \n",
       "259601       wagon              2008  manual    110     astra     125000   \n",
       "\n",
       "        FuelType          Brand Repaired  \n",
       "43157     petrol  mercedes_benz       no  \n",
       "271088    petrol          dacia       no  \n",
       "321102   unknown     mitsubishi       no  \n",
       "225254   unknown           audi  unknown  \n",
       "289901  gasoline     volkswagen       no  \n",
       "...          ...            ...      ...  \n",
       "130106    petrol           ford       no  \n",
       "87433   gasoline     volkswagen       no  \n",
       "162068  gasoline     volkswagen       no  \n",
       "168306    petrol            bmw       no  \n",
       "259601  gasoline           opel       no  \n",
       "\n",
       "[59445 rows x 9 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "164326     5900\n",
       "135606      750\n",
       "198806     4500\n",
       "259323     1400\n",
       "26633      1250\n",
       "          ...  \n",
       "116973     2650\n",
       "187470     1750\n",
       "182985     6500\n",
       "105210     8500\n",
       "320656    10789\n",
       "Name: Price, Length: 178334, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "43157      3600\n",
       "271088     4350\n",
       "321102     6000\n",
       "225254     1600\n",
       "289901     1690\n",
       "          ...  \n",
       "130106    10350\n",
       "87433      5800\n",
       "162068     4350\n",
       "168306     8150\n",
       "259601     6999\n",
       "Name: Price, Length: 59445, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "features_train, features_test, target_train, target_test = train_test_split(\n",
    "    features, target, test_size=0.25, random_state=R_STATE)\n",
    "\n",
    "display(features_train, features_test, target_train, target_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Преобразуем категориальные признаки в численные техникой OrdinalEncoder():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_cat = ['VehicleType', 'Model', 'FuelType', 'Brand', 'Repaired', 'Gearbox']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_oe = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_oe.fit(features_train[columns_cat])\n",
    "features_train_oe = features_train.copy()\n",
    "features_train_oe[columns_cat] = pd.DataFrame(encoder_oe.transform(features_train[columns_cat]), \n",
    "                                              columns=features_train[columns_cat].columns,\n",
    "                                              index=features_train_oe.index)\n",
    "features_train_oe = features_train_oe.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_ohe = OneHotEncoder(handle_unknown='ignore', drop='first', sparse=False)\n",
    "encoder_ohe.fit(features_train[columns_cat])\n",
    "features_train_ohe = features_train.copy()\n",
    "features_train_ohe[encoder_ohe.get_feature_names_out()] = encoder_ohe.transform(features_train[columns_cat])                                    \n",
    "features_train_ohe = features_train_ohe.drop(features_train[columns_cat], axis=1)\n",
    "features_train_ohe = features_train_ohe.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RegistrationYear</th>\n",
       "      <th>Power</th>\n",
       "      <th>Kilometer</th>\n",
       "      <th>VehicleType_convertible</th>\n",
       "      <th>VehicleType_coupe</th>\n",
       "      <th>VehicleType_other</th>\n",
       "      <th>VehicleType_sedan</th>\n",
       "      <th>VehicleType_small</th>\n",
       "      <th>VehicleType_suv</th>\n",
       "      <th>VehicleType_unknown</th>\n",
       "      <th>VehicleType_wagon</th>\n",
       "      <th>Model_145</th>\n",
       "      <th>Model_147</th>\n",
       "      <th>Model_156</th>\n",
       "      <th>Model_159</th>\n",
       "      <th>Model_1_reihe</th>\n",
       "      <th>Model_1er</th>\n",
       "      <th>Model_200</th>\n",
       "      <th>Model_2_reihe</th>\n",
       "      <th>Model_300c</th>\n",
       "      <th>Model_3_reihe</th>\n",
       "      <th>Model_3er</th>\n",
       "      <th>Model_4_reihe</th>\n",
       "      <th>Model_500</th>\n",
       "      <th>Model_5_reihe</th>\n",
       "      <th>Model_5er</th>\n",
       "      <th>Model_601</th>\n",
       "      <th>Model_6_reihe</th>\n",
       "      <th>Model_6er</th>\n",
       "      <th>Model_7er</th>\n",
       "      <th>Model_80</th>\n",
       "      <th>Model_850</th>\n",
       "      <th>Model_90</th>\n",
       "      <th>Model_900</th>\n",
       "      <th>Model_9000</th>\n",
       "      <th>Model_911</th>\n",
       "      <th>Model_a1</th>\n",
       "      <th>Model_a2</th>\n",
       "      <th>Model_a3</th>\n",
       "      <th>Model_a4</th>\n",
       "      <th>Model_a5</th>\n",
       "      <th>Model_a6</th>\n",
       "      <th>Model_a8</th>\n",
       "      <th>Model_a_klasse</th>\n",
       "      <th>Model_accord</th>\n",
       "      <th>Model_agila</th>\n",
       "      <th>Model_alhambra</th>\n",
       "      <th>Model_almera</th>\n",
       "      <th>Model_altea</th>\n",
       "      <th>Model_amarok</th>\n",
       "      <th>Model_antara</th>\n",
       "      <th>Model_arosa</th>\n",
       "      <th>Model_astra</th>\n",
       "      <th>Model_auris</th>\n",
       "      <th>Model_avensis</th>\n",
       "      <th>Model_aveo</th>\n",
       "      <th>Model_aygo</th>\n",
       "      <th>Model_b_klasse</th>\n",
       "      <th>Model_b_max</th>\n",
       "      <th>Model_beetle</th>\n",
       "      <th>Model_berlingo</th>\n",
       "      <th>Model_bora</th>\n",
       "      <th>Model_boxster</th>\n",
       "      <th>Model_bravo</th>\n",
       "      <th>Model_c1</th>\n",
       "      <th>Model_c2</th>\n",
       "      <th>Model_c3</th>\n",
       "      <th>Model_c4</th>\n",
       "      <th>Model_c5</th>\n",
       "      <th>Model_c_klasse</th>\n",
       "      <th>Model_c_max</th>\n",
       "      <th>Model_c_reihe</th>\n",
       "      <th>Model_caddy</th>\n",
       "      <th>Model_calibra</th>\n",
       "      <th>Model_captiva</th>\n",
       "      <th>Model_carisma</th>\n",
       "      <th>Model_carnival</th>\n",
       "      <th>Model_cayenne</th>\n",
       "      <th>Model_cc</th>\n",
       "      <th>Model_ceed</th>\n",
       "      <th>Model_charade</th>\n",
       "      <th>Model_cherokee</th>\n",
       "      <th>Model_citigo</th>\n",
       "      <th>Model_civic</th>\n",
       "      <th>Model_cl</th>\n",
       "      <th>Model_clio</th>\n",
       "      <th>Model_clk</th>\n",
       "      <th>Model_clubman</th>\n",
       "      <th>Model_colt</th>\n",
       "      <th>Model_combo</th>\n",
       "      <th>Model_cooper</th>\n",
       "      <th>Model_cordoba</th>\n",
       "      <th>Model_corolla</th>\n",
       "      <th>Model_corsa</th>\n",
       "      <th>Model_cr_reihe</th>\n",
       "      <th>Model_croma</th>\n",
       "      <th>Model_crossfire</th>\n",
       "      <th>Model_cuore</th>\n",
       "      <th>Model_cx_reihe</th>\n",
       "      <th>Model_defender</th>\n",
       "      <th>Model_delta</th>\n",
       "      <th>Model_discovery</th>\n",
       "      <th>Model_doblo</th>\n",
       "      <th>Model_ducato</th>\n",
       "      <th>Model_duster</th>\n",
       "      <th>Model_e_klasse</th>\n",
       "      <th>Model_elefantino</th>\n",
       "      <th>Model_eos</th>\n",
       "      <th>Model_escort</th>\n",
       "      <th>Model_espace</th>\n",
       "      <th>Model_exeo</th>\n",
       "      <th>Model_fabia</th>\n",
       "      <th>Model_fiesta</th>\n",
       "      <th>Model_focus</th>\n",
       "      <th>Model_forester</th>\n",
       "      <th>Model_forfour</th>\n",
       "      <th>Model_fortwo</th>\n",
       "      <th>Model_fox</th>\n",
       "      <th>Model_freelander</th>\n",
       "      <th>Model_fusion</th>\n",
       "      <th>Model_g_klasse</th>\n",
       "      <th>Model_galant</th>\n",
       "      <th>Model_galaxy</th>\n",
       "      <th>Model_getz</th>\n",
       "      <th>Model_gl</th>\n",
       "      <th>Model_glk</th>\n",
       "      <th>Model_golf</th>\n",
       "      <th>Model_grand</th>\n",
       "      <th>Model_i3</th>\n",
       "      <th>Model_i_reihe</th>\n",
       "      <th>Model_ibiza</th>\n",
       "      <th>Model_impreza</th>\n",
       "      <th>Model_insignia</th>\n",
       "      <th>Model_jazz</th>\n",
       "      <th>Model_jetta</th>\n",
       "      <th>Model_jimny</th>\n",
       "      <th>Model_juke</th>\n",
       "      <th>Model_justy</th>\n",
       "      <th>Model_ka</th>\n",
       "      <th>Model_kadett</th>\n",
       "      <th>Model_kaefer</th>\n",
       "      <th>Model_kalina</th>\n",
       "      <th>Model_kalos</th>\n",
       "      <th>Model_kangoo</th>\n",
       "      <th>Model_kappa</th>\n",
       "      <th>Model_kuga</th>\n",
       "      <th>Model_laguna</th>\n",
       "      <th>Model_lancer</th>\n",
       "      <th>Model_lanos</th>\n",
       "      <th>Model_legacy</th>\n",
       "      <th>Model_leon</th>\n",
       "      <th>Model_lodgy</th>\n",
       "      <th>Model_logan</th>\n",
       "      <th>Model_lupo</th>\n",
       "      <th>Model_lybra</th>\n",
       "      <th>Model_m_klasse</th>\n",
       "      <th>Model_m_reihe</th>\n",
       "      <th>Model_materia</th>\n",
       "      <th>Model_matiz</th>\n",
       "      <th>Model_megane</th>\n",
       "      <th>Model_meriva</th>\n",
       "      <th>Model_micra</th>\n",
       "      <th>Model_mii</th>\n",
       "      <th>Model_modus</th>\n",
       "      <th>Model_mondeo</th>\n",
       "      <th>Model_move</th>\n",
       "      <th>Model_musa</th>\n",
       "      <th>Model_mustang</th>\n",
       "      <th>Model_mx_reihe</th>\n",
       "      <th>Model_navara</th>\n",
       "      <th>Model_niva</th>\n",
       "      <th>Model_note</th>\n",
       "      <th>Model_nubira</th>\n",
       "      <th>Model_octavia</th>\n",
       "      <th>Model_omega</th>\n",
       "      <th>Model_one</th>\n",
       "      <th>Model_other</th>\n",
       "      <th>Model_outlander</th>\n",
       "      <th>Model_pajero</th>\n",
       "      <th>Model_panda</th>\n",
       "      <th>Model_passat</th>\n",
       "      <th>Model_phaeton</th>\n",
       "      <th>Model_picanto</th>\n",
       "      <th>Model_polo</th>\n",
       "      <th>Model_primera</th>\n",
       "      <th>Model_ptcruiser</th>\n",
       "      <th>Model_punto</th>\n",
       "      <th>Model_q3</th>\n",
       "      <th>Model_q5</th>\n",
       "      <th>Model_q7</th>\n",
       "      <th>Model_qashqai</th>\n",
       "      <th>Model_r19</th>\n",
       "      <th>Model_range_rover</th>\n",
       "      <th>Model_range_rover_sport</th>\n",
       "      <th>Model_rangerover</th>\n",
       "      <th>Model_rav</th>\n",
       "      <th>Model_rio</th>\n",
       "      <th>Model_roadster</th>\n",
       "      <th>Model_roomster</th>\n",
       "      <th>Model_rx_reihe</th>\n",
       "      <th>Model_s60</th>\n",
       "      <th>Model_s_klasse</th>\n",
       "      <th>Model_s_max</th>\n",
       "      <th>Model_s_type</th>\n",
       "      <th>Model_samara</th>\n",
       "      <th>Model_sandero</th>\n",
       "      <th>Model_santa</th>\n",
       "      <th>Model_scenic</th>\n",
       "      <th>Model_scirocco</th>\n",
       "      <th>Model_seicento</th>\n",
       "      <th>Model_serie_2</th>\n",
       "      <th>Model_serie_3</th>\n",
       "      <th>Model_sharan</th>\n",
       "      <th>Model_signum</th>\n",
       "      <th>Model_sirion</th>\n",
       "      <th>Model_sl</th>\n",
       "      <th>Model_slk</th>\n",
       "      <th>Model_sorento</th>\n",
       "      <th>Model_spark</th>\n",
       "      <th>Model_spider</th>\n",
       "      <th>Model_sportage</th>\n",
       "      <th>Model_sprinter</th>\n",
       "      <th>Model_stilo</th>\n",
       "      <th>Model_superb</th>\n",
       "      <th>Model_swift</th>\n",
       "      <th>Model_terios</th>\n",
       "      <th>Model_tigra</th>\n",
       "      <th>Model_tiguan</th>\n",
       "      <th>Model_toledo</th>\n",
       "      <th>Model_touareg</th>\n",
       "      <th>Model_touran</th>\n",
       "      <th>Model_transit</th>\n",
       "      <th>Model_transporter</th>\n",
       "      <th>Model_tt</th>\n",
       "      <th>Model_tucson</th>\n",
       "      <th>Model_twingo</th>\n",
       "      <th>Model_unknown</th>\n",
       "      <th>Model_up</th>\n",
       "      <th>Model_v40</th>\n",
       "      <th>Model_v50</th>\n",
       "      <th>Model_v60</th>\n",
       "      <th>Model_v70</th>\n",
       "      <th>Model_v_klasse</th>\n",
       "      <th>Model_vectra</th>\n",
       "      <th>Model_verso</th>\n",
       "      <th>Model_viano</th>\n",
       "      <th>Model_vito</th>\n",
       "      <th>Model_vivaro</th>\n",
       "      <th>Model_voyager</th>\n",
       "      <th>Model_wrangler</th>\n",
       "      <th>Model_x_reihe</th>\n",
       "      <th>Model_x_trail</th>\n",
       "      <th>Model_x_type</th>\n",
       "      <th>Model_xc_reihe</th>\n",
       "      <th>Model_yaris</th>\n",
       "      <th>Model_yeti</th>\n",
       "      <th>Model_ypsilon</th>\n",
       "      <th>Model_z_reihe</th>\n",
       "      <th>Model_zafira</th>\n",
       "      <th>FuelType_electric</th>\n",
       "      <th>FuelType_gasoline</th>\n",
       "      <th>FuelType_hybrid</th>\n",
       "      <th>FuelType_lpg</th>\n",
       "      <th>FuelType_other</th>\n",
       "      <th>FuelType_petrol</th>\n",
       "      <th>FuelType_unknown</th>\n",
       "      <th>Brand_audi</th>\n",
       "      <th>Brand_bmw</th>\n",
       "      <th>Brand_chevrolet</th>\n",
       "      <th>Brand_chrysler</th>\n",
       "      <th>Brand_citroen</th>\n",
       "      <th>Brand_dacia</th>\n",
       "      <th>Brand_daewoo</th>\n",
       "      <th>Brand_daihatsu</th>\n",
       "      <th>Brand_fiat</th>\n",
       "      <th>Brand_ford</th>\n",
       "      <th>Brand_honda</th>\n",
       "      <th>Brand_hyundai</th>\n",
       "      <th>Brand_jaguar</th>\n",
       "      <th>Brand_jeep</th>\n",
       "      <th>Brand_kia</th>\n",
       "      <th>Brand_lada</th>\n",
       "      <th>Brand_lancia</th>\n",
       "      <th>Brand_land_rover</th>\n",
       "      <th>Brand_mazda</th>\n",
       "      <th>Brand_mercedes_benz</th>\n",
       "      <th>Brand_mini</th>\n",
       "      <th>Brand_mitsubishi</th>\n",
       "      <th>Brand_nissan</th>\n",
       "      <th>Brand_opel</th>\n",
       "      <th>Brand_peugeot</th>\n",
       "      <th>Brand_porsche</th>\n",
       "      <th>Brand_renault</th>\n",
       "      <th>Brand_rover</th>\n",
       "      <th>Brand_saab</th>\n",
       "      <th>Brand_seat</th>\n",
       "      <th>Brand_skoda</th>\n",
       "      <th>Brand_smart</th>\n",
       "      <th>Brand_sonstige_autos</th>\n",
       "      <th>Brand_subaru</th>\n",
       "      <th>Brand_suzuki</th>\n",
       "      <th>Brand_toyota</th>\n",
       "      <th>Brand_trabant</th>\n",
       "      <th>Brand_volkswagen</th>\n",
       "      <th>Brand_volvo</th>\n",
       "      <th>Repaired_unknown</th>\n",
       "      <th>Repaired_yes</th>\n",
       "      <th>Gearbox_manual</th>\n",
       "      <th>Gearbox_unknown</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010</td>\n",
       "      <td>60</td>\n",
       "      <td>50000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016</td>\n",
       "      <td>101</td>\n",
       "      <td>150000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2003</td>\n",
       "      <td>179</td>\n",
       "      <td>150000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1997</td>\n",
       "      <td>75</td>\n",
       "      <td>125000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1998</td>\n",
       "      <td>98</td>\n",
       "      <td>125000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178329</th>\n",
       "      <td>2008</td>\n",
       "      <td>75</td>\n",
       "      <td>60000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178330</th>\n",
       "      <td>1999</td>\n",
       "      <td>116</td>\n",
       "      <td>150000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178331</th>\n",
       "      <td>2005</td>\n",
       "      <td>231</td>\n",
       "      <td>150000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178332</th>\n",
       "      <td>2008</td>\n",
       "      <td>200</td>\n",
       "      <td>150000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178333</th>\n",
       "      <td>2012</td>\n",
       "      <td>75</td>\n",
       "      <td>5000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>178334 rows × 309 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        RegistrationYear  Power  Kilometer  VehicleType_convertible  \\\n",
       "0                   2010     60      50000                      0.0   \n",
       "1                   2016    101     150000                      0.0   \n",
       "2                   2003    179     150000                      0.0   \n",
       "3                   1997     75     125000                      0.0   \n",
       "4                   1998     98     125000                      0.0   \n",
       "...                  ...    ...        ...                      ...   \n",
       "178329              2008     75      60000                      0.0   \n",
       "178330              1999    116     150000                      0.0   \n",
       "178331              2005    231     150000                      0.0   \n",
       "178332              2008    200     150000                      0.0   \n",
       "178333              2012     75       5000                      0.0   \n",
       "\n",
       "        VehicleType_coupe  VehicleType_other  VehicleType_sedan  \\\n",
       "0                     0.0                0.0                0.0   \n",
       "1                     0.0                0.0                0.0   \n",
       "2                     0.0                0.0                1.0   \n",
       "3                     0.0                0.0                0.0   \n",
       "4                     0.0                0.0                1.0   \n",
       "...                   ...                ...                ...   \n",
       "178329                0.0                0.0                0.0   \n",
       "178330                0.0                0.0                1.0   \n",
       "178331                0.0                0.0                1.0   \n",
       "178332                0.0                0.0                1.0   \n",
       "178333                0.0                0.0                0.0   \n",
       "\n",
       "        VehicleType_small  VehicleType_suv  VehicleType_unknown  \\\n",
       "0                     1.0              0.0                  0.0   \n",
       "1                     0.0              0.0                  1.0   \n",
       "2                     0.0              0.0                  0.0   \n",
       "3                     1.0              0.0                  0.0   \n",
       "4                     0.0              0.0                  0.0   \n",
       "...                   ...              ...                  ...   \n",
       "178329                1.0              0.0                  0.0   \n",
       "178330                0.0              0.0                  0.0   \n",
       "178331                0.0              0.0                  0.0   \n",
       "178332                0.0              0.0                  0.0   \n",
       "178333                1.0              0.0                  0.0   \n",
       "\n",
       "        VehicleType_wagon  Model_145  Model_147  Model_156  Model_159  \\\n",
       "0                     0.0        0.0        0.0        0.0        0.0   \n",
       "1                     0.0        0.0        0.0        0.0        0.0   \n",
       "2                     0.0        0.0        0.0        0.0        0.0   \n",
       "3                     0.0        0.0        0.0        0.0        0.0   \n",
       "4                     0.0        0.0        0.0        0.0        0.0   \n",
       "...                   ...        ...        ...        ...        ...   \n",
       "178329                0.0        0.0        0.0        0.0        0.0   \n",
       "178330                0.0        0.0        0.0        0.0        0.0   \n",
       "178331                0.0        0.0        0.0        0.0        0.0   \n",
       "178332                0.0        0.0        0.0        0.0        0.0   \n",
       "178333                0.0        0.0        0.0        0.0        0.0   \n",
       "\n",
       "        Model_1_reihe  Model_1er  Model_200  Model_2_reihe  Model_300c  \\\n",
       "0                 0.0        0.0        0.0            0.0         0.0   \n",
       "1                 0.0        0.0        0.0            0.0         0.0   \n",
       "2                 0.0        0.0        0.0            0.0         0.0   \n",
       "3                 0.0        0.0        0.0            0.0         0.0   \n",
       "4                 0.0        0.0        0.0            0.0         0.0   \n",
       "...               ...        ...        ...            ...         ...   \n",
       "178329            0.0        0.0        0.0            0.0         0.0   \n",
       "178330            0.0        0.0        0.0            0.0         0.0   \n",
       "178331            0.0        0.0        0.0            0.0         0.0   \n",
       "178332            0.0        0.0        0.0            0.0         0.0   \n",
       "178333            0.0        0.0        0.0            0.0         0.0   \n",
       "\n",
       "        Model_3_reihe  Model_3er  Model_4_reihe  Model_500  Model_5_reihe  \\\n",
       "0                 0.0        0.0            0.0        0.0            0.0   \n",
       "1                 0.0        0.0            0.0        0.0            0.0   \n",
       "2                 0.0        0.0            0.0        0.0            0.0   \n",
       "3                 0.0        0.0            0.0        0.0            0.0   \n",
       "4                 0.0        0.0            0.0        0.0            0.0   \n",
       "...               ...        ...            ...        ...            ...   \n",
       "178329            0.0        0.0            0.0        0.0            0.0   \n",
       "178330            0.0        0.0            0.0        0.0            0.0   \n",
       "178331            0.0        0.0            0.0        0.0            0.0   \n",
       "178332            0.0        0.0            0.0        0.0            0.0   \n",
       "178333            0.0        0.0            0.0        0.0            0.0   \n",
       "\n",
       "        Model_5er  Model_601  Model_6_reihe  Model_6er  Model_7er  Model_80  \\\n",
       "0             0.0        0.0            0.0        0.0        0.0       0.0   \n",
       "1             0.0        0.0            0.0        0.0        0.0       0.0   \n",
       "2             0.0        0.0            0.0        0.0        0.0       0.0   \n",
       "3             0.0        0.0            0.0        0.0        0.0       0.0   \n",
       "4             0.0        0.0            0.0        0.0        0.0       0.0   \n",
       "...           ...        ...            ...        ...        ...       ...   \n",
       "178329        0.0        0.0            0.0        0.0        0.0       0.0   \n",
       "178330        0.0        0.0            0.0        0.0        0.0       0.0   \n",
       "178331        1.0        0.0            0.0        0.0        0.0       0.0   \n",
       "178332        0.0        0.0            0.0        0.0        0.0       0.0   \n",
       "178333        0.0        0.0            0.0        0.0        0.0       0.0   \n",
       "\n",
       "        Model_850  Model_90  Model_900  Model_9000  Model_911  Model_a1  \\\n",
       "0             0.0       0.0        0.0         0.0        0.0       0.0   \n",
       "1             0.0       0.0        0.0         0.0        0.0       0.0   \n",
       "2             0.0       0.0        0.0         0.0        0.0       0.0   \n",
       "3             0.0       0.0        0.0         0.0        0.0       0.0   \n",
       "4             0.0       0.0        0.0         0.0        0.0       0.0   \n",
       "...           ...       ...        ...         ...        ...       ...   \n",
       "178329        0.0       0.0        0.0         0.0        0.0       0.0   \n",
       "178330        0.0       0.0        0.0         0.0        0.0       0.0   \n",
       "178331        0.0       0.0        0.0         0.0        0.0       0.0   \n",
       "178332        0.0       0.0        0.0         0.0        0.0       0.0   \n",
       "178333        0.0       0.0        0.0         0.0        0.0       0.0   \n",
       "\n",
       "        Model_a2  Model_a3  Model_a4  Model_a5  Model_a6  Model_a8  \\\n",
       "0            0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "1            0.0       0.0       1.0       0.0       0.0       0.0   \n",
       "2            0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "3            0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "4            0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "...          ...       ...       ...       ...       ...       ...   \n",
       "178329       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "178330       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "178331       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "178332       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "178333       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "\n",
       "        Model_a_klasse  Model_accord  Model_agila  Model_alhambra  \\\n",
       "0                  0.0           0.0          0.0             0.0   \n",
       "1                  0.0           0.0          0.0             0.0   \n",
       "2                  0.0           0.0          0.0             0.0   \n",
       "3                  0.0           0.0          0.0             0.0   \n",
       "4                  0.0           0.0          0.0             0.0   \n",
       "...                ...           ...          ...             ...   \n",
       "178329             0.0           0.0          0.0             0.0   \n",
       "178330             0.0           0.0          0.0             0.0   \n",
       "178331             0.0           0.0          0.0             0.0   \n",
       "178332             0.0           0.0          0.0             0.0   \n",
       "178333             0.0           0.0          0.0             0.0   \n",
       "\n",
       "        Model_almera  Model_altea  Model_amarok  Model_antara  Model_arosa  \\\n",
       "0                0.0          0.0           0.0           0.0          0.0   \n",
       "1                0.0          0.0           0.0           0.0          0.0   \n",
       "2                0.0          0.0           0.0           0.0          0.0   \n",
       "3                0.0          0.0           0.0           0.0          0.0   \n",
       "4                0.0          0.0           0.0           0.0          0.0   \n",
       "...              ...          ...           ...           ...          ...   \n",
       "178329           0.0          0.0           0.0           0.0          0.0   \n",
       "178330           0.0          0.0           0.0           0.0          0.0   \n",
       "178331           0.0          0.0           0.0           0.0          0.0   \n",
       "178332           0.0          0.0           0.0           0.0          0.0   \n",
       "178333           0.0          0.0           0.0           0.0          0.0   \n",
       "\n",
       "        Model_astra  Model_auris  Model_avensis  Model_aveo  Model_aygo  \\\n",
       "0               0.0          0.0            0.0         0.0         0.0   \n",
       "1               0.0          0.0            0.0         0.0         0.0   \n",
       "2               0.0          0.0            0.0         0.0         0.0   \n",
       "3               0.0          0.0            0.0         0.0         0.0   \n",
       "4               0.0          0.0            0.0         0.0         0.0   \n",
       "...             ...          ...            ...         ...         ...   \n",
       "178329          0.0          0.0            0.0         0.0         0.0   \n",
       "178330          0.0          0.0            0.0         0.0         0.0   \n",
       "178331          0.0          0.0            0.0         0.0         0.0   \n",
       "178332          0.0          0.0            0.0         0.0         0.0   \n",
       "178333          0.0          0.0            0.0         0.0         0.0   \n",
       "\n",
       "        Model_b_klasse  Model_b_max  Model_beetle  Model_berlingo  Model_bora  \\\n",
       "0                  0.0          0.0           0.0             0.0         0.0   \n",
       "1                  0.0          0.0           0.0             0.0         0.0   \n",
       "2                  0.0          0.0           0.0             0.0         0.0   \n",
       "3                  0.0          0.0           0.0             0.0         0.0   \n",
       "4                  0.0          0.0           0.0             0.0         0.0   \n",
       "...                ...          ...           ...             ...         ...   \n",
       "178329             0.0          0.0           0.0             0.0         0.0   \n",
       "178330             0.0          0.0           0.0             0.0         0.0   \n",
       "178331             0.0          0.0           0.0             0.0         0.0   \n",
       "178332             0.0          0.0           0.0             0.0         0.0   \n",
       "178333             0.0          0.0           0.0             0.0         0.0   \n",
       "\n",
       "        Model_boxster  Model_bravo  Model_c1  Model_c2  Model_c3  Model_c4  \\\n",
       "0                 0.0          0.0       0.0       0.0       0.0       0.0   \n",
       "1                 0.0          0.0       0.0       0.0       0.0       0.0   \n",
       "2                 0.0          0.0       0.0       0.0       0.0       0.0   \n",
       "3                 0.0          0.0       0.0       0.0       0.0       0.0   \n",
       "4                 0.0          0.0       0.0       0.0       0.0       0.0   \n",
       "...               ...          ...       ...       ...       ...       ...   \n",
       "178329            0.0          0.0       0.0       0.0       0.0       0.0   \n",
       "178330            0.0          0.0       0.0       0.0       0.0       0.0   \n",
       "178331            0.0          0.0       0.0       0.0       0.0       0.0   \n",
       "178332            0.0          0.0       0.0       0.0       0.0       0.0   \n",
       "178333            0.0          0.0       0.0       0.0       0.0       0.0   \n",
       "\n",
       "        Model_c5  Model_c_klasse  Model_c_max  Model_c_reihe  Model_caddy  \\\n",
       "0            0.0             0.0          0.0            0.0          0.0   \n",
       "1            0.0             0.0          0.0            0.0          0.0   \n",
       "2            0.0             0.0          0.0            0.0          0.0   \n",
       "3            0.0             0.0          0.0            0.0          0.0   \n",
       "4            0.0             0.0          0.0            0.0          0.0   \n",
       "...          ...             ...          ...            ...          ...   \n",
       "178329       0.0             0.0          0.0            0.0          0.0   \n",
       "178330       0.0             0.0          0.0            0.0          0.0   \n",
       "178331       0.0             0.0          0.0            0.0          0.0   \n",
       "178332       0.0             0.0          0.0            0.0          0.0   \n",
       "178333       0.0             0.0          0.0            0.0          0.0   \n",
       "\n",
       "        Model_calibra  Model_captiva  Model_carisma  Model_carnival  \\\n",
       "0                 0.0            0.0            0.0             0.0   \n",
       "1                 0.0            0.0            0.0             0.0   \n",
       "2                 0.0            0.0            0.0             0.0   \n",
       "3                 0.0            0.0            0.0             0.0   \n",
       "4                 0.0            0.0            0.0             0.0   \n",
       "...               ...            ...            ...             ...   \n",
       "178329            0.0            0.0            0.0             0.0   \n",
       "178330            0.0            0.0            0.0             0.0   \n",
       "178331            0.0            0.0            0.0             0.0   \n",
       "178332            0.0            0.0            0.0             0.0   \n",
       "178333            0.0            0.0            0.0             0.0   \n",
       "\n",
       "        Model_cayenne  Model_cc  Model_ceed  Model_charade  Model_cherokee  \\\n",
       "0                 0.0       0.0         0.0            0.0             0.0   \n",
       "1                 0.0       0.0         0.0            0.0             0.0   \n",
       "2                 0.0       0.0         0.0            0.0             0.0   \n",
       "3                 0.0       0.0         0.0            0.0             0.0   \n",
       "4                 0.0       0.0         0.0            0.0             0.0   \n",
       "...               ...       ...         ...            ...             ...   \n",
       "178329            0.0       0.0         0.0            0.0             0.0   \n",
       "178330            0.0       0.0         0.0            0.0             0.0   \n",
       "178331            0.0       0.0         0.0            0.0             0.0   \n",
       "178332            0.0       0.0         0.0            0.0             0.0   \n",
       "178333            0.0       0.0         0.0            0.0             0.0   \n",
       "\n",
       "        Model_citigo  Model_civic  Model_cl  Model_clio  Model_clk  \\\n",
       "0                0.0          0.0       0.0         0.0        0.0   \n",
       "1                0.0          0.0       0.0         0.0        0.0   \n",
       "2                0.0          0.0       0.0         0.0        0.0   \n",
       "3                0.0          1.0       0.0         0.0        0.0   \n",
       "4                0.0          0.0       0.0         0.0        0.0   \n",
       "...              ...          ...       ...         ...        ...   \n",
       "178329           0.0          0.0       0.0         0.0        0.0   \n",
       "178330           0.0          0.0       0.0         0.0        0.0   \n",
       "178331           0.0          0.0       0.0         0.0        0.0   \n",
       "178332           0.0          0.0       0.0         0.0        0.0   \n",
       "178333           0.0          0.0       0.0         0.0        0.0   \n",
       "\n",
       "        Model_clubman  Model_colt  Model_combo  Model_cooper  Model_cordoba  \\\n",
       "0                 0.0         0.0          0.0           0.0            0.0   \n",
       "1                 0.0         0.0          0.0           0.0            0.0   \n",
       "2                 0.0         0.0          0.0           0.0            0.0   \n",
       "3                 0.0         0.0          0.0           0.0            0.0   \n",
       "4                 0.0         0.0          0.0           0.0            0.0   \n",
       "...               ...         ...          ...           ...            ...   \n",
       "178329            0.0         0.0          0.0           0.0            0.0   \n",
       "178330            0.0         0.0          0.0           0.0            0.0   \n",
       "178331            0.0         0.0          0.0           0.0            0.0   \n",
       "178332            0.0         0.0          0.0           0.0            0.0   \n",
       "178333            0.0         0.0          0.0           0.0            0.0   \n",
       "\n",
       "        Model_corolla  Model_corsa  Model_cr_reihe  Model_croma  \\\n",
       "0                 0.0          0.0             0.0          0.0   \n",
       "1                 0.0          0.0             0.0          0.0   \n",
       "2                 0.0          0.0             0.0          0.0   \n",
       "3                 0.0          0.0             0.0          0.0   \n",
       "4                 0.0          0.0             0.0          0.0   \n",
       "...               ...          ...             ...          ...   \n",
       "178329            0.0          0.0             0.0          0.0   \n",
       "178330            0.0          0.0             0.0          0.0   \n",
       "178331            0.0          0.0             0.0          0.0   \n",
       "178332            0.0          0.0             0.0          0.0   \n",
       "178333            0.0          0.0             0.0          0.0   \n",
       "\n",
       "        Model_crossfire  Model_cuore  Model_cx_reihe  Model_defender  \\\n",
       "0                   0.0          0.0             0.0             0.0   \n",
       "1                   0.0          0.0             0.0             0.0   \n",
       "2                   0.0          0.0             0.0             0.0   \n",
       "3                   0.0          0.0             0.0             0.0   \n",
       "4                   0.0          0.0             0.0             0.0   \n",
       "...                 ...          ...             ...             ...   \n",
       "178329              0.0          0.0             0.0             0.0   \n",
       "178330              0.0          0.0             0.0             0.0   \n",
       "178331              0.0          0.0             0.0             0.0   \n",
       "178332              0.0          0.0             0.0             0.0   \n",
       "178333              0.0          0.0             0.0             0.0   \n",
       "\n",
       "        Model_delta  Model_discovery  Model_doblo  Model_ducato  Model_duster  \\\n",
       "0               0.0              0.0          0.0           0.0           0.0   \n",
       "1               0.0              0.0          0.0           0.0           0.0   \n",
       "2               0.0              0.0          0.0           0.0           0.0   \n",
       "3               0.0              0.0          0.0           0.0           0.0   \n",
       "4               0.0              0.0          0.0           0.0           0.0   \n",
       "...             ...              ...          ...           ...           ...   \n",
       "178329          0.0              0.0          0.0           0.0           0.0   \n",
       "178330          0.0              0.0          0.0           0.0           0.0   \n",
       "178331          0.0              0.0          0.0           0.0           0.0   \n",
       "178332          0.0              0.0          0.0           0.0           0.0   \n",
       "178333          0.0              0.0          0.0           0.0           0.0   \n",
       "\n",
       "        Model_e_klasse  Model_elefantino  Model_eos  Model_escort  \\\n",
       "0                  0.0               0.0        0.0           0.0   \n",
       "1                  0.0               0.0        0.0           0.0   \n",
       "2                  0.0               0.0        0.0           0.0   \n",
       "3                  0.0               0.0        0.0           0.0   \n",
       "4                  0.0               0.0        0.0           0.0   \n",
       "...                ...               ...        ...           ...   \n",
       "178329             0.0               0.0        0.0           0.0   \n",
       "178330             0.0               0.0        0.0           0.0   \n",
       "178331             0.0               0.0        0.0           0.0   \n",
       "178332             0.0               0.0        0.0           0.0   \n",
       "178333             0.0               0.0        0.0           0.0   \n",
       "\n",
       "        Model_espace  Model_exeo  Model_fabia  Model_fiesta  Model_focus  \\\n",
       "0                0.0         0.0          1.0           0.0          0.0   \n",
       "1                0.0         0.0          0.0           0.0          0.0   \n",
       "2                0.0         0.0          0.0           0.0          0.0   \n",
       "3                0.0         0.0          0.0           0.0          0.0   \n",
       "4                0.0         0.0          0.0           0.0          0.0   \n",
       "...              ...         ...          ...           ...          ...   \n",
       "178329           0.0         0.0          0.0           0.0          0.0   \n",
       "178330           0.0         0.0          0.0           0.0          1.0   \n",
       "178331           0.0         0.0          0.0           0.0          0.0   \n",
       "178332           0.0         0.0          0.0           0.0          0.0   \n",
       "178333           0.0         0.0          0.0           0.0          0.0   \n",
       "\n",
       "        Model_forester  Model_forfour  Model_fortwo  Model_fox  \\\n",
       "0                  0.0            0.0           0.0        0.0   \n",
       "1                  0.0            0.0           0.0        0.0   \n",
       "2                  0.0            0.0           0.0        0.0   \n",
       "3                  0.0            0.0           0.0        0.0   \n",
       "4                  0.0            0.0           0.0        0.0   \n",
       "...                ...            ...           ...        ...   \n",
       "178329             0.0            0.0           0.0        0.0   \n",
       "178330             0.0            0.0           0.0        0.0   \n",
       "178331             0.0            0.0           0.0        0.0   \n",
       "178332             0.0            0.0           0.0        0.0   \n",
       "178333             0.0            0.0           0.0        0.0   \n",
       "\n",
       "        Model_freelander  Model_fusion  Model_g_klasse  Model_galant  \\\n",
       "0                    0.0           0.0             0.0           0.0   \n",
       "1                    0.0           0.0             0.0           0.0   \n",
       "2                    0.0           0.0             0.0           0.0   \n",
       "3                    0.0           0.0             0.0           0.0   \n",
       "4                    0.0           0.0             0.0           0.0   \n",
       "...                  ...           ...             ...           ...   \n",
       "178329               0.0           0.0             0.0           0.0   \n",
       "178330               0.0           0.0             0.0           0.0   \n",
       "178331               0.0           0.0             0.0           0.0   \n",
       "178332               0.0           0.0             0.0           0.0   \n",
       "178333               0.0           0.0             0.0           0.0   \n",
       "\n",
       "        Model_galaxy  Model_getz  Model_gl  Model_glk  Model_golf  \\\n",
       "0                0.0         0.0       0.0        0.0         0.0   \n",
       "1                0.0         0.0       0.0        0.0         0.0   \n",
       "2                0.0         0.0       0.0        0.0         1.0   \n",
       "3                0.0         0.0       0.0        0.0         0.0   \n",
       "4                0.0         0.0       0.0        0.0         0.0   \n",
       "...              ...         ...       ...        ...         ...   \n",
       "178329           0.0         0.0       0.0        0.0         0.0   \n",
       "178330           0.0         0.0       0.0        0.0         0.0   \n",
       "178331           0.0         0.0       0.0        0.0         0.0   \n",
       "178332           0.0         0.0       0.0        0.0         1.0   \n",
       "178333           0.0         0.0       0.0        0.0         0.0   \n",
       "\n",
       "        Model_grand  Model_i3  Model_i_reihe  Model_ibiza  Model_impreza  \\\n",
       "0               0.0       0.0            0.0          0.0            0.0   \n",
       "1               0.0       0.0            0.0          0.0            0.0   \n",
       "2               0.0       0.0            0.0          0.0            0.0   \n",
       "3               0.0       0.0            0.0          0.0            0.0   \n",
       "4               0.0       0.0            0.0          0.0            0.0   \n",
       "...             ...       ...            ...          ...            ...   \n",
       "178329          0.0       0.0            0.0          0.0            0.0   \n",
       "178330          0.0       0.0            0.0          0.0            0.0   \n",
       "178331          0.0       0.0            0.0          0.0            0.0   \n",
       "178332          0.0       0.0            0.0          0.0            0.0   \n",
       "178333          0.0       0.0            0.0          0.0            0.0   \n",
       "\n",
       "        Model_insignia  Model_jazz  Model_jetta  Model_jimny  Model_juke  \\\n",
       "0                  0.0         0.0          0.0          0.0         0.0   \n",
       "1                  0.0         0.0          0.0          0.0         0.0   \n",
       "2                  0.0         0.0          0.0          0.0         0.0   \n",
       "3                  0.0         0.0          0.0          0.0         0.0   \n",
       "4                  0.0         0.0          0.0          0.0         0.0   \n",
       "...                ...         ...          ...          ...         ...   \n",
       "178329             0.0         0.0          0.0          0.0         0.0   \n",
       "178330             0.0         0.0          0.0          0.0         0.0   \n",
       "178331             0.0         0.0          0.0          0.0         0.0   \n",
       "178332             0.0         0.0          0.0          0.0         0.0   \n",
       "178333             0.0         0.0          0.0          0.0         0.0   \n",
       "\n",
       "        Model_justy  Model_ka  Model_kadett  Model_kaefer  Model_kalina  \\\n",
       "0               0.0       0.0           0.0           0.0           0.0   \n",
       "1               0.0       0.0           0.0           0.0           0.0   \n",
       "2               0.0       0.0           0.0           0.0           0.0   \n",
       "3               0.0       0.0           0.0           0.0           0.0   \n",
       "4               0.0       0.0           0.0           0.0           0.0   \n",
       "...             ...       ...           ...           ...           ...   \n",
       "178329          0.0       0.0           0.0           0.0           0.0   \n",
       "178330          0.0       0.0           0.0           0.0           0.0   \n",
       "178331          0.0       0.0           0.0           0.0           0.0   \n",
       "178332          0.0       0.0           0.0           0.0           0.0   \n",
       "178333          0.0       0.0           0.0           0.0           0.0   \n",
       "\n",
       "        Model_kalos  Model_kangoo  Model_kappa  Model_kuga  Model_laguna  \\\n",
       "0               0.0           0.0          0.0         0.0           0.0   \n",
       "1               0.0           0.0          0.0         0.0           0.0   \n",
       "2               0.0           0.0          0.0         0.0           0.0   \n",
       "3               0.0           0.0          0.0         0.0           0.0   \n",
       "4               0.0           0.0          0.0         0.0           0.0   \n",
       "...             ...           ...          ...         ...           ...   \n",
       "178329          0.0           0.0          0.0         0.0           0.0   \n",
       "178330          0.0           0.0          0.0         0.0           0.0   \n",
       "178331          0.0           0.0          0.0         0.0           0.0   \n",
       "178332          0.0           0.0          0.0         0.0           0.0   \n",
       "178333          0.0           0.0          0.0         0.0           0.0   \n",
       "\n",
       "        Model_lancer  Model_lanos  Model_legacy  Model_leon  Model_lodgy  \\\n",
       "0                0.0          0.0           0.0         0.0          0.0   \n",
       "1                0.0          0.0           0.0         0.0          0.0   \n",
       "2                0.0          0.0           0.0         0.0          0.0   \n",
       "3                0.0          0.0           0.0         0.0          0.0   \n",
       "4                0.0          0.0           0.0         0.0          0.0   \n",
       "...              ...          ...           ...         ...          ...   \n",
       "178329           0.0          0.0           0.0         0.0          0.0   \n",
       "178330           0.0          0.0           0.0         0.0          0.0   \n",
       "178331           0.0          0.0           0.0         0.0          0.0   \n",
       "178332           0.0          0.0           0.0         0.0          0.0   \n",
       "178333           0.0          0.0           0.0         0.0          0.0   \n",
       "\n",
       "        Model_logan  Model_lupo  Model_lybra  Model_m_klasse  Model_m_reihe  \\\n",
       "0               0.0         0.0          0.0             0.0            0.0   \n",
       "1               0.0         0.0          0.0             0.0            0.0   \n",
       "2               0.0         0.0          0.0             0.0            0.0   \n",
       "3               0.0         0.0          0.0             0.0            0.0   \n",
       "4               0.0         0.0          0.0             0.0            0.0   \n",
       "...             ...         ...          ...             ...            ...   \n",
       "178329          0.0         0.0          0.0             0.0            0.0   \n",
       "178330          0.0         0.0          0.0             0.0            0.0   \n",
       "178331          0.0         0.0          0.0             0.0            0.0   \n",
       "178332          0.0         0.0          0.0             0.0            0.0   \n",
       "178333          0.0         0.0          0.0             0.0            0.0   \n",
       "\n",
       "        Model_materia  Model_matiz  Model_megane  Model_meriva  Model_micra  \\\n",
       "0                 0.0          0.0           0.0           0.0          0.0   \n",
       "1                 0.0          0.0           0.0           0.0          0.0   \n",
       "2                 0.0          0.0           0.0           0.0          0.0   \n",
       "3                 0.0          0.0           0.0           0.0          0.0   \n",
       "4                 0.0          0.0           0.0           0.0          0.0   \n",
       "...               ...          ...           ...           ...          ...   \n",
       "178329            0.0          0.0           0.0           0.0          0.0   \n",
       "178330            0.0          0.0           0.0           0.0          0.0   \n",
       "178331            0.0          0.0           0.0           0.0          0.0   \n",
       "178332            0.0          0.0           0.0           0.0          0.0   \n",
       "178333            0.0          0.0           0.0           0.0          0.0   \n",
       "\n",
       "        Model_mii  Model_modus  Model_mondeo  Model_move  Model_musa  \\\n",
       "0             0.0          0.0           0.0         0.0         0.0   \n",
       "1             0.0          0.0           0.0         0.0         0.0   \n",
       "2             0.0          0.0           0.0         0.0         0.0   \n",
       "3             0.0          0.0           0.0         0.0         0.0   \n",
       "4             0.0          0.0           0.0         0.0         0.0   \n",
       "...           ...          ...           ...         ...         ...   \n",
       "178329        0.0          0.0           0.0         0.0         0.0   \n",
       "178330        0.0          0.0           0.0         0.0         0.0   \n",
       "178331        0.0          0.0           0.0         0.0         0.0   \n",
       "178332        0.0          0.0           0.0         0.0         0.0   \n",
       "178333        0.0          0.0           0.0         0.0         0.0   \n",
       "\n",
       "        Model_mustang  Model_mx_reihe  Model_navara  Model_niva  Model_note  \\\n",
       "0                 0.0             0.0           0.0         0.0         0.0   \n",
       "1                 0.0             0.0           0.0         0.0         0.0   \n",
       "2                 0.0             0.0           0.0         0.0         0.0   \n",
       "3                 0.0             0.0           0.0         0.0         0.0   \n",
       "4                 0.0             0.0           0.0         0.0         0.0   \n",
       "...               ...             ...           ...         ...         ...   \n",
       "178329            0.0             0.0           0.0         0.0         0.0   \n",
       "178330            0.0             0.0           0.0         0.0         0.0   \n",
       "178331            0.0             0.0           0.0         0.0         0.0   \n",
       "178332            0.0             0.0           0.0         0.0         0.0   \n",
       "178333            0.0             0.0           0.0         0.0         0.0   \n",
       "\n",
       "        Model_nubira  Model_octavia  Model_omega  Model_one  Model_other  \\\n",
       "0                0.0            0.0          0.0        0.0          0.0   \n",
       "1                0.0            0.0          0.0        0.0          0.0   \n",
       "2                0.0            0.0          0.0        0.0          0.0   \n",
       "3                0.0            0.0          0.0        0.0          0.0   \n",
       "4                0.0            0.0          0.0        0.0          1.0   \n",
       "...              ...            ...          ...        ...          ...   \n",
       "178329           0.0            0.0          0.0        0.0          0.0   \n",
       "178330           0.0            0.0          0.0        0.0          0.0   \n",
       "178331           0.0            0.0          0.0        0.0          0.0   \n",
       "178332           0.0            0.0          0.0        0.0          0.0   \n",
       "178333           0.0            0.0          0.0        0.0          0.0   \n",
       "\n",
       "        Model_outlander  Model_pajero  Model_panda  Model_passat  \\\n",
       "0                   0.0           0.0          0.0           0.0   \n",
       "1                   0.0           0.0          0.0           0.0   \n",
       "2                   0.0           0.0          0.0           0.0   \n",
       "3                   0.0           0.0          0.0           0.0   \n",
       "4                   0.0           0.0          0.0           0.0   \n",
       "...                 ...           ...          ...           ...   \n",
       "178329              0.0           0.0          0.0           0.0   \n",
       "178330              0.0           0.0          0.0           0.0   \n",
       "178331              0.0           0.0          0.0           0.0   \n",
       "178332              0.0           0.0          0.0           0.0   \n",
       "178333              0.0           0.0          0.0           0.0   \n",
       "\n",
       "        Model_phaeton  Model_picanto  Model_polo  Model_primera  \\\n",
       "0                 0.0            0.0         0.0            0.0   \n",
       "1                 0.0            0.0         0.0            0.0   \n",
       "2                 0.0            0.0         0.0            0.0   \n",
       "3                 0.0            0.0         0.0            0.0   \n",
       "4                 0.0            0.0         0.0            0.0   \n",
       "...               ...            ...         ...            ...   \n",
       "178329            0.0            0.0         0.0            0.0   \n",
       "178330            0.0            0.0         0.0            0.0   \n",
       "178331            0.0            0.0         0.0            0.0   \n",
       "178332            0.0            0.0         0.0            0.0   \n",
       "178333            0.0            0.0         1.0            0.0   \n",
       "\n",
       "        Model_ptcruiser  Model_punto  Model_q3  Model_q5  Model_q7  \\\n",
       "0                   0.0          0.0       0.0       0.0       0.0   \n",
       "1                   0.0          0.0       0.0       0.0       0.0   \n",
       "2                   0.0          0.0       0.0       0.0       0.0   \n",
       "3                   0.0          0.0       0.0       0.0       0.0   \n",
       "4                   0.0          0.0       0.0       0.0       0.0   \n",
       "...                 ...          ...       ...       ...       ...   \n",
       "178329              0.0          0.0       0.0       0.0       0.0   \n",
       "178330              0.0          0.0       0.0       0.0       0.0   \n",
       "178331              0.0          0.0       0.0       0.0       0.0   \n",
       "178332              0.0          0.0       0.0       0.0       0.0   \n",
       "178333              0.0          0.0       0.0       0.0       0.0   \n",
       "\n",
       "        Model_qashqai  Model_r19  Model_range_rover  Model_range_rover_sport  \\\n",
       "0                 0.0        0.0                0.0                      0.0   \n",
       "1                 0.0        0.0                0.0                      0.0   \n",
       "2                 0.0        0.0                0.0                      0.0   \n",
       "3                 0.0        0.0                0.0                      0.0   \n",
       "4                 0.0        0.0                0.0                      0.0   \n",
       "...               ...        ...                ...                      ...   \n",
       "178329            0.0        0.0                0.0                      0.0   \n",
       "178330            0.0        0.0                0.0                      0.0   \n",
       "178331            0.0        0.0                0.0                      0.0   \n",
       "178332            0.0        0.0                0.0                      0.0   \n",
       "178333            0.0        0.0                0.0                      0.0   \n",
       "\n",
       "        Model_rangerover  Model_rav  Model_rio  Model_roadster  \\\n",
       "0                    0.0        0.0        0.0             0.0   \n",
       "1                    0.0        0.0        0.0             0.0   \n",
       "2                    0.0        0.0        0.0             0.0   \n",
       "3                    0.0        0.0        0.0             0.0   \n",
       "4                    0.0        0.0        0.0             0.0   \n",
       "...                  ...        ...        ...             ...   \n",
       "178329               0.0        0.0        0.0             0.0   \n",
       "178330               0.0        0.0        0.0             0.0   \n",
       "178331               0.0        0.0        0.0             0.0   \n",
       "178332               0.0        0.0        0.0             0.0   \n",
       "178333               0.0        0.0        0.0             0.0   \n",
       "\n",
       "        Model_roomster  Model_rx_reihe  Model_s60  Model_s_klasse  \\\n",
       "0                  0.0             0.0        0.0             0.0   \n",
       "1                  0.0             0.0        0.0             0.0   \n",
       "2                  0.0             0.0        0.0             0.0   \n",
       "3                  0.0             0.0        0.0             0.0   \n",
       "4                  0.0             0.0        0.0             0.0   \n",
       "...                ...             ...        ...             ...   \n",
       "178329             0.0             0.0        0.0             0.0   \n",
       "178330             0.0             0.0        0.0             0.0   \n",
       "178331             0.0             0.0        0.0             0.0   \n",
       "178332             0.0             0.0        0.0             0.0   \n",
       "178333             0.0             0.0        0.0             0.0   \n",
       "\n",
       "        Model_s_max  Model_s_type  Model_samara  Model_sandero  Model_santa  \\\n",
       "0               0.0           0.0           0.0            0.0          0.0   \n",
       "1               0.0           0.0           0.0            0.0          0.0   \n",
       "2               0.0           0.0           0.0            0.0          0.0   \n",
       "3               0.0           0.0           0.0            0.0          0.0   \n",
       "4               0.0           0.0           0.0            0.0          0.0   \n",
       "...             ...           ...           ...            ...          ...   \n",
       "178329          0.0           0.0           0.0            0.0          0.0   \n",
       "178330          0.0           0.0           0.0            0.0          0.0   \n",
       "178331          0.0           0.0           0.0            0.0          0.0   \n",
       "178332          0.0           0.0           0.0            0.0          0.0   \n",
       "178333          0.0           0.0           0.0            0.0          0.0   \n",
       "\n",
       "        Model_scenic  Model_scirocco  Model_seicento  Model_serie_2  \\\n",
       "0                0.0             0.0             0.0            0.0   \n",
       "1                0.0             0.0             0.0            0.0   \n",
       "2                0.0             0.0             0.0            0.0   \n",
       "3                0.0             0.0             0.0            0.0   \n",
       "4                0.0             0.0             0.0            0.0   \n",
       "...              ...             ...             ...            ...   \n",
       "178329           0.0             0.0             0.0            0.0   \n",
       "178330           0.0             0.0             0.0            0.0   \n",
       "178331           0.0             0.0             0.0            0.0   \n",
       "178332           0.0             0.0             0.0            0.0   \n",
       "178333           0.0             0.0             0.0            0.0   \n",
       "\n",
       "        Model_serie_3  Model_sharan  Model_signum  Model_sirion  Model_sl  \\\n",
       "0                 0.0           0.0           0.0           0.0       0.0   \n",
       "1                 0.0           0.0           0.0           0.0       0.0   \n",
       "2                 0.0           0.0           0.0           0.0       0.0   \n",
       "3                 0.0           0.0           0.0           0.0       0.0   \n",
       "4                 0.0           0.0           0.0           0.0       0.0   \n",
       "...               ...           ...           ...           ...       ...   \n",
       "178329            0.0           0.0           0.0           0.0       0.0   \n",
       "178330            0.0           0.0           0.0           0.0       0.0   \n",
       "178331            0.0           0.0           0.0           0.0       0.0   \n",
       "178332            0.0           0.0           0.0           0.0       0.0   \n",
       "178333            0.0           0.0           0.0           0.0       0.0   \n",
       "\n",
       "        Model_slk  Model_sorento  Model_spark  Model_spider  Model_sportage  \\\n",
       "0             0.0            0.0          0.0           0.0             0.0   \n",
       "1             0.0            0.0          0.0           0.0             0.0   \n",
       "2             0.0            0.0          0.0           0.0             0.0   \n",
       "3             0.0            0.0          0.0           0.0             0.0   \n",
       "4             0.0            0.0          0.0           0.0             0.0   \n",
       "...           ...            ...          ...           ...             ...   \n",
       "178329        0.0            0.0          0.0           0.0             0.0   \n",
       "178330        0.0            0.0          0.0           0.0             0.0   \n",
       "178331        0.0            0.0          0.0           0.0             0.0   \n",
       "178332        0.0            0.0          0.0           0.0             0.0   \n",
       "178333        0.0            0.0          0.0           0.0             0.0   \n",
       "\n",
       "        Model_sprinter  Model_stilo  Model_superb  Model_swift  Model_terios  \\\n",
       "0                  0.0          0.0           0.0          0.0           0.0   \n",
       "1                  0.0          0.0           0.0          0.0           0.0   \n",
       "2                  0.0          0.0           0.0          0.0           0.0   \n",
       "3                  0.0          0.0           0.0          0.0           0.0   \n",
       "4                  0.0          0.0           0.0          0.0           0.0   \n",
       "...                ...          ...           ...          ...           ...   \n",
       "178329             0.0          0.0           0.0          0.0           0.0   \n",
       "178330             0.0          0.0           0.0          0.0           0.0   \n",
       "178331             0.0          0.0           0.0          0.0           0.0   \n",
       "178332             0.0          0.0           0.0          0.0           0.0   \n",
       "178333             0.0          0.0           0.0          0.0           0.0   \n",
       "\n",
       "        Model_tigra  Model_tiguan  Model_toledo  Model_touareg  Model_touran  \\\n",
       "0               0.0           0.0           0.0            0.0           0.0   \n",
       "1               0.0           0.0           0.0            0.0           0.0   \n",
       "2               0.0           0.0           0.0            0.0           0.0   \n",
       "3               0.0           0.0           0.0            0.0           0.0   \n",
       "4               0.0           0.0           0.0            0.0           0.0   \n",
       "...             ...           ...           ...            ...           ...   \n",
       "178329          0.0           0.0           0.0            0.0           0.0   \n",
       "178330          0.0           0.0           0.0            0.0           0.0   \n",
       "178331          0.0           0.0           0.0            0.0           0.0   \n",
       "178332          0.0           0.0           0.0            0.0           0.0   \n",
       "178333          0.0           0.0           0.0            0.0           0.0   \n",
       "\n",
       "        Model_transit  Model_transporter  Model_tt  Model_tucson  \\\n",
       "0                 0.0                0.0       0.0           0.0   \n",
       "1                 0.0                0.0       0.0           0.0   \n",
       "2                 0.0                0.0       0.0           0.0   \n",
       "3                 0.0                0.0       0.0           0.0   \n",
       "4                 0.0                0.0       0.0           0.0   \n",
       "...               ...                ...       ...           ...   \n",
       "178329            0.0                0.0       0.0           0.0   \n",
       "178330            0.0                0.0       0.0           0.0   \n",
       "178331            0.0                0.0       0.0           0.0   \n",
       "178332            0.0                0.0       0.0           0.0   \n",
       "178333            0.0                0.0       0.0           0.0   \n",
       "\n",
       "        Model_twingo  Model_unknown  Model_up  Model_v40  Model_v50  \\\n",
       "0                0.0            0.0       0.0        0.0        0.0   \n",
       "1                0.0            0.0       0.0        0.0        0.0   \n",
       "2                0.0            0.0       0.0        0.0        0.0   \n",
       "3                0.0            0.0       0.0        0.0        0.0   \n",
       "4                0.0            0.0       0.0        0.0        0.0   \n",
       "...              ...            ...       ...        ...        ...   \n",
       "178329           0.0            1.0       0.0        0.0        0.0   \n",
       "178330           0.0            0.0       0.0        0.0        0.0   \n",
       "178331           0.0            0.0       0.0        0.0        0.0   \n",
       "178332           0.0            0.0       0.0        0.0        0.0   \n",
       "178333           0.0            0.0       0.0        0.0        0.0   \n",
       "\n",
       "        Model_v60  Model_v70  Model_v_klasse  Model_vectra  Model_verso  \\\n",
       "0             0.0        0.0             0.0           0.0          0.0   \n",
       "1             0.0        0.0             0.0           0.0          0.0   \n",
       "2             0.0        0.0             0.0           0.0          0.0   \n",
       "3             0.0        0.0             0.0           0.0          0.0   \n",
       "4             0.0        0.0             0.0           0.0          0.0   \n",
       "...           ...        ...             ...           ...          ...   \n",
       "178329        0.0        0.0             0.0           0.0          0.0   \n",
       "178330        0.0        0.0             0.0           0.0          0.0   \n",
       "178331        0.0        0.0             0.0           0.0          0.0   \n",
       "178332        0.0        0.0             0.0           0.0          0.0   \n",
       "178333        0.0        0.0             0.0           0.0          0.0   \n",
       "\n",
       "        Model_viano  Model_vito  Model_vivaro  Model_voyager  Model_wrangler  \\\n",
       "0               0.0         0.0           0.0            0.0             0.0   \n",
       "1               0.0         0.0           0.0            0.0             0.0   \n",
       "2               0.0         0.0           0.0            0.0             0.0   \n",
       "3               0.0         0.0           0.0            0.0             0.0   \n",
       "4               0.0         0.0           0.0            0.0             0.0   \n",
       "...             ...         ...           ...            ...             ...   \n",
       "178329          0.0         0.0           0.0            0.0             0.0   \n",
       "178330          0.0         0.0           0.0            0.0             0.0   \n",
       "178331          0.0         0.0           0.0            0.0             0.0   \n",
       "178332          0.0         0.0           0.0            0.0             0.0   \n",
       "178333          0.0         0.0           0.0            0.0             0.0   \n",
       "\n",
       "        Model_x_reihe  Model_x_trail  Model_x_type  Model_xc_reihe  \\\n",
       "0                 0.0            0.0           0.0             0.0   \n",
       "1                 0.0            0.0           0.0             0.0   \n",
       "2                 0.0            0.0           0.0             0.0   \n",
       "3                 0.0            0.0           0.0             0.0   \n",
       "4                 0.0            0.0           0.0             0.0   \n",
       "...               ...            ...           ...             ...   \n",
       "178329            0.0            0.0           0.0             0.0   \n",
       "178330            0.0            0.0           0.0             0.0   \n",
       "178331            0.0            0.0           0.0             0.0   \n",
       "178332            0.0            0.0           0.0             0.0   \n",
       "178333            0.0            0.0           0.0             0.0   \n",
       "\n",
       "        Model_yaris  Model_yeti  Model_ypsilon  Model_z_reihe  Model_zafira  \\\n",
       "0               0.0         0.0            0.0            0.0           0.0   \n",
       "1               0.0         0.0            0.0            0.0           0.0   \n",
       "2               0.0         0.0            0.0            0.0           0.0   \n",
       "3               0.0         0.0            0.0            0.0           0.0   \n",
       "4               0.0         0.0            0.0            0.0           0.0   \n",
       "...             ...         ...            ...            ...           ...   \n",
       "178329          0.0         0.0            0.0            0.0           0.0   \n",
       "178330          0.0         0.0            0.0            0.0           0.0   \n",
       "178331          0.0         0.0            0.0            0.0           0.0   \n",
       "178332          0.0         0.0            0.0            0.0           0.0   \n",
       "178333          0.0         0.0            0.0            0.0           0.0   \n",
       "\n",
       "        FuelType_electric  FuelType_gasoline  FuelType_hybrid  FuelType_lpg  \\\n",
       "0                     0.0                0.0              0.0           0.0   \n",
       "1                     0.0                0.0              0.0           0.0   \n",
       "2                     0.0                0.0              0.0           0.0   \n",
       "3                     0.0                0.0              0.0           0.0   \n",
       "4                     0.0                0.0              0.0           0.0   \n",
       "...                   ...                ...              ...           ...   \n",
       "178329                0.0                0.0              0.0           0.0   \n",
       "178330                0.0                0.0              0.0           0.0   \n",
       "178331                0.0                1.0              0.0           0.0   \n",
       "178332                0.0                0.0              0.0           0.0   \n",
       "178333                0.0                1.0              0.0           0.0   \n",
       "\n",
       "        FuelType_other  FuelType_petrol  FuelType_unknown  Brand_audi  \\\n",
       "0                  0.0              1.0               0.0         0.0   \n",
       "1                  0.0              1.0               0.0         1.0   \n",
       "2                  0.0              1.0               0.0         0.0   \n",
       "3                  0.0              1.0               0.0         0.0   \n",
       "4                  0.0              1.0               0.0         0.0   \n",
       "...                ...              ...               ...         ...   \n",
       "178329             0.0              1.0               0.0         0.0   \n",
       "178330             0.0              1.0               0.0         0.0   \n",
       "178331             0.0              0.0               0.0         0.0   \n",
       "178332             0.0              1.0               0.0         0.0   \n",
       "178333             0.0              0.0               0.0         0.0   \n",
       "\n",
       "        Brand_bmw  Brand_chevrolet  Brand_chrysler  Brand_citroen  \\\n",
       "0             0.0              0.0             0.0            0.0   \n",
       "1             0.0              0.0             0.0            0.0   \n",
       "2             0.0              0.0             0.0            0.0   \n",
       "3             0.0              0.0             0.0            0.0   \n",
       "4             0.0              0.0             1.0            0.0   \n",
       "...           ...              ...             ...            ...   \n",
       "178329        0.0              1.0             0.0            0.0   \n",
       "178330        0.0              0.0             0.0            0.0   \n",
       "178331        1.0              0.0             0.0            0.0   \n",
       "178332        0.0              0.0             0.0            0.0   \n",
       "178333        0.0              0.0             0.0            0.0   \n",
       "\n",
       "        Brand_dacia  Brand_daewoo  Brand_daihatsu  Brand_fiat  Brand_ford  \\\n",
       "0               0.0           0.0             0.0         0.0         0.0   \n",
       "1               0.0           0.0             0.0         0.0         0.0   \n",
       "2               0.0           0.0             0.0         0.0         0.0   \n",
       "3               0.0           0.0             0.0         0.0         0.0   \n",
       "4               0.0           0.0             0.0         0.0         0.0   \n",
       "...             ...           ...             ...         ...         ...   \n",
       "178329          0.0           0.0             0.0         0.0         0.0   \n",
       "178330          0.0           0.0             0.0         0.0         1.0   \n",
       "178331          0.0           0.0             0.0         0.0         0.0   \n",
       "178332          0.0           0.0             0.0         0.0         0.0   \n",
       "178333          0.0           0.0             0.0         0.0         0.0   \n",
       "\n",
       "        Brand_honda  Brand_hyundai  Brand_jaguar  Brand_jeep  Brand_kia  \\\n",
       "0               0.0            0.0           0.0         0.0        0.0   \n",
       "1               0.0            0.0           0.0         0.0        0.0   \n",
       "2               0.0            0.0           0.0         0.0        0.0   \n",
       "3               1.0            0.0           0.0         0.0        0.0   \n",
       "4               0.0            0.0           0.0         0.0        0.0   \n",
       "...             ...            ...           ...         ...        ...   \n",
       "178329          0.0            0.0           0.0         0.0        0.0   \n",
       "178330          0.0            0.0           0.0         0.0        0.0   \n",
       "178331          0.0            0.0           0.0         0.0        0.0   \n",
       "178332          0.0            0.0           0.0         0.0        0.0   \n",
       "178333          0.0            0.0           0.0         0.0        0.0   \n",
       "\n",
       "        Brand_lada  Brand_lancia  Brand_land_rover  Brand_mazda  \\\n",
       "0              0.0           0.0               0.0          0.0   \n",
       "1              0.0           0.0               0.0          0.0   \n",
       "2              0.0           0.0               0.0          0.0   \n",
       "3              0.0           0.0               0.0          0.0   \n",
       "4              0.0           0.0               0.0          0.0   \n",
       "...            ...           ...               ...          ...   \n",
       "178329         0.0           0.0               0.0          0.0   \n",
       "178330         0.0           0.0               0.0          0.0   \n",
       "178331         0.0           0.0               0.0          0.0   \n",
       "178332         0.0           0.0               0.0          0.0   \n",
       "178333         0.0           0.0               0.0          0.0   \n",
       "\n",
       "        Brand_mercedes_benz  Brand_mini  Brand_mitsubishi  Brand_nissan  \\\n",
       "0                       0.0         0.0               0.0           0.0   \n",
       "1                       0.0         0.0               0.0           0.0   \n",
       "2                       0.0         0.0               0.0           0.0   \n",
       "3                       0.0         0.0               0.0           0.0   \n",
       "4                       0.0         0.0               0.0           0.0   \n",
       "...                     ...         ...               ...           ...   \n",
       "178329                  0.0         0.0               0.0           0.0   \n",
       "178330                  0.0         0.0               0.0           0.0   \n",
       "178331                  0.0         0.0               0.0           0.0   \n",
       "178332                  0.0         0.0               0.0           0.0   \n",
       "178333                  0.0         0.0               0.0           0.0   \n",
       "\n",
       "        Brand_opel  Brand_peugeot  Brand_porsche  Brand_renault  Brand_rover  \\\n",
       "0              0.0            0.0            0.0            0.0          0.0   \n",
       "1              0.0            0.0            0.0            0.0          0.0   \n",
       "2              0.0            0.0            0.0            0.0          0.0   \n",
       "3              0.0            0.0            0.0            0.0          0.0   \n",
       "4              0.0            0.0            0.0            0.0          0.0   \n",
       "...            ...            ...            ...            ...          ...   \n",
       "178329         0.0            0.0            0.0            0.0          0.0   \n",
       "178330         0.0            0.0            0.0            0.0          0.0   \n",
       "178331         0.0            0.0            0.0            0.0          0.0   \n",
       "178332         0.0            0.0            0.0            0.0          0.0   \n",
       "178333         0.0            0.0            0.0            0.0          0.0   \n",
       "\n",
       "        Brand_saab  Brand_seat  Brand_skoda  Brand_smart  \\\n",
       "0              0.0         0.0          1.0          0.0   \n",
       "1              0.0         0.0          0.0          0.0   \n",
       "2              0.0         0.0          0.0          0.0   \n",
       "3              0.0         0.0          0.0          0.0   \n",
       "4              0.0         0.0          0.0          0.0   \n",
       "...            ...         ...          ...          ...   \n",
       "178329         0.0         0.0          0.0          0.0   \n",
       "178330         0.0         0.0          0.0          0.0   \n",
       "178331         0.0         0.0          0.0          0.0   \n",
       "178332         0.0         0.0          0.0          0.0   \n",
       "178333         0.0         0.0          0.0          0.0   \n",
       "\n",
       "        Brand_sonstige_autos  Brand_subaru  Brand_suzuki  Brand_toyota  \\\n",
       "0                        0.0           0.0           0.0           0.0   \n",
       "1                        0.0           0.0           0.0           0.0   \n",
       "2                        0.0           0.0           0.0           0.0   \n",
       "3                        0.0           0.0           0.0           0.0   \n",
       "4                        0.0           0.0           0.0           0.0   \n",
       "...                      ...           ...           ...           ...   \n",
       "178329                   0.0           0.0           0.0           0.0   \n",
       "178330                   0.0           0.0           0.0           0.0   \n",
       "178331                   0.0           0.0           0.0           0.0   \n",
       "178332                   0.0           0.0           0.0           0.0   \n",
       "178333                   0.0           0.0           0.0           0.0   \n",
       "\n",
       "        Brand_trabant  Brand_volkswagen  Brand_volvo  Repaired_unknown  \\\n",
       "0                 0.0               0.0          0.0               0.0   \n",
       "1                 0.0               0.0          0.0               0.0   \n",
       "2                 0.0               1.0          0.0               0.0   \n",
       "3                 0.0               0.0          0.0               0.0   \n",
       "4                 0.0               0.0          0.0               0.0   \n",
       "...               ...               ...          ...               ...   \n",
       "178329            0.0               0.0          0.0               0.0   \n",
       "178330            0.0               0.0          0.0               0.0   \n",
       "178331            0.0               0.0          0.0               0.0   \n",
       "178332            0.0               1.0          0.0               0.0   \n",
       "178333            0.0               1.0          0.0               0.0   \n",
       "\n",
       "        Repaired_yes  Gearbox_manual  Gearbox_unknown  \n",
       "0                0.0             1.0              0.0  \n",
       "1                0.0             1.0              0.0  \n",
       "2                0.0             1.0              0.0  \n",
       "3                1.0             1.0              0.0  \n",
       "4                0.0             1.0              0.0  \n",
       "...              ...             ...              ...  \n",
       "178329           0.0             1.0              0.0  \n",
       "178330           0.0             1.0              0.0  \n",
       "178331           1.0             0.0              0.0  \n",
       "178332           0.0             0.0              0.0  \n",
       "178333           0.0             1.0              0.0  \n",
       "\n",
       "[178334 rows x 309 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VehicleType</th>\n",
       "      <th>RegistrationYear</th>\n",
       "      <th>Gearbox</th>\n",
       "      <th>Power</th>\n",
       "      <th>Model</th>\n",
       "      <th>Kilometer</th>\n",
       "      <th>FuelType</th>\n",
       "      <th>Brand</th>\n",
       "      <th>Repaired</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2010</td>\n",
       "      <td>1.0</td>\n",
       "      <td>60</td>\n",
       "      <td>101.0</td>\n",
       "      <td>50000</td>\n",
       "      <td>6.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.0</td>\n",
       "      <td>2016</td>\n",
       "      <td>1.0</td>\n",
       "      <td>101</td>\n",
       "      <td>29.0</td>\n",
       "      <td>150000</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.0</td>\n",
       "      <td>2003</td>\n",
       "      <td>1.0</td>\n",
       "      <td>179</td>\n",
       "      <td>116.0</td>\n",
       "      <td>150000</td>\n",
       "      <td>6.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.0</td>\n",
       "      <td>1997</td>\n",
       "      <td>1.0</td>\n",
       "      <td>75</td>\n",
       "      <td>73.0</td>\n",
       "      <td>125000</td>\n",
       "      <td>6.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>1998</td>\n",
       "      <td>1.0</td>\n",
       "      <td>98</td>\n",
       "      <td>166.0</td>\n",
       "      <td>125000</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178329</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2008</td>\n",
       "      <td>1.0</td>\n",
       "      <td>75</td>\n",
       "      <td>226.0</td>\n",
       "      <td>60000</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178330</th>\n",
       "      <td>4.0</td>\n",
       "      <td>1999</td>\n",
       "      <td>1.0</td>\n",
       "      <td>116</td>\n",
       "      <td>103.0</td>\n",
       "      <td>150000</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178331</th>\n",
       "      <td>4.0</td>\n",
       "      <td>2005</td>\n",
       "      <td>0.0</td>\n",
       "      <td>231</td>\n",
       "      <td>15.0</td>\n",
       "      <td>150000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178332</th>\n",
       "      <td>4.0</td>\n",
       "      <td>2008</td>\n",
       "      <td>0.0</td>\n",
       "      <td>200</td>\n",
       "      <td>116.0</td>\n",
       "      <td>150000</td>\n",
       "      <td>6.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178333</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2012</td>\n",
       "      <td>1.0</td>\n",
       "      <td>75</td>\n",
       "      <td>173.0</td>\n",
       "      <td>5000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>178334 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        VehicleType  RegistrationYear  Gearbox  Power  Model  Kilometer  \\\n",
       "0               5.0              2010      1.0     60  101.0      50000   \n",
       "1               7.0              2016      1.0    101   29.0     150000   \n",
       "2               4.0              2003      1.0    179  116.0     150000   \n",
       "3               5.0              1997      1.0     75   73.0     125000   \n",
       "4               4.0              1998      1.0     98  166.0     125000   \n",
       "...             ...               ...      ...    ...    ...        ...   \n",
       "178329          5.0              2008      1.0     75  226.0      60000   \n",
       "178330          4.0              1999      1.0    116  103.0     150000   \n",
       "178331          4.0              2005      0.0    231   15.0     150000   \n",
       "178332          4.0              2008      0.0    200  116.0     150000   \n",
       "178333          5.0              2012      1.0     75  173.0       5000   \n",
       "\n",
       "        FuelType  Brand  Repaired  \n",
       "0            6.0   31.0       0.0  \n",
       "1            6.0    1.0       0.0  \n",
       "2            6.0   38.0       0.0  \n",
       "3            6.0   11.0       2.0  \n",
       "4            6.0    4.0       0.0  \n",
       "...          ...    ...       ...  \n",
       "178329       6.0    3.0       0.0  \n",
       "178330       6.0   10.0       0.0  \n",
       "178331       2.0    2.0       2.0  \n",
       "178332       6.0   38.0       0.0  \n",
       "178333       2.0   38.0       0.0  \n",
       "\n",
       "[178334 rows x 9 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "display(features_train_ohe)\n",
    "features_train_oe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Стандартизируем численные данные:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.06943722, -1.01649733, -2.03214012, ..., -0.30284658,\n",
       "         0.54788692, -0.13320248],\n",
       "       [ 2.0336154 , -0.38029907,  0.6252799 , ..., -0.30284658,\n",
       "         0.54788692, -0.13320248],\n",
       "       [-0.05543734,  0.83002932,  0.6252799 , ..., -0.30284658,\n",
       "         0.54788692, -0.13320248],\n",
       "       ...,\n",
       "       [ 0.26595539,  1.63691492,  0.6252799 , ...,  3.302002  ,\n",
       "        -1.82519414, -0.13320248],\n",
       "       [ 0.74804449,  1.15588697,  0.6252799 , ..., -0.30284658,\n",
       "        -1.82519414, -0.13320248],\n",
       "       [ 1.39082995, -0.78374187, -3.22797913, ..., -0.30284658,\n",
       "         0.54788692, -0.13320248]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.20260951,  1.06943722,  0.4467957 , ...,  0.65785178,\n",
       "         0.7932585 , -0.49775554],\n",
       "       [ 1.01517084,  2.0336154 ,  0.4467957 , ...,  0.65785178,\n",
       "        -1.4473624 , -0.49775554],\n",
       "       [-0.20367115, -0.05543734,  0.4467957 , ...,  0.65785178,\n",
       "         1.31607004, -0.49775554],\n",
       "       ...,\n",
       "       [-0.20367115,  0.26595539, -1.83267401, ..., -1.43622194,\n",
       "        -1.37267504,  2.74308917],\n",
       "       [-0.20367115,  0.74804449, -1.83267401, ...,  0.65785178,\n",
       "         1.31607004, -0.49775554],\n",
       "       [ 0.20260951,  1.39082995,  0.4467957 , ..., -1.43622194,\n",
       "         1.31607004, -0.49775554]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def scaled(data):\n",
    "    numeric = data.columns\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(data[numeric]) \n",
    "    return scaler.transform(data[numeric])\n",
    "\n",
    "\n",
    "display(scaled(features_train_ohe))\n",
    "scaled(features_train_oe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим данные моделью - **Дерево Решений** с использованием кодировщика OneHotEncoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Лучшее значение глубины: 9\n",
      "Лучшая оценка кросс-валидации: 2066.672678111925\n",
      "CPU times: total: 1min 13s\n",
      "Wall time: 1min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "parametrs = {'max_depth': range (1, 10)}\n",
    "model = DecisionTreeRegressor(random_state=R_STATE)\n",
    "\n",
    "grid_search = GridSearchCV(model, param_grid=parametrs, cv=5, scoring='neg_root_mean_squared_error')\n",
    "grid_search.fit(features_train_ohe, target_train)\n",
    "\n",
    "best_parametrs = pd.Series(grid_search.best_params_)[0]\n",
    "best_score_rough_DT = -1 * grid_search.best_score_\n",
    "\n",
    "print(\"Лучшее значение глубины:\", best_parametrs)\n",
    "print(\"Лучшая оценка кросс-валидации:\", best_score_rough_DT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Время обучения в минутах: 0.04390360116958618\n",
      "Время предсказания в минутах: 0.004963155587514242\n",
      "CPU times: total: 2.92 s\n",
      "Wall time: 2.93 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "start = time()\n",
    "model = DecisionTreeRegressor(random_state=R_STATE, max_depth=9)\n",
    "model.fit(features_train_ohe, target_train) \n",
    "end = time()\n",
    "model_time_DT = (end-start)/60\n",
    "\n",
    "start = time()\n",
    "model.predict(features_train_ohe)\n",
    "end = time()\n",
    "model_time_DTP = (end-start)/60\n",
    "\n",
    "print(\"Время обучения в минутах:\", model_time_DT)\n",
    "print(\"Время предсказания в минутах:\", model_time_DTP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим данные моделью - **Линейная регрессия** с использованием кодировщика OneHotEncoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Время обучения в минутах: 0.04793680111567179\n",
      "CPU times: total: 9.39 s\n",
      "Wall time: 2.88 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "start = time()\n",
    "  \n",
    "model = LinearRegression()\n",
    "model.fit(features_train_ohe, target_train)\n",
    "\n",
    "end = time()\n",
    "\n",
    "model_time_LR = (end-start)/60\n",
    "\n",
    "print(\"Время обучения в минутах:\", model_time_LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Лучшая оценка кросс-валидации: 2659.4639682893144\n",
      "Время предсказания в минутах: 0.004128436247507731\n",
      "CPU times: total: 578 ms\n",
      "Wall time: 251 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "start = time()\n",
    "prediction_LR = model.predict(features_train_ohe)\n",
    "end = time()\n",
    "model_time_LRP = (end-start)/60\n",
    "\n",
    "best_score_rough_LR = mean_squared_error(target_train, prediction_LR) ** 0.5\n",
    "\n",
    "print(\"Лучшая оценка кросс-валидации:\", best_score_rough_LR)\n",
    "print(\"Время предсказания в минутах:\", model_time_LRP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим данные моделью - **CatBoost** с использованием собственного кодировщика cat_features: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Лучшее значение параметров: {'depth': 9, 'n_estimators': 9}\n",
      "Лучшая оценка кросс-валидации: 1818.0848410848848\n",
      "CPU times: total: 13min 2s\n",
      "Wall time: 5min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "parametrs = {'depth': range (1, 10),\n",
    "             'n_estimators': range(1, 10)\n",
    "}\n",
    "cat_features = ['VehicleType', 'Model', 'FuelType', 'Brand', 'Repaired', 'Gearbox']\n",
    "\n",
    "model = CatBoostRegressor(random_state=R_STATE, learning_rate=0.5, \n",
    "                          cat_features = cat_features)\n",
    "\n",
    "grid_search = GridSearchCV(model, param_grid=parametrs, cv=5, scoring='neg_root_mean_squared_error')\n",
    "grid_search.fit(features_train, target_train, verbose=False)\n",
    "\n",
    "best_parametrs = grid_search.best_params_\n",
    "best_score_rough_CB = -1 * grid_search.best_score_\n",
    "\n",
    "print(\"Лучшее значение параметров:\", best_parametrs)\n",
    "print(\"Лучшая оценка кросс-валидации:\", best_score_rough_CB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Время обучения в минутах: 0.024344146251678467\n",
      "Время предсказания в минутах: 0.006540600458780924\n",
      "CPU times: total: 5.22 s\n",
      "Wall time: 1.85 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "start = time()\n",
    "model = CatBoostRegressor(random_state=R_STATE, depth=9, n_estimators=9, learning_rate=0.5, \n",
    "                          cat_features = cat_features)\n",
    "model.fit(features_train, target_train, verbose=False) \n",
    "end = time()\n",
    "model_time_CB = (end-start)/60\n",
    "\n",
    "start = time()\n",
    "model.predict(features_train)\n",
    "end = time()\n",
    "model_time_CBP = (end-start)/60\n",
    "\n",
    "print(\"Время обучения в минутах:\", model_time_CB)\n",
    "print(\"Время предсказания в минутах:\", model_time_CBP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим данные моделью - **LGBM** с использованием кодировщика OrdinalEncoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004372 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 623\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5302.917774\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003446 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5300.828804\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003971 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5294.429244\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006310 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5295.055521\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005716 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142668, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5304.607368\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003635 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 623\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5302.917774\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003424 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5300.828804\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003617 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5294.429244\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003137 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5295.055521\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003234 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142668, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5304.607368\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004229 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 623\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5302.917774\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003152 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5300.828804\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005355 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5294.429244\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003431 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5295.055521\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004306 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142668, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5304.607368\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005206 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 623\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5302.917774\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003697 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5300.828804\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006190 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5294.429244\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003715 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5295.055521\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006594 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142668, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5304.607368\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004085 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 623\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5302.917774\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004950 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5300.828804\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005567 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5294.429244\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006355 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5295.055521\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003321 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142668, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5304.607368\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003722 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 623\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5302.917774\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005206 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5300.828804\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006466 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5294.429244\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003463 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5295.055521\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005450 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142668, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5304.607368\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003877 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 623\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5302.917774\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003890 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5300.828804\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003452 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5294.429244\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005044 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5295.055521\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003952 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142668, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5304.607368\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003798 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 623\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5302.917774\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003725 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5300.828804\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003444 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5294.429244\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003859 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5295.055521\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003299 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142668, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5304.607368\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003434 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 623\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5302.917774\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004993 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5300.828804\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003655 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5294.429244\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007815 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5295.055521\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004582 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142668, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5304.607368\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003850 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 623\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5302.917774\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003456 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5300.828804\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005139 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5294.429244\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006687 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5295.055521\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003741 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142668, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5304.607368\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003862 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 623\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5302.917774\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003268 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5300.828804\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003877 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5294.429244\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005436 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5295.055521\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005293 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142668, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5304.607368\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003796 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 623\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5302.917774\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003660 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5300.828804\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004174 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5294.429244\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004568 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5295.055521\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003437 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142668, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5304.607368\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005280 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 623\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5302.917774\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003326 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5300.828804\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004448 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5294.429244\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003894 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5295.055521\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005334 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142668, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5304.607368\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005179 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 623\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5302.917774\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003767 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5300.828804\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005095 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5294.429244\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003563 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5295.055521\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003704 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142668, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5304.607368\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003993 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 623\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5302.917774\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004000 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5300.828804\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004224 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5294.429244\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006544 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5295.055521\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006876 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142668, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5304.607368\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004069 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 623\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5302.917774\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003517 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5300.828804\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004257 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5294.429244\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003907 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5295.055521\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004199 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142668, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5304.607368\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005274 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 623\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5302.917774\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005435 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5300.828804\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005760 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5294.429244\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006049 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5295.055521\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006594 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142668, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5304.607368\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005869 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 623\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5302.917774\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006653 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5300.828804\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004520 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5294.429244\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006459 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5295.055521\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006552 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142668, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5304.607368\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005240 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 623\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5302.917774\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005200 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5300.828804\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003560 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5294.429244\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004112 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5295.055521\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006143 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142668, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5304.607368\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004162 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 623\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5302.917774\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003757 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5300.828804\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004954 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5294.429244\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003841 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5295.055521\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005396 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142668, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5304.607368\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004253 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 623\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5302.917774\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006717 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5300.828804\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004538 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5294.429244\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007081 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5295.055521\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003590 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142668, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5304.607368\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005516 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 623\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5302.917774\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004025 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5300.828804\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004014 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5294.429244\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003657 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5295.055521\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003855 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142668, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5304.607368\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003542 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 623\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5302.917774\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003976 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5300.828804\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003835 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5294.429244\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004540 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5295.055521\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003868 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142668, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5304.607368\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004432 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 623\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5302.917774\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003776 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5300.828804\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005115 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5294.429244\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003969 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5295.055521\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003949 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142668, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5304.607368\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006532 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 623\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5302.917774\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003451 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5300.828804\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003412 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5294.429244\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003818 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5295.055521\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003706 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142668, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5304.607368\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004152 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 623\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5302.917774\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005120 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5300.828804\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003866 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5294.429244\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003809 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5295.055521\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004500 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142668, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5304.607368\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003777 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 623\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5302.917774\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003794 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5300.828804\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003353 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5294.429244\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006243 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5295.055521\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009156 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142668, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5304.607368\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003409 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 623\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5302.917774\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005115 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5300.828804\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003616 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5294.429244\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005283 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5295.055521\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003832 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142668, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5304.607368\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005950 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 623\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5302.917774\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003366 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5300.828804\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004049 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5294.429244\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006306 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5295.055521\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006535 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142668, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5304.607368\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005025 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 623\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5302.917774\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003845 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5300.828804\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006573 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5294.429244\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003942 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5295.055521\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005535 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142668, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5304.607368\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006811 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 623\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5302.917774\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003905 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5300.828804\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005005 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5294.429244\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004211 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5295.055521\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003699 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142668, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5304.607368\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004971 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 623\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5302.917774\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005060 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5300.828804\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003448 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5294.429244\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004991 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5295.055521\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005262 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142668, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5304.607368\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006337 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 623\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5302.917774\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005052 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5300.828804\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005180 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5294.429244\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003962 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5295.055521\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006347 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142668, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5304.607368\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006391 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 623\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5302.917774\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003483 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5300.828804\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005046 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5294.429244\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006655 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5295.055521\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004207 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142668, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5304.607368\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008323 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 623\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5302.917774\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006542 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5300.828804\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003499 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5294.429244\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004001 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5295.055521\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006641 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142668, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5304.607368\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004153 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 623\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5302.917774\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003787 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5300.828804\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003215 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5294.429244\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004031 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5295.055521\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003834 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142668, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5304.607368\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006419 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 623\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5302.917774\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004863 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5300.828804\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004962 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5294.429244\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004031 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5295.055521\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008126 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142668, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5304.607368\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004115 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 623\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5302.917774\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003906 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5300.828804\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005328 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5294.429244\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005285 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5295.055521\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003455 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142668, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5304.607368\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003488 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 623\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5302.917774\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005284 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5300.828804\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005857 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5294.429244\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003777 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5295.055521\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005314 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142668, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5304.607368\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007466 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 623\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5302.917774\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005847 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5300.828804\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005718 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5294.429244\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004808 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5295.055521\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006691 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142668, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5304.607368\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005314 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 623\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5302.917774\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005259 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5300.828804\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005518 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5294.429244\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003471 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5295.055521\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004572 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142668, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5304.607368\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003670 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 623\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5302.917774\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003204 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5300.828804\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003875 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5294.429244\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004938 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5295.055521\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005692 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142668, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5304.607368\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005052 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 623\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5302.917774\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003396 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5300.828804\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005484 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5294.429244\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005175 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5295.055521\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005506 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142668, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5304.607368\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003452 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 623\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5302.917774\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003480 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5300.828804\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004502 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5294.429244\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005329 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5295.055521\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003517 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142668, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5304.607368\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007771 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 623\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5302.917774\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003729 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5300.828804\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004159 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5294.429244\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004673 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5295.055521\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009303 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142668, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5304.607368\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003529 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 623\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5302.917774\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005182 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5300.828804\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003686 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5294.429244\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003837 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5295.055521\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003790 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142668, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5304.607368\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005392 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 623\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5302.917774\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003087 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5300.828804\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003652 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5294.429244\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006248 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5295.055521\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003588 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142668, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5304.607368\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005066 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 623\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5302.917774\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003547 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5300.828804\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006375 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5294.429244\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003944 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5295.055521\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003516 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142668, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5304.607368\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004055 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 623\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5302.917774\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003999 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5300.828804\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004014 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5294.429244\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004033 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5295.055521\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003420 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142668, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5304.607368\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005036 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 623\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5302.917774\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003623 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5300.828804\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004706 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5294.429244\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004344 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5295.055521\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004091 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142668, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5304.607368\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004132 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 623\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5302.917774\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005860 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5300.828804\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006711 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5294.429244\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004596 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5295.055521\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003867 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142668, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5304.607368\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003934 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 623\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5302.917774\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005793 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5300.828804\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003843 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5294.429244\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003953 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5295.055521\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003879 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142668, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5304.607368\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005051 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 623\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5302.917774\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004326 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5300.828804\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003816 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5294.429244\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003722 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5295.055521\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003676 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142668, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5304.607368\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003493 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 623\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5302.917774\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005275 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5300.828804\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006615 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5294.429244\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007156 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5295.055521\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004285 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142668, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5304.607368\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005887 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 623\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5302.917774\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003699 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5300.828804\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006727 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5294.429244\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006083 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5295.055521\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006908 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142668, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5304.607368\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003783 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 623\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5302.917774\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003992 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5300.828804\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004111 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5294.429244\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004138 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5295.055521\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004139 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142668, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5304.607368\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006607 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 623\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5302.917774\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004016 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5300.828804\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005042 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5294.429244\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005896 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5295.055521\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006225 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142668, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5304.607368\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004111 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 623\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5302.917774\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005893 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5300.828804\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005053 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5294.429244\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003933 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5295.055521\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006863 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142668, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5304.607368\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004101 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 623\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5302.917774\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004375 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5300.828804\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004082 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5294.429244\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003704 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5295.055521\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003438 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142668, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5304.607368\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003714 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 623\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5302.917774\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004505 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5300.828804\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006208 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5294.429244\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004139 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5295.055521\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008040 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142668, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5304.607368\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004202 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 623\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5302.917774\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005861 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5300.828804\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003623 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5294.429244\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004221 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5295.055521\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003849 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142668, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5304.607368\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006007 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 623\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5302.917774\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006298 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5300.828804\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005928 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5294.429244\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003711 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5295.055521\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004145 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142668, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5304.607368\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004525 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 623\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5302.917774\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005817 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5300.828804\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003902 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5294.429244\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003853 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5295.055521\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007823 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142668, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5304.607368\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003857 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 623\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5302.917774\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003945 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5300.828804\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003487 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5294.429244\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003732 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5295.055521\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003370 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142668, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5304.607368\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003748 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 623\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5302.917774\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003477 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5300.828804\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003907 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5294.429244\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005785 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5295.055521\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003418 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142668, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5304.607368\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003978 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 623\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5302.917774\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003131 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5300.828804\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003709 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5294.429244\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004447 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5295.055521\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003707 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142668, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5304.607368\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006500 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 623\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5302.917774\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004366 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5300.828804\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007134 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5294.429244\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003480 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5295.055521\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004241 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142668, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5304.607368\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004635 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 623\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5302.917774\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008310 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5300.828804\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004639 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5294.429244\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005753 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5295.055521\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006687 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142668, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5304.607368\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006810 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 623\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5302.917774\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003962 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5300.828804\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003580 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5294.429244\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005673 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5295.055521\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004480 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142668, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5304.607368\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008127 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 623\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5302.917774\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003617 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5300.828804\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006549 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5294.429244\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003886 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5295.055521\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004080 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142668, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5304.607368\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004514 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 623\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5302.917774\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004313 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5300.828804\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003412 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5294.429244\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004162 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5295.055521\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003359 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142668, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5304.607368\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003828 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 623\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5302.917774\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004077 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5300.828804\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003868 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5294.429244\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007598 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5295.055521\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006257 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142668, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5304.607368\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004490 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 623\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5302.917774\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004104 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5300.828804\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005904 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5294.429244\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003782 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5295.055521\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003734 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142668, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5304.607368\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004044 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 623\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5302.917774\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006243 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5300.828804\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003656 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5294.429244\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007596 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5295.055521\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003310 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142668, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5304.607368\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004358 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 623\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5302.917774\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003972 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5300.828804\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003942 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5294.429244\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004011 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5295.055521\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006201 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142668, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5304.607368\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004721 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 623\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5302.917774\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003752 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5300.828804\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004082 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5294.429244\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003956 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5295.055521\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006486 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142668, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5304.607368\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006141 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 623\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5302.917774\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003599 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5300.828804\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003714 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5294.429244\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003657 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5295.055521\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003509 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142668, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5304.607368\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003098 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 623\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5302.917774\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003976 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5300.828804\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004201 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5294.429244\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003383 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5295.055521\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003314 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142668, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5304.607368\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003861 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 623\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5302.917774\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006315 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5300.828804\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003819 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5294.429244\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006798 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5295.055521\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005298 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142668, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5304.607368\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003438 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 623\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5302.917774\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003808 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5300.828804\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004211 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5294.429244\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003886 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5295.055521\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003588 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142668, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5304.607368\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005379 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 623\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5302.917774\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004303 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5300.828804\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003443 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5294.429244\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003494 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 624\n",
      "[LightGBM] [Info] Number of data points in the train set: 142667, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5295.055521\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004263 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 142668, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5304.607368\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005083 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 178334, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5299.567749\n",
      "Лучшее значение глубины: {'max_depth': 7, 'n_estimators': 9}\n",
      "Лучшая оценка кросс-валидации: 3459.7027158647898\n",
      "CPU times: total: 4min 27s\n",
      "Wall time: 1min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "parametrs = {'max_depth': range (1, 10),\n",
    "             'n_estimators': range(1, 10)\n",
    "}\n",
    "model = LGBMRegressor(learning_rate=0.05, random_state=R_STATE)\n",
    "\n",
    "grid_search = GridSearchCV(model, param_grid=parametrs, cv=5, scoring='neg_root_mean_squared_error')\n",
    "grid_search.fit(features_train_oe, target_train)\n",
    "\n",
    "best_parametrs = grid_search.best_params_\n",
    "best_score_rough_L = -1 * grid_search.best_score_\n",
    "\n",
    "print(\"Лучшее значение глубины:\", best_parametrs)\n",
    "print(\"Лучшая оценка кросс-валидации:\", best_score_rough_L)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004831 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 178334, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 5299.567749\n",
      "Время обучения в минутах: 0.012479118506113688\n",
      "Время предсказания в минутах: 0.007084083557128906\n",
      "CPU times: total: 4.58 s\n",
      "Wall time: 1.17 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "start = time()\n",
    "model = LGBMRegressor(learning_rate=0.05, random_state=R_STATE, max_depth=9)\n",
    "model.fit(features_train_oe, target_train) \n",
    "end = time()\n",
    "model_time_L = (end-start)/60\n",
    "\n",
    "start = time()\n",
    "model.predict(features_train_oe)\n",
    "end = time()\n",
    "model_time_LP = (end-start)/60\n",
    "\n",
    "print(\"Время обучения в минутах:\", model_time_L)\n",
    "print(\"Время предсказания в минутах:\", model_time_LP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вывод:**\n",
    "- Разделили данные на признаки и целевой признак\n",
    "- Преобразовали категориальные данные в численные\n",
    "- Разделили данные на обучающую и тестовую выборки\n",
    "- Обучили данные на следующих моделях: Дерево решений, Линейная регрессия, CatBoost и LGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Анализ моделей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В результате обучения данных на 4х разных моделях собрали следующие результаты:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DecisionTreeRegressor</th>\n",
       "      <th>LinearRegression</th>\n",
       "      <th>CatBoostRegressor</th>\n",
       "      <th>LightGBM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>RMSE модели</th>\n",
       "      <td>2066.672678</td>\n",
       "      <td>2659.463968</td>\n",
       "      <td>1818.084841</td>\n",
       "      <td>3459.702716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Скорость обучения, min</th>\n",
       "      <td>0.043904</td>\n",
       "      <td>0.047937</td>\n",
       "      <td>0.024344</td>\n",
       "      <td>0.012479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Скорость предсказания, min</th>\n",
       "      <td>0.004963</td>\n",
       "      <td>0.004128</td>\n",
       "      <td>0.006541</td>\n",
       "      <td>0.007084</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            DecisionTreeRegressor  LinearRegression  \\\n",
       "RMSE модели                           2066.672678       2659.463968   \n",
       "Скорость обучения, min                   0.043904          0.047937   \n",
       "Скорость предсказания, min               0.004963          0.004128   \n",
       "\n",
       "                            CatBoostRegressor     LightGBM  \n",
       "RMSE модели                       1818.084841  3459.702716  \n",
       "Скорость обучения, min               0.024344     0.012479  \n",
       "Скорость предсказания, min           0.006541     0.007084  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column=['RMSE модели','Скорость обучения, min','Скорость предсказания, min']\n",
    "\n",
    "comparison_table = pd.DataFrame(index=['RMSE модели', 'Скорость обучения, min', 'Скорость предсказания, min'], columns=['DecisionTreeRegressor', 'LinearRegression', 'CatBoostRegressor','LightGBM'])\n",
    "comparison_table['DecisionTreeRegressor'] = best_score_rough_DT, model_time_DT, model_time_DTP\n",
    "comparison_table['LinearRegression'] = best_score_rough_LR, model_time_LR, model_time_LRP\n",
    "comparison_table['CatBoostRegressor'] = best_score_rough_CB, model_time_CB, model_time_CBP\n",
    "comparison_table['LightGBM'] = best_score_rough_L, model_time_L, model_time_LP\n",
    "\n",
    "comparison_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Из таблицы видим, что лучшей моделью по качеству является CatBoostRegressor. \n",
    "Проведем на тестирование на лучшей модели градиентного бустинга:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Время в минутах: 0.024429094791412354\n",
      "CPU times: total: 5.42 s\n",
      "Wall time: 1.47 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "start = time()\n",
    "\n",
    "model_cat = CatBoostRegressor(random_state=R_STATE, depth=9, n_estimators=9, learning_rate=0.5, \n",
    "                          cat_features = cat_features)\n",
    "model_cat.fit(features_train, target_train, verbose=False)\n",
    "\n",
    "end = time()\n",
    "\n",
    "model_time_T = (end-start)/60\n",
    "\n",
    "print(\"Время в минутах:\", model_time_T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 1846.6224387811294\n",
      "Время в минутах: 0.002350111802419027\n",
      "CPU times: total: 219 ms\n",
      "Wall time: 143 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "start = time()\n",
    "\n",
    "predictions_T = model_cat.predict(features_test)\n",
    "end = time()\n",
    "model_time_TP = (end-start)/60\n",
    "\n",
    "RMSE = mean_squared_error(target_test, predictions_T)**0.5\n",
    "\n",
    "print('RMSE:', RMSE)\n",
    "print(\"Время в минутах:\", model_time_TP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результат метрики качества на тестовой выборке показал результат чуть хуже, чем на тренировочной выборке, однако адекватный"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим лучшую модель на адекватность на константной модели:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Время в минутах: 3.332297007242839e-05\n",
      "CPU times: total: 0 ns\n",
      "Wall time: 2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "start = time()\n",
    "\n",
    "model = DummyRegressor()\n",
    "model.fit(features_train, target_train)\n",
    "\n",
    "end = time()\n",
    "\n",
    "model_time_DR = (end-start)/60\n",
    "\n",
    "print(\"Время в минутах:\", model_time_DR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 4645.373433281228\n",
      "Время в минутах: 0.0\n",
      "CPU times: total: 0 ns\n",
      "Wall time: 2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "start = time()\n",
    "predictions_DR = model.predict(features_test)\n",
    "end = time()\n",
    "model_time_DRP = (end-start)/60\n",
    "\n",
    "RMSE_DR = mean_squared_error(target_test, predictions_DR)**0.5\n",
    "\n",
    "print('RMSE:', RMSE_DR)\n",
    "print(\"Время в минутах:\", model_time_DRP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Константная модель выдает результат более, чем в 2 раза хуже, что подтверждает адекватность выбранной модели."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вывод:** \n",
    "\n",
    "Лучшей по качеству и скорости предсказаний и скорости обучения оказалась модель градиентного бустинга - CatBoost. \n",
    "Метрика качества показала результат около 1,5 раз больше минимального порога.\n",
    "Проверка модели на тестовой выборке показала результат чуть лучше, чем на обучающей и подтвердила адекватность модели."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Общий вывод"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Цель исследования - построить модель для определения стоимости автомобилей. При этом заказчику важны:\n",
    "- качество предсказания;\n",
    "- скорость предсказания;\n",
    "- время обучения.\n",
    "\n",
    "1) На первом этапе рассмотрели и подготовили данные:\n",
    "- Данные состоят из 16 столбцов и 354369 строк. \n",
    "- В нескольких столбцах содержались пропуски, общая доля которых довольно высока, заменили пропуски отдельной категорией unknown. \n",
    "- Удалили 4  дубликата из данных.\n",
    "- Подготовили данные к обучению, удалив неважные для обучения столбцы. Получили 9 столбцов с признаками и один с целевым признаком.\n",
    "- Обработали анамольные значения в числовых данных.\n",
    "\n",
    "2) На этапе обучения моделей:\n",
    "- Разделили данные на признаки и целевой признак\n",
    "- Преобразовали категориальные данные в численные\n",
    "- Разделили данные на обучающую и тестовую выборки\n",
    "- Обучили данные на следующих моделях: Дерево решений, Линейная регрессия, CatBoost и LGBM\n",
    "\n",
    "3) Проанализировав работу моделей выявили следующие результаты:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DecisionTreeRegressor</th>\n",
       "      <th>LinearRegression</th>\n",
       "      <th>CatBoostRegressor</th>\n",
       "      <th>LightGBM</th>\n",
       "      <th>CatBoostRegressor на тестовой выборке</th>\n",
       "      <th>DummyRegressor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>RMSE модели</th>\n",
       "      <td>2066.672678</td>\n",
       "      <td>2659.463968</td>\n",
       "      <td>1818.084841</td>\n",
       "      <td>3459.702716</td>\n",
       "      <td>1846.622439</td>\n",
       "      <td>4645.373433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Скорость обучения, min</th>\n",
       "      <td>0.043904</td>\n",
       "      <td>0.047937</td>\n",
       "      <td>0.024344</td>\n",
       "      <td>0.012479</td>\n",
       "      <td>0.024429</td>\n",
       "      <td>0.000033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Скорость предсказания, min</th>\n",
       "      <td>0.004963</td>\n",
       "      <td>0.004128</td>\n",
       "      <td>0.006541</td>\n",
       "      <td>0.007084</td>\n",
       "      <td>0.002350</td>\n",
       "      <td>0.000033</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            DecisionTreeRegressor  LinearRegression  \\\n",
       "RMSE модели                           2066.672678       2659.463968   \n",
       "Скорость обучения, min                   0.043904          0.047937   \n",
       "Скорость предсказания, min               0.004963          0.004128   \n",
       "\n",
       "                            CatBoostRegressor     LightGBM  \\\n",
       "RMSE модели                       1818.084841  3459.702716   \n",
       "Скорость обучения, min               0.024344     0.012479   \n",
       "Скорость предсказания, min           0.006541     0.007084   \n",
       "\n",
       "                            CatBoostRegressor на тестовой выборке  \\\n",
       "RMSE модели                                           1846.622439   \n",
       "Скорость обучения, min                                   0.024429   \n",
       "Скорость предсказания, min                               0.002350   \n",
       "\n",
       "                            DummyRegressor  \n",
       "RMSE модели                    4645.373433  \n",
       "Скорость обучения, min            0.000033  \n",
       "Скорость предсказания, min        0.000033  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column=['RMSE модели','Скорость обучения, min','Скорость предсказания, min']\n",
    "\n",
    "comparison_table = pd.DataFrame(index=['RMSE модели', 'Скорость обучения, min', 'Скорость предсказания, min'], columns=['DecisionTreeRegressor', 'LinearRegression', 'CatBoostRegressor','LightGBM'])\n",
    "comparison_table['DecisionTreeRegressor'] = best_score_rough_DT, model_time_DT, model_time_DTP\n",
    "comparison_table['LinearRegression'] = best_score_rough_LR, model_time_LR, model_time_LRP\n",
    "comparison_table['CatBoostRegressor'] = best_score_rough_CB, model_time_CB, model_time_CBP\n",
    "comparison_table['LightGBM'] = best_score_rough_L, model_time_L, model_time_LP\n",
    "comparison_table['CatBoostRegressor на тестовой выборке'] = RMSE, model_time_T, model_time_TP\n",
    "comparison_table['DummyRegressor'] = RMSE_DR, model_time_DR, model_time_DR\n",
    "\n",
    "comparison_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Из таблицы видим, что лучшей моделью и по скорости и по качеству является CatBoostRegressor. \n",
    "\n",
    "Проверка модели на тестовой выборке показала результат чуть хуже, чем на обучающей, однако адекватный."
   ]
  }
 ],
 "metadata": {
  "ExecuteTimeLog": [
   {
    "duration": 1734,
    "start_time": "2024-06-24T14:47:52.598Z"
   },
   {
    "duration": 3191,
    "start_time": "2024-06-24T14:48:50.622Z"
   },
   {
    "duration": 50,
    "start_time": "2024-06-24T14:49:40.050Z"
   },
   {
    "duration": 94,
    "start_time": "2024-06-24T14:49:40.798Z"
   },
   {
    "duration": 188,
    "start_time": "2024-06-24T14:50:11.134Z"
   },
   {
    "duration": 82,
    "start_time": "2024-06-24T14:50:56.667Z"
   },
   {
    "duration": 1187,
    "start_time": "2024-06-24T14:51:15.967Z"
   },
   {
    "duration": 122,
    "start_time": "2024-06-24T14:51:28.887Z"
   },
   {
    "duration": 88,
    "start_time": "2024-06-24T14:55:49.422Z"
   },
   {
    "duration": 15,
    "start_time": "2024-06-24T14:59:38.751Z"
   },
   {
    "duration": 32,
    "start_time": "2024-06-24T15:00:04.459Z"
   },
   {
    "duration": 1125,
    "start_time": "2024-06-24T15:00:54.130Z"
   },
   {
    "duration": 103,
    "start_time": "2024-06-24T15:01:00.784Z"
   },
   {
    "duration": 104,
    "start_time": "2024-06-24T15:01:51.147Z"
   },
   {
    "duration": 201,
    "start_time": "2024-06-24T15:02:14.007Z"
   },
   {
    "duration": 159,
    "start_time": "2024-06-24T15:02:56.344Z"
   },
   {
    "duration": 194,
    "start_time": "2024-06-24T15:03:12.134Z"
   },
   {
    "duration": 65,
    "start_time": "2024-06-24T15:03:34.624Z"
   },
   {
    "duration": 12,
    "start_time": "2024-06-24T15:03:39.751Z"
   },
   {
    "duration": 1170,
    "start_time": "2024-06-24T15:06:13.528Z"
   },
   {
    "duration": 1110,
    "start_time": "2024-06-24T15:06:37.912Z"
   },
   {
    "duration": 1549,
    "start_time": "2024-06-24T15:06:57.856Z"
   },
   {
    "duration": 1194,
    "start_time": "2024-06-24T15:06:59.407Z"
   },
   {
    "duration": 58,
    "start_time": "2024-06-24T15:07:00.603Z"
   },
   {
    "duration": 184,
    "start_time": "2024-06-24T15:07:00.663Z"
   },
   {
    "duration": 91,
    "start_time": "2024-06-24T15:07:01.079Z"
   },
   {
    "duration": 172,
    "start_time": "2024-06-24T15:07:01.700Z"
   },
   {
    "duration": 1321,
    "start_time": "2024-06-24T15:07:57.281Z"
   },
   {
    "duration": 1078,
    "start_time": "2024-06-24T15:08:16.483Z"
   },
   {
    "duration": 1525,
    "start_time": "2024-06-24T15:08:35.297Z"
   },
   {
    "duration": 1109,
    "start_time": "2024-06-24T15:08:37.369Z"
   },
   {
    "duration": 48,
    "start_time": "2024-06-24T15:08:38.479Z"
   },
   {
    "duration": 115,
    "start_time": "2024-06-24T15:08:38.529Z"
   },
   {
    "duration": 94,
    "start_time": "2024-06-24T15:08:38.646Z"
   },
   {
    "duration": 113,
    "start_time": "2024-06-24T15:08:39.328Z"
   },
   {
    "duration": 199,
    "start_time": "2024-06-24T15:09:19.586Z"
   },
   {
    "duration": 170,
    "start_time": "2024-06-24T15:10:30.298Z"
   },
   {
    "duration": 1515,
    "start_time": "2024-06-24T15:11:11.105Z"
   },
   {
    "duration": 1131,
    "start_time": "2024-06-24T15:11:12.622Z"
   },
   {
    "duration": 512,
    "start_time": "2024-06-24T15:11:13.762Z"
   },
   {
    "duration": 122,
    "start_time": "2024-06-24T15:11:39.264Z"
   },
   {
    "duration": 11,
    "start_time": "2024-06-24T15:11:48.626Z"
   },
   {
    "duration": 580,
    "start_time": "2024-06-24T15:12:28.880Z"
   },
   {
    "duration": 32,
    "start_time": "2024-06-24T15:14:38.229Z"
   },
   {
    "duration": 44,
    "start_time": "2024-06-24T15:18:37.282Z"
   },
   {
    "duration": 96,
    "start_time": "2024-06-24T15:18:38.170Z"
   },
   {
    "duration": 99,
    "start_time": "2024-06-24T15:18:41.985Z"
   },
   {
    "duration": 142,
    "start_time": "2024-06-24T15:18:46.978Z"
   },
   {
    "duration": 211,
    "start_time": "2024-06-24T15:18:50.481Z"
   },
   {
    "duration": 122516,
    "start_time": "2024-06-24T15:21:33.658Z"
   },
   {
    "duration": 14,
    "start_time": "2024-06-24T15:28:36.411Z"
   },
   {
    "duration": 3,
    "start_time": "2024-06-24T15:32:40.691Z"
   },
   {
    "duration": 18,
    "start_time": "2024-06-24T15:33:37.142Z"
   },
   {
    "duration": 1550,
    "start_time": "2024-06-24T15:35:26.453Z"
   },
   {
    "duration": 1159,
    "start_time": "2024-06-24T15:35:29.053Z"
   },
   {
    "duration": 579,
    "start_time": "2024-06-24T15:35:30.214Z"
   },
   {
    "duration": 566,
    "start_time": "2024-06-24T15:35:30.798Z"
   },
   {
    "duration": 49,
    "start_time": "2024-06-24T15:35:31.366Z"
   },
   {
    "duration": 92,
    "start_time": "2024-06-24T15:35:31.707Z"
   },
   {
    "duration": 93,
    "start_time": "2024-06-24T15:35:32.196Z"
   },
   {
    "duration": 118,
    "start_time": "2024-06-24T15:35:32.676Z"
   },
   {
    "duration": 184,
    "start_time": "2024-06-24T15:35:33.204Z"
   },
   {
    "duration": 124082,
    "start_time": "2024-06-24T15:35:33.660Z"
   },
   {
    "duration": 3,
    "start_time": "2024-06-24T15:37:37.745Z"
   },
   {
    "duration": 149,
    "start_time": "2024-06-24T15:37:37.750Z"
   },
   {
    "duration": 11,
    "start_time": "2024-06-24T15:38:42.693Z"
   },
   {
    "duration": 11,
    "start_time": "2024-06-24T15:38:50.941Z"
   },
   {
    "duration": 12,
    "start_time": "2024-06-24T15:40:23.389Z"
   },
   {
    "duration": 13,
    "start_time": "2024-06-24T15:40:31.429Z"
   },
   {
    "duration": 3,
    "start_time": "2024-06-24T15:40:32.300Z"
   },
   {
    "duration": 55,
    "start_time": "2024-06-24T15:40:32.910Z"
   },
   {
    "duration": 90,
    "start_time": "2024-06-24T15:41:10.596Z"
   },
   {
    "duration": 58,
    "start_time": "2024-06-24T15:52:22.766Z"
   },
   {
    "duration": 51,
    "start_time": "2024-06-24T15:53:13.684Z"
   },
   {
    "duration": 70,
    "start_time": "2024-06-24T15:55:17.021Z"
   },
   {
    "duration": 59,
    "start_time": "2024-06-24T15:56:23.707Z"
   },
   {
    "duration": 56,
    "start_time": "2024-06-24T15:56:52.627Z"
   },
   {
    "duration": 1694,
    "start_time": "2024-06-24T15:58:12.718Z"
   },
   {
    "duration": 1188,
    "start_time": "2024-06-24T15:58:15.685Z"
   },
   {
    "duration": 518,
    "start_time": "2024-06-24T15:58:17.420Z"
   },
   {
    "duration": 549,
    "start_time": "2024-06-24T15:58:17.966Z"
   },
   {
    "duration": 79,
    "start_time": "2024-06-24T15:58:18.748Z"
   },
   {
    "duration": 66,
    "start_time": "2024-06-24T15:58:19.581Z"
   },
   {
    "duration": 56,
    "start_time": "2024-06-24T15:58:21.244Z"
   },
   {
    "duration": 68,
    "start_time": "2024-06-24T15:58:22.506Z"
   },
   {
    "duration": 1290,
    "start_time": "2024-06-24T15:58:29.572Z"
   },
   {
    "duration": 1141,
    "start_time": "2024-06-24T15:58:46.901Z"
   },
   {
    "duration": 60,
    "start_time": "2024-06-24T15:59:04.003Z"
   },
   {
    "duration": 1500,
    "start_time": "2024-06-24T15:59:39.310Z"
   },
   {
    "duration": 1159,
    "start_time": "2024-06-24T15:59:41.189Z"
   },
   {
    "duration": 532,
    "start_time": "2024-06-24T15:59:43.094Z"
   },
   {
    "duration": 580,
    "start_time": "2024-06-24T15:59:44.373Z"
   },
   {
    "duration": 78,
    "start_time": "2024-06-24T15:59:45.637Z"
   },
   {
    "duration": 70,
    "start_time": "2024-06-24T15:59:49.862Z"
   },
   {
    "duration": 60,
    "start_time": "2024-06-24T15:59:58.147Z"
   },
   {
    "duration": 68,
    "start_time": "2024-06-24T15:59:59.916Z"
   },
   {
    "duration": 59,
    "start_time": "2024-06-24T16:00:23.917Z"
   },
   {
    "duration": 139,
    "start_time": "2024-06-24T16:00:34.838Z"
   },
   {
    "duration": 321277,
    "start_time": "2024-06-24T16:00:43.965Z"
   },
   {
    "duration": 14,
    "start_time": "2024-06-24T16:07:51.348Z"
   },
   {
    "duration": 591,
    "start_time": "2024-06-24T16:07:52.109Z"
   },
   {
    "duration": 2,
    "start_time": "2024-06-24T16:07:56.077Z"
   },
   {
    "duration": 376,
    "start_time": "2024-06-24T16:07:57.092Z"
   },
   {
    "duration": 180,
    "start_time": "2024-06-24T16:11:38.164Z"
   },
   {
    "duration": 73701,
    "start_time": "2024-06-24T16:14:32.879Z"
   },
   {
    "duration": 72,
    "start_time": "2024-06-24T16:20:20.872Z"
   },
   {
    "duration": 87,
    "start_time": "2024-06-24T16:21:01.825Z"
   },
   {
    "duration": 90177,
    "start_time": "2024-06-24T16:22:37.658Z"
   },
   {
    "duration": 103738,
    "start_time": "2024-06-24T16:25:36.223Z"
   },
   {
    "duration": 13,
    "start_time": "2024-06-24T16:31:39.100Z"
   },
   {
    "duration": 2857,
    "start_time": "2024-06-24T16:36:18.861Z"
   },
   {
    "duration": 92994,
    "start_time": "2024-06-24T16:38:25.124Z"
   },
   {
    "duration": 1498,
    "start_time": "2024-06-24T17:23:40.101Z"
   },
   {
    "duration": 1154,
    "start_time": "2024-06-24T17:23:42.064Z"
   },
   {
    "duration": 534,
    "start_time": "2024-06-24T17:23:43.221Z"
   },
   {
    "duration": 609,
    "start_time": "2024-06-24T17:23:43.757Z"
   },
   {
    "duration": 69,
    "start_time": "2024-06-24T17:23:44.368Z"
   },
   {
    "duration": 81,
    "start_time": "2024-06-24T17:23:44.438Z"
   },
   {
    "duration": 60,
    "start_time": "2024-06-24T17:23:44.570Z"
   },
   {
    "duration": 62,
    "start_time": "2024-06-24T17:23:45.018Z"
   },
   {
    "duration": 54,
    "start_time": "2024-06-24T17:23:45.476Z"
   },
   {
    "duration": 168,
    "start_time": "2024-06-24T17:23:45.933Z"
   },
   {
    "duration": 319227,
    "start_time": "2024-06-24T17:23:46.379Z"
   },
   {
    "duration": 14,
    "start_time": "2024-06-24T17:29:05.608Z"
   },
   {
    "duration": 764,
    "start_time": "2024-06-24T17:29:05.624Z"
   },
   {
    "duration": 3,
    "start_time": "2024-06-24T17:29:06.390Z"
   },
   {
    "duration": 386,
    "start_time": "2024-06-24T17:29:06.394Z"
   },
   {
    "duration": 91417,
    "start_time": "2024-06-24T17:29:06.781Z"
   },
   {
    "duration": 105068,
    "start_time": "2024-06-24T17:30:38.200Z"
   },
   {
    "duration": 93579,
    "start_time": "2024-06-24T17:32:23.271Z"
   },
   {
    "duration": 408316,
    "start_time": "2024-06-24T17:33:56.852Z"
   },
   {
    "duration": 84,
    "start_time": "2024-06-24T17:40:45.171Z"
   },
   {
    "duration": 0,
    "start_time": "2024-06-24T17:40:45.257Z"
   },
   {
    "duration": 0,
    "start_time": "2024-06-24T17:40:45.258Z"
   },
   {
    "duration": 4,
    "start_time": "2024-06-24T17:44:08.363Z"
   },
   {
    "duration": 31696,
    "start_time": "2024-06-24T17:44:11.343Z"
   },
   {
    "duration": 120,
    "start_time": "2024-06-24T17:44:43.041Z"
   },
   {
    "duration": 3,
    "start_time": "2024-06-24T17:45:10.711Z"
   },
   {
    "duration": 31999,
    "start_time": "2024-06-24T17:45:11.395Z"
   },
   {
    "duration": 4,
    "start_time": "2024-06-24T17:46:01.779Z"
   },
   {
    "duration": 32058,
    "start_time": "2024-06-24T17:46:02.811Z"
   },
   {
    "duration": 2278,
    "start_time": "2024-06-24T17:55:17.043Z"
   },
   {
    "duration": 2544,
    "start_time": "2024-06-24T17:57:00.243Z"
   },
   {
    "duration": 2341,
    "start_time": "2024-06-24T17:57:19.291Z"
   },
   {
    "duration": 4,
    "start_time": "2024-06-24T18:00:57.205Z"
   },
   {
    "duration": 1603,
    "start_time": "2024-06-24T20:13:44.979Z"
   },
   {
    "duration": 1171,
    "start_time": "2024-06-24T20:13:48.523Z"
   },
   {
    "duration": 521,
    "start_time": "2024-06-24T20:13:51.203Z"
   },
   {
    "duration": 565,
    "start_time": "2024-06-24T20:13:51.726Z"
   },
   {
    "duration": 18,
    "start_time": "2024-06-24T20:27:21.082Z"
   },
   {
    "duration": 27,
    "start_time": "2024-06-24T20:27:56.207Z"
   },
   {
    "duration": 119,
    "start_time": "2024-06-24T20:28:28.518Z"
   },
   {
    "duration": 24,
    "start_time": "2024-06-24T20:28:39.146Z"
   },
   {
    "duration": 135,
    "start_time": "2024-06-24T20:28:51.739Z"
   },
   {
    "duration": 113,
    "start_time": "2024-06-24T20:29:25.705Z"
   },
   {
    "duration": 26,
    "start_time": "2024-06-24T20:29:45.193Z"
   },
   {
    "duration": 29,
    "start_time": "2024-06-24T20:30:02.723Z"
   },
   {
    "duration": 126,
    "start_time": "2024-06-24T20:31:39.979Z"
   },
   {
    "duration": 12,
    "start_time": "2024-06-24T20:41:59.243Z"
   },
   {
    "duration": 10,
    "start_time": "2024-06-24T20:42:54.811Z"
   },
   {
    "duration": 1494,
    "start_time": "2024-06-24T20:43:10.976Z"
   },
   {
    "duration": 1197,
    "start_time": "2024-06-24T20:43:15.386Z"
   },
   {
    "duration": 513,
    "start_time": "2024-06-24T20:43:17.084Z"
   },
   {
    "duration": 560,
    "start_time": "2024-06-24T20:43:17.631Z"
   },
   {
    "duration": 30,
    "start_time": "2024-06-24T20:43:19.442Z"
   },
   {
    "duration": 47,
    "start_time": "2024-06-24T20:43:21.267Z"
   },
   {
    "duration": 98,
    "start_time": "2024-06-24T20:43:23.515Z"
   },
   {
    "duration": 88,
    "start_time": "2024-06-24T20:43:25.348Z"
   },
   {
    "duration": 115,
    "start_time": "2024-06-24T20:43:28.923Z"
   },
   {
    "duration": 14,
    "start_time": "2024-06-24T20:45:45.106Z"
   },
   {
    "duration": 11,
    "start_time": "2024-06-24T20:47:20.793Z"
   },
   {
    "duration": 103,
    "start_time": "2024-06-24T20:47:29.073Z"
   },
   {
    "duration": 183,
    "start_time": "2024-06-24T20:47:35.212Z"
   },
   {
    "duration": 269,
    "start_time": "2024-06-24T20:52:15.513Z"
   },
   {
    "duration": 165,
    "start_time": "2024-06-24T20:52:47.600Z"
   },
   {
    "duration": 14,
    "start_time": "2024-06-24T20:54:48.474Z"
   },
   {
    "duration": 10,
    "start_time": "2024-06-24T20:56:15.320Z"
   },
   {
    "duration": 863,
    "start_time": "2024-06-24T20:56:20.736Z"
   },
   {
    "duration": 649,
    "start_time": "2024-06-24T20:56:44.832Z"
   },
   {
    "duration": 763,
    "start_time": "2024-06-24T20:57:35.801Z"
   },
   {
    "duration": 956,
    "start_time": "2024-06-24T20:57:45.558Z"
   },
   {
    "duration": 60,
    "start_time": "2024-06-24T21:01:02.255Z"
   },
   {
    "duration": 27,
    "start_time": "2024-06-24T21:01:58.697Z"
   },
   {
    "duration": 96,
    "start_time": "2024-06-24T21:02:36.003Z"
   },
   {
    "duration": 50,
    "start_time": "2024-06-24T21:02:56.367Z"
   },
   {
    "duration": 95,
    "start_time": "2024-06-24T21:03:02.235Z"
   },
   {
    "duration": 83,
    "start_time": "2024-06-24T21:03:52.808Z"
   },
   {
    "duration": 1462,
    "start_time": "2024-06-24T21:04:24.408Z"
   },
   {
    "duration": 1107,
    "start_time": "2024-06-24T21:04:27.572Z"
   },
   {
    "duration": 524,
    "start_time": "2024-06-24T21:04:28.937Z"
   },
   {
    "duration": 513,
    "start_time": "2024-06-24T21:04:29.547Z"
   },
   {
    "duration": 29,
    "start_time": "2024-06-24T21:04:30.952Z"
   },
   {
    "duration": 62,
    "start_time": "2024-06-24T21:04:32.434Z"
   },
   {
    "duration": 89,
    "start_time": "2024-06-24T21:04:34.800Z"
   },
   {
    "duration": 95,
    "start_time": "2024-06-24T21:04:37.138Z"
   },
   {
    "duration": 189,
    "start_time": "2024-06-24T21:04:57.362Z"
   },
   {
    "duration": 141288,
    "start_time": "2024-06-24T21:05:06.418Z"
   },
   {
    "duration": 787,
    "start_time": "2024-06-24T21:07:27.708Z"
   },
   {
    "duration": 1502,
    "start_time": "2024-06-24T21:08:12.633Z"
   },
   {
    "duration": 1096,
    "start_time": "2024-06-24T21:08:15.308Z"
   },
   {
    "duration": 507,
    "start_time": "2024-06-24T21:08:16.722Z"
   },
   {
    "duration": 548,
    "start_time": "2024-06-24T21:08:17.230Z"
   },
   {
    "duration": 28,
    "start_time": "2024-06-24T21:08:18.482Z"
   },
   {
    "duration": 62,
    "start_time": "2024-06-24T21:08:19.666Z"
   },
   {
    "duration": 92,
    "start_time": "2024-06-24T21:08:21.326Z"
   },
   {
    "duration": 87,
    "start_time": "2024-06-24T21:08:22.346Z"
   },
   {
    "duration": 202,
    "start_time": "2024-06-24T21:08:23.682Z"
   },
   {
    "duration": 3,
    "start_time": "2024-06-24T21:08:27.584Z"
   },
   {
    "duration": 751,
    "start_time": "2024-06-24T21:08:29.114Z"
   },
   {
    "duration": 672,
    "start_time": "2024-06-24T21:08:45.234Z"
   },
   {
    "duration": 14,
    "start_time": "2024-06-24T21:09:54.563Z"
   },
   {
    "duration": 606,
    "start_time": "2024-06-24T21:10:32.115Z"
   },
   {
    "duration": 607,
    "start_time": "2024-06-24T21:10:41.373Z"
   },
   {
    "duration": 6,
    "start_time": "2024-06-24T21:13:21.635Z"
   },
   {
    "duration": 126,
    "start_time": "2024-06-24T21:13:40.387Z"
   },
   {
    "duration": 12,
    "start_time": "2024-06-24T21:13:56.658Z"
   },
   {
    "duration": 35,
    "start_time": "2024-06-24T21:14:34.873Z"
   },
   {
    "duration": 141,
    "start_time": "2024-06-24T21:18:25.028Z"
   },
   {
    "duration": 154,
    "start_time": "2024-06-24T21:18:46.186Z"
   },
   {
    "duration": 254,
    "start_time": "2024-06-24T21:18:55.281Z"
   },
   {
    "duration": 152,
    "start_time": "2024-06-24T21:19:51.896Z"
   },
   {
    "duration": 145,
    "start_time": "2024-06-24T21:20:10.785Z"
   },
   {
    "duration": 34,
    "start_time": "2024-06-24T21:26:13.884Z"
   },
   {
    "duration": 31,
    "start_time": "2024-06-24T21:28:50.582Z"
   },
   {
    "duration": 10,
    "start_time": "2024-06-24T21:32:39.879Z"
   },
   {
    "duration": 8,
    "start_time": "2024-06-24T21:32:54.370Z"
   },
   {
    "duration": 8,
    "start_time": "2024-06-24T21:33:11.367Z"
   },
   {
    "duration": 7,
    "start_time": "2024-06-24T21:34:32.250Z"
   },
   {
    "duration": 1556,
    "start_time": "2024-06-24T21:35:11.126Z"
   },
   {
    "duration": 1182,
    "start_time": "2024-06-24T21:35:12.896Z"
   },
   {
    "duration": 550,
    "start_time": "2024-06-24T21:35:14.080Z"
   },
   {
    "duration": 564,
    "start_time": "2024-06-24T21:35:14.632Z"
   },
   {
    "duration": 28,
    "start_time": "2024-06-24T21:35:15.198Z"
   },
   {
    "duration": 57,
    "start_time": "2024-06-24T21:35:15.365Z"
   },
   {
    "duration": 91,
    "start_time": "2024-06-24T21:35:16.477Z"
   },
   {
    "duration": 102,
    "start_time": "2024-06-24T21:35:16.983Z"
   },
   {
    "duration": 194,
    "start_time": "2024-06-24T21:35:17.644Z"
   },
   {
    "duration": 610,
    "start_time": "2024-06-24T21:35:18.398Z"
   },
   {
    "duration": 37,
    "start_time": "2024-06-24T21:35:19.010Z"
   },
   {
    "duration": 154,
    "start_time": "2024-06-24T21:35:19.171Z"
   },
   {
    "duration": 27,
    "start_time": "2024-06-24T21:35:20.198Z"
   },
   {
    "duration": 30,
    "start_time": "2024-06-24T21:35:20.639Z"
   },
   {
    "duration": 29,
    "start_time": "2024-06-24T21:35:22.615Z"
   },
   {
    "duration": 118,
    "start_time": "2024-06-24T21:37:27.224Z"
   },
   {
    "duration": 1415,
    "start_time": "2024-06-24T21:37:42.807Z"
   },
   {
    "duration": 1108,
    "start_time": "2024-06-24T21:37:44.365Z"
   },
   {
    "duration": 565,
    "start_time": "2024-06-24T21:37:45.475Z"
   },
   {
    "duration": 604,
    "start_time": "2024-06-24T21:37:46.042Z"
   },
   {
    "duration": 29,
    "start_time": "2024-06-24T21:37:46.648Z"
   },
   {
    "duration": 62,
    "start_time": "2024-06-24T21:37:46.916Z"
   },
   {
    "duration": 99,
    "start_time": "2024-06-24T21:37:47.935Z"
   },
   {
    "duration": 90,
    "start_time": "2024-06-24T21:37:48.542Z"
   },
   {
    "duration": 194,
    "start_time": "2024-06-24T21:37:49.077Z"
   },
   {
    "duration": 622,
    "start_time": "2024-06-24T21:37:50.627Z"
   },
   {
    "duration": 42,
    "start_time": "2024-06-24T21:37:51.250Z"
   },
   {
    "duration": 157,
    "start_time": "2024-06-24T21:37:51.613Z"
   },
   {
    "duration": 26,
    "start_time": "2024-06-24T21:37:53.793Z"
   },
   {
    "duration": 29,
    "start_time": "2024-06-24T21:37:54.319Z"
   },
   {
    "duration": 57,
    "start_time": "2024-06-24T21:37:56.462Z"
   },
   {
    "duration": 33,
    "start_time": "2024-06-24T21:38:58.790Z"
   },
   {
    "duration": 130,
    "start_time": "2024-06-24T21:44:27.879Z"
   },
   {
    "duration": 58,
    "start_time": "2024-06-24T21:44:44.592Z"
   },
   {
    "duration": 25,
    "start_time": "2024-06-24T21:46:44.344Z"
   },
   {
    "duration": 14,
    "start_time": "2024-06-24T21:48:29.976Z"
   },
   {
    "duration": 39,
    "start_time": "2024-06-24T21:53:26.985Z"
   },
   {
    "duration": 131,
    "start_time": "2024-06-24T21:56:15.146Z"
   },
   {
    "duration": 140,
    "start_time": "2024-06-24T21:57:45.395Z"
   },
   {
    "duration": 112,
    "start_time": "2024-06-24T22:02:33.578Z"
   },
   {
    "duration": 1505,
    "start_time": "2024-06-24T22:03:40.627Z"
   },
   {
    "duration": 1173,
    "start_time": "2024-06-24T22:03:45.374Z"
   },
   {
    "duration": 530,
    "start_time": "2024-06-24T22:03:48.162Z"
   },
   {
    "duration": 570,
    "start_time": "2024-06-24T22:03:48.694Z"
   },
   {
    "duration": 29,
    "start_time": "2024-06-24T22:03:49.490Z"
   },
   {
    "duration": 59,
    "start_time": "2024-06-24T22:03:50.419Z"
   },
   {
    "duration": 96,
    "start_time": "2024-06-24T22:03:51.641Z"
   },
   {
    "duration": 94,
    "start_time": "2024-06-24T22:03:52.337Z"
   },
   {
    "duration": 197,
    "start_time": "2024-06-24T22:03:52.986Z"
   },
   {
    "duration": 598,
    "start_time": "2024-06-24T22:03:54.321Z"
   },
   {
    "duration": 40,
    "start_time": "2024-06-24T22:03:55.234Z"
   },
   {
    "duration": 169,
    "start_time": "2024-06-24T22:03:55.906Z"
   },
   {
    "duration": 36,
    "start_time": "2024-06-24T22:03:57.017Z"
   },
   {
    "duration": 29,
    "start_time": "2024-06-24T22:03:57.545Z"
   },
   {
    "duration": 215,
    "start_time": "2024-06-24T22:03:58.127Z"
   },
   {
    "duration": 26,
    "start_time": "2024-06-24T22:04:03.731Z"
   },
   {
    "duration": 33,
    "start_time": "2024-06-24T22:04:04.562Z"
   },
   {
    "duration": 146,
    "start_time": "2024-06-24T22:04:05.251Z"
   },
   {
    "duration": 32,
    "start_time": "2024-06-24T22:04:06.674Z"
   },
   {
    "duration": 15,
    "start_time": "2024-06-24T22:04:07.833Z"
   },
   {
    "duration": 103,
    "start_time": "2024-06-24T22:04:09.207Z"
   },
   {
    "duration": 152,
    "start_time": "2024-06-24T22:08:46.182Z"
   },
   {
    "duration": 174,
    "start_time": "2024-06-24T22:10:13.045Z"
   },
   {
    "duration": 321,
    "start_time": "2024-06-24T22:10:21.684Z"
   },
   {
    "duration": 1513,
    "start_time": "2024-06-24T22:10:48.988Z"
   },
   {
    "duration": 1097,
    "start_time": "2024-06-24T22:10:51.266Z"
   },
   {
    "duration": 520,
    "start_time": "2024-06-24T22:10:52.457Z"
   },
   {
    "duration": 540,
    "start_time": "2024-06-24T22:10:52.979Z"
   },
   {
    "duration": 27,
    "start_time": "2024-06-24T22:10:53.714Z"
   },
   {
    "duration": 60,
    "start_time": "2024-06-24T22:10:54.915Z"
   },
   {
    "duration": 112,
    "start_time": "2024-06-24T22:10:56.099Z"
   },
   {
    "duration": 87,
    "start_time": "2024-06-24T22:10:56.818Z"
   },
   {
    "duration": 194,
    "start_time": "2024-06-24T22:10:57.506Z"
   },
   {
    "duration": 573,
    "start_time": "2024-06-24T22:10:58.489Z"
   },
   {
    "duration": 36,
    "start_time": "2024-06-24T22:10:59.064Z"
   },
   {
    "duration": 182,
    "start_time": "2024-06-24T22:10:59.101Z"
   },
   {
    "duration": 38,
    "start_time": "2024-06-24T22:10:59.498Z"
   },
   {
    "duration": 40,
    "start_time": "2024-06-24T22:10:59.762Z"
   },
   {
    "duration": 214,
    "start_time": "2024-06-24T22:11:00.011Z"
   },
   {
    "duration": 27,
    "start_time": "2024-06-24T22:11:00.578Z"
   },
   {
    "duration": 34,
    "start_time": "2024-06-24T22:11:00.859Z"
   },
   {
    "duration": 146,
    "start_time": "2024-06-24T22:11:01.373Z"
   },
   {
    "duration": 25,
    "start_time": "2024-06-24T22:11:01.712Z"
   },
   {
    "duration": 15,
    "start_time": "2024-06-24T22:11:02.313Z"
   },
   {
    "duration": 126,
    "start_time": "2024-06-24T22:11:03.091Z"
   },
   {
    "duration": 153,
    "start_time": "2024-06-24T22:11:03.900Z"
   },
   {
    "duration": 299,
    "start_time": "2024-06-24T22:11:04.962Z"
   },
   {
    "duration": 279,
    "start_time": "2024-06-24T22:11:50.651Z"
   },
   {
    "duration": 13,
    "start_time": "2024-06-24T22:14:13.330Z"
   },
   {
    "duration": 3,
    "start_time": "2024-06-24T22:15:08.539Z"
   },
   {
    "duration": 126,
    "start_time": "2024-06-24T22:15:09.743Z"
   },
   {
    "duration": 80,
    "start_time": "2024-06-24T22:15:21.216Z"
   },
   {
    "duration": 4,
    "start_time": "2024-06-24T22:17:15.730Z"
   },
   {
    "duration": 5,
    "start_time": "2024-06-24T22:22:54.188Z"
   },
   {
    "duration": 3,
    "start_time": "2024-06-24T22:27:12.355Z"
   },
   {
    "duration": 532,
    "start_time": "2024-06-24T22:29:08.052Z"
   },
   {
    "duration": 29,
    "start_time": "2024-06-24T22:31:42.218Z"
   },
   {
    "duration": 435,
    "start_time": "2024-06-24T22:32:56.395Z"
   },
   {
    "duration": 10,
    "start_time": "2024-06-24T22:37:22.349Z"
   },
   {
    "duration": 4,
    "start_time": "2024-06-24T22:39:09.548Z"
   },
   {
    "duration": 88,
    "start_time": "2024-06-24T22:39:24.874Z"
   },
   {
    "duration": 790,
    "start_time": "2024-06-24T22:40:37.426Z"
   },
   {
    "duration": 1485,
    "start_time": "2024-06-24T22:41:44.797Z"
   },
   {
    "duration": 1180,
    "start_time": "2024-06-24T22:41:50.773Z"
   },
   {
    "duration": 505,
    "start_time": "2024-06-24T22:41:52.062Z"
   },
   {
    "duration": 562,
    "start_time": "2024-06-24T22:41:52.569Z"
   },
   {
    "duration": 30,
    "start_time": "2024-06-24T22:41:53.380Z"
   },
   {
    "duration": 56,
    "start_time": "2024-06-24T22:41:54.444Z"
   },
   {
    "duration": 91,
    "start_time": "2024-06-24T22:41:55.597Z"
   },
   {
    "duration": 106,
    "start_time": "2024-06-24T22:41:56.396Z"
   },
   {
    "duration": 232,
    "start_time": "2024-06-24T22:41:56.963Z"
   },
   {
    "duration": 631,
    "start_time": "2024-06-24T22:41:57.834Z"
   },
   {
    "duration": 40,
    "start_time": "2024-06-24T22:41:58.468Z"
   },
   {
    "duration": 150,
    "start_time": "2024-06-24T22:41:58.726Z"
   },
   {
    "duration": 44,
    "start_time": "2024-06-24T22:41:59.660Z"
   },
   {
    "duration": 35,
    "start_time": "2024-06-24T22:42:00.099Z"
   },
   {
    "duration": 213,
    "start_time": "2024-06-24T22:42:01.236Z"
   },
   {
    "duration": 25,
    "start_time": "2024-06-24T22:42:02.230Z"
   },
   {
    "duration": 32,
    "start_time": "2024-06-24T22:42:02.693Z"
   },
   {
    "duration": 145,
    "start_time": "2024-06-24T22:42:03.140Z"
   },
   {
    "duration": 26,
    "start_time": "2024-06-24T22:42:04.004Z"
   },
   {
    "duration": 19,
    "start_time": "2024-06-24T22:42:04.899Z"
   },
   {
    "duration": 114,
    "start_time": "2024-06-24T22:42:05.677Z"
   },
   {
    "duration": 149,
    "start_time": "2024-06-24T22:42:07.235Z"
   },
   {
    "duration": 259,
    "start_time": "2024-06-24T22:42:07.578Z"
   },
   {
    "duration": 13,
    "start_time": "2024-06-24T22:42:09.636Z"
   },
   {
    "duration": 2,
    "start_time": "2024-06-24T22:42:11.140Z"
   },
   {
    "duration": 80,
    "start_time": "2024-06-24T22:42:11.710Z"
   },
   {
    "duration": 2,
    "start_time": "2024-06-24T22:42:14.061Z"
   },
   {
    "duration": 443,
    "start_time": "2024-06-24T22:42:14.894Z"
   },
   {
    "duration": 92,
    "start_time": "2024-06-24T22:42:19.273Z"
   },
   {
    "duration": 848,
    "start_time": "2024-06-24T22:42:20.885Z"
   },
   {
    "duration": 19,
    "start_time": "2024-06-24T22:46:30.063Z"
   },
   {
    "duration": 1503,
    "start_time": "2024-06-24T22:46:53.931Z"
   },
   {
    "duration": 1107,
    "start_time": "2024-06-24T22:46:57.786Z"
   },
   {
    "duration": 535,
    "start_time": "2024-06-24T22:46:59.099Z"
   },
   {
    "duration": 555,
    "start_time": "2024-06-24T22:46:59.636Z"
   },
   {
    "duration": 28,
    "start_time": "2024-06-24T22:47:00.886Z"
   },
   {
    "duration": 59,
    "start_time": "2024-06-24T22:47:02.250Z"
   },
   {
    "duration": 94,
    "start_time": "2024-06-24T22:47:04.037Z"
   },
   {
    "duration": 84,
    "start_time": "2024-06-24T22:47:04.939Z"
   },
   {
    "duration": 196,
    "start_time": "2024-06-24T22:47:06.042Z"
   },
   {
    "duration": 619,
    "start_time": "2024-06-24T22:47:07.588Z"
   },
   {
    "duration": 37,
    "start_time": "2024-06-24T22:47:08.209Z"
   },
   {
    "duration": 153,
    "start_time": "2024-06-24T22:47:08.628Z"
   },
   {
    "duration": 32,
    "start_time": "2024-06-24T22:47:09.403Z"
   },
   {
    "duration": 29,
    "start_time": "2024-06-24T22:47:09.855Z"
   },
   {
    "duration": 204,
    "start_time": "2024-06-24T22:47:10.308Z"
   },
   {
    "duration": 25,
    "start_time": "2024-06-24T22:47:10.859Z"
   },
   {
    "duration": 32,
    "start_time": "2024-06-24T22:47:11.242Z"
   },
   {
    "duration": 142,
    "start_time": "2024-06-24T22:47:11.736Z"
   },
   {
    "duration": 24,
    "start_time": "2024-06-24T22:47:12.965Z"
   },
   {
    "duration": 15,
    "start_time": "2024-06-24T22:47:13.794Z"
   },
   {
    "duration": 111,
    "start_time": "2024-06-24T22:47:14.586Z"
   },
   {
    "duration": 148,
    "start_time": "2024-06-24T22:47:16.876Z"
   },
   {
    "duration": 259,
    "start_time": "2024-06-24T22:47:17.466Z"
   },
   {
    "duration": 12,
    "start_time": "2024-06-24T22:47:21.243Z"
   },
   {
    "duration": 2,
    "start_time": "2024-06-24T22:47:23.804Z"
   },
   {
    "duration": 82,
    "start_time": "2024-06-24T22:47:24.371Z"
   },
   {
    "duration": 3,
    "start_time": "2024-06-24T22:47:32.085Z"
   },
   {
    "duration": 432,
    "start_time": "2024-06-24T22:47:33.996Z"
   },
   {
    "duration": 50,
    "start_time": "2024-06-25T12:29:26.008Z"
   },
   {
    "duration": 1646,
    "start_time": "2024-06-25T12:29:43.479Z"
   },
   {
    "duration": 3068,
    "start_time": "2024-06-25T12:29:48.156Z"
   },
   {
    "duration": 536,
    "start_time": "2024-06-25T12:29:51.226Z"
   },
   {
    "duration": 535,
    "start_time": "2024-06-25T12:29:51.764Z"
   },
   {
    "duration": 31,
    "start_time": "2024-06-25T12:29:52.300Z"
   },
   {
    "duration": 54,
    "start_time": "2024-06-25T12:29:52.347Z"
   },
   {
    "duration": 92,
    "start_time": "2024-06-25T12:29:53.858Z"
   },
   {
    "duration": 110,
    "start_time": "2024-06-25T12:29:54.777Z"
   },
   {
    "duration": 217,
    "start_time": "2024-06-25T12:29:55.741Z"
   },
   {
    "duration": 615,
    "start_time": "2024-06-25T12:29:57.315Z"
   },
   {
    "duration": 48,
    "start_time": "2024-06-25T12:29:57.932Z"
   },
   {
    "duration": 180,
    "start_time": "2024-06-25T12:29:58.444Z"
   },
   {
    "duration": 32,
    "start_time": "2024-06-25T12:30:00.462Z"
   },
   {
    "duration": 30,
    "start_time": "2024-06-25T12:30:00.827Z"
   },
   {
    "duration": 981,
    "start_time": "2024-06-25T12:30:01.141Z"
   },
   {
    "duration": 1627,
    "start_time": "2024-06-25T12:30:02.125Z"
   },
   {
    "duration": 640,
    "start_time": "2024-06-25T12:30:03.753Z"
   },
   {
    "duration": 398,
    "start_time": "2024-06-25T12:30:04.395Z"
   },
   {
    "duration": 24,
    "start_time": "2024-06-25T12:30:04.795Z"
   },
   {
    "duration": 43,
    "start_time": "2024-06-25T12:30:04.820Z"
   },
   {
    "duration": 267,
    "start_time": "2024-06-25T12:30:04.865Z"
   },
   {
    "duration": 150,
    "start_time": "2024-06-25T12:30:05.211Z"
   },
   {
    "duration": 289,
    "start_time": "2024-06-25T12:30:05.756Z"
   },
   {
    "duration": 12,
    "start_time": "2024-06-25T12:30:08.099Z"
   },
   {
    "duration": 2,
    "start_time": "2024-06-25T12:30:09.591Z"
   },
   {
    "duration": 78,
    "start_time": "2024-06-25T12:30:09.947Z"
   },
   {
    "duration": 17,
    "start_time": "2024-06-25T12:30:12.118Z"
   },
   {
    "duration": 64,
    "start_time": "2024-06-25T12:30:42.212Z"
   },
   {
    "duration": 59,
    "start_time": "2024-06-25T12:30:52.156Z"
   },
   {
    "duration": 3,
    "start_time": "2024-06-25T12:31:42.132Z"
   },
   {
    "duration": 451,
    "start_time": "2024-06-25T12:31:42.686Z"
   },
   {
    "duration": 4,
    "start_time": "2024-06-25T12:31:51.564Z"
   },
   {
    "duration": 18,
    "start_time": "2024-06-25T13:32:15.553Z"
   },
   {
    "duration": 9,
    "start_time": "2024-06-25T13:32:26.784Z"
   },
   {
    "duration": 284,
    "start_time": "2024-06-25T15:59:29.168Z"
   },
   {
    "duration": 475,
    "start_time": "2024-06-25T16:00:04.464Z"
   },
   {
    "duration": 276,
    "start_time": "2024-06-25T16:00:11.458Z"
   },
   {
    "duration": 24,
    "start_time": "2024-06-25T16:00:27.616Z"
   },
   {
    "duration": 48,
    "start_time": "2024-06-26T17:13:33.413Z"
   },
   {
    "duration": 1544,
    "start_time": "2024-06-26T17:13:53.110Z"
   },
   {
    "duration": 3064,
    "start_time": "2024-06-26T17:13:56.931Z"
   },
   {
    "duration": 490,
    "start_time": "2024-06-26T17:13:59.997Z"
   },
   {
    "duration": 503,
    "start_time": "2024-06-26T17:14:00.489Z"
   },
   {
    "duration": 29,
    "start_time": "2024-06-26T17:14:00.994Z"
   },
   {
    "duration": 251,
    "start_time": "2024-06-26T17:14:01.024Z"
   },
   {
    "duration": 94,
    "start_time": "2024-06-26T17:14:01.997Z"
   },
   {
    "duration": 85,
    "start_time": "2024-06-26T17:14:02.472Z"
   },
   {
    "duration": 185,
    "start_time": "2024-06-26T17:14:03.159Z"
   },
   {
    "duration": 576,
    "start_time": "2024-06-26T17:14:04.798Z"
   },
   {
    "duration": 36,
    "start_time": "2024-06-26T17:14:05.376Z"
   },
   {
    "duration": 146,
    "start_time": "2024-06-26T17:14:05.743Z"
   },
   {
    "duration": 32,
    "start_time": "2024-06-26T17:14:06.294Z"
   },
   {
    "duration": 29,
    "start_time": "2024-06-26T17:14:06.621Z"
   },
   {
    "duration": 194,
    "start_time": "2024-06-26T17:14:06.894Z"
   },
   {
    "duration": 25,
    "start_time": "2024-06-26T17:14:07.684Z"
   },
   {
    "duration": 32,
    "start_time": "2024-06-26T17:14:07.980Z"
   },
   {
    "duration": 139,
    "start_time": "2024-06-26T17:14:08.351Z"
   },
   {
    "duration": 23,
    "start_time": "2024-06-26T17:14:09.054Z"
   },
   {
    "duration": 14,
    "start_time": "2024-06-26T17:14:10.033Z"
   },
   {
    "duration": 117,
    "start_time": "2024-06-26T17:14:11.039Z"
   },
   {
    "duration": 145,
    "start_time": "2024-06-26T17:14:12.478Z"
   },
   {
    "duration": 250,
    "start_time": "2024-06-26T17:14:12.910Z"
   },
   {
    "duration": 12,
    "start_time": "2024-06-26T17:14:15.605Z"
   },
   {
    "duration": 3,
    "start_time": "2024-06-26T17:14:17.650Z"
   },
   {
    "duration": 77,
    "start_time": "2024-06-26T17:14:18.111Z"
   },
   {
    "duration": 16,
    "start_time": "2024-06-26T17:14:19.823Z"
   },
   {
    "duration": 54,
    "start_time": "2024-06-26T17:14:20.502Z"
   },
   {
    "duration": 3,
    "start_time": "2024-06-26T17:14:21.271Z"
   },
   {
    "duration": 70,
    "start_time": "2024-06-26T17:14:23.055Z"
   },
   {
    "duration": 84,
    "start_time": "2024-06-26T17:19:45.239Z"
   },
   {
    "duration": 209,
    "start_time": "2024-06-26T17:20:59.903Z"
   },
   {
    "duration": 373,
    "start_time": "2024-06-26T17:21:25.392Z"
   },
   {
    "duration": 18,
    "start_time": "2024-06-26T17:21:34.969Z"
   },
   {
    "duration": 80,
    "start_time": "2024-06-26T17:21:41.160Z"
   },
   {
    "duration": 81,
    "start_time": "2024-06-26T17:22:06.528Z"
   },
   {
    "duration": 8288,
    "start_time": "2024-06-26T17:22:16.583Z"
   },
   {
    "duration": 4,
    "start_time": "2024-06-26T17:22:53.259Z"
   },
   {
    "duration": 265,
    "start_time": "2024-06-26T17:22:54.807Z"
   },
   {
    "duration": 1181,
    "start_time": "2024-06-26T17:23:06.395Z"
   },
   {
    "duration": 4,
    "start_time": "2024-06-26T17:23:57.167Z"
   },
   {
    "duration": 42160,
    "start_time": "2024-06-26T17:24:27.480Z"
   },
   {
    "duration": 1607,
    "start_time": "2024-06-26T21:15:29.457Z"
   },
   {
    "duration": 1086,
    "start_time": "2024-06-26T21:15:34.452Z"
   },
   {
    "duration": 518,
    "start_time": "2024-06-26T21:15:36.099Z"
   },
   {
    "duration": 555,
    "start_time": "2024-06-26T21:15:36.619Z"
   },
   {
    "duration": 29,
    "start_time": "2024-06-26T21:15:38.100Z"
   },
   {
    "duration": 56,
    "start_time": "2024-06-26T21:15:39.889Z"
   },
   {
    "duration": 94,
    "start_time": "2024-06-26T21:15:41.994Z"
   },
   {
    "duration": 86,
    "start_time": "2024-06-26T21:15:42.737Z"
   },
   {
    "duration": 211,
    "start_time": "2024-06-26T21:15:43.537Z"
   },
   {
    "duration": 568,
    "start_time": "2024-06-26T21:15:45.290Z"
   },
   {
    "duration": 37,
    "start_time": "2024-06-26T21:15:45.859Z"
   },
   {
    "duration": 149,
    "start_time": "2024-06-26T21:15:46.264Z"
   },
   {
    "duration": 32,
    "start_time": "2024-06-26T21:15:47.105Z"
   },
   {
    "duration": 29,
    "start_time": "2024-06-26T21:15:47.617Z"
   },
   {
    "duration": 199,
    "start_time": "2024-06-26T21:15:48.306Z"
   },
   {
    "duration": 25,
    "start_time": "2024-06-26T21:15:49.552Z"
   },
   {
    "duration": 31,
    "start_time": "2024-06-26T21:15:50.082Z"
   },
   {
    "duration": 134,
    "start_time": "2024-06-26T21:15:50.492Z"
   },
   {
    "duration": 23,
    "start_time": "2024-06-26T21:15:51.441Z"
   },
   {
    "duration": 14,
    "start_time": "2024-06-26T21:15:52.604Z"
   },
   {
    "duration": 132,
    "start_time": "2024-06-26T21:15:53.538Z"
   },
   {
    "duration": 140,
    "start_time": "2024-06-26T21:15:55.443Z"
   },
   {
    "duration": 257,
    "start_time": "2024-06-26T21:15:55.922Z"
   },
   {
    "duration": 13,
    "start_time": "2024-06-26T21:15:58.540Z"
   },
   {
    "duration": 3,
    "start_time": "2024-06-26T21:16:01.320Z"
   },
   {
    "duration": 79,
    "start_time": "2024-06-26T21:16:01.739Z"
   },
   {
    "duration": 378,
    "start_time": "2024-06-26T21:16:03.799Z"
   },
   {
    "duration": 18,
    "start_time": "2024-06-26T21:16:04.473Z"
   },
   {
    "duration": 77,
    "start_time": "2024-06-26T21:16:16.214Z"
   },
   {
    "duration": 8387,
    "start_time": "2024-06-26T21:16:17.507Z"
   },
   {
    "duration": 1087,
    "start_time": "2024-06-26T21:16:25.896Z"
   },
   {
    "duration": 4244,
    "start_time": "2024-06-26T21:16:26.984Z"
   },
   {
    "duration": 897,
    "start_time": "2024-06-26T21:16:31.230Z"
   },
   {
    "duration": 2989,
    "start_time": "2024-06-26T21:16:32.129Z"
   },
   {
    "duration": 5,
    "start_time": "2024-06-26T21:16:35.189Z"
   },
   {
    "duration": 75,
    "start_time": "2024-06-26T21:17:30.184Z"
   },
   {
    "duration": 122,
    "start_time": "2024-06-26T21:19:38.573Z"
   },
   {
    "duration": 1382,
    "start_time": "2024-06-26T21:22:24.514Z"
   },
   {
    "duration": 1061,
    "start_time": "2024-06-26T21:22:29.342Z"
   },
   {
    "duration": 505,
    "start_time": "2024-06-26T21:22:30.936Z"
   },
   {
    "duration": 541,
    "start_time": "2024-06-26T21:22:31.443Z"
   },
   {
    "duration": 29,
    "start_time": "2024-06-26T21:22:32.890Z"
   },
   {
    "duration": 58,
    "start_time": "2024-06-26T21:22:34.738Z"
   },
   {
    "duration": 92,
    "start_time": "2024-06-26T21:22:37.012Z"
   },
   {
    "duration": 85,
    "start_time": "2024-06-26T21:22:37.818Z"
   },
   {
    "duration": 189,
    "start_time": "2024-06-26T21:22:38.631Z"
   },
   {
    "duration": 563,
    "start_time": "2024-06-26T21:22:40.395Z"
   },
   {
    "duration": 36,
    "start_time": "2024-06-26T21:22:40.960Z"
   },
   {
    "duration": 150,
    "start_time": "2024-06-26T21:22:41.284Z"
   },
   {
    "duration": 31,
    "start_time": "2024-06-26T21:22:42.201Z"
   },
   {
    "duration": 28,
    "start_time": "2024-06-26T21:22:42.698Z"
   },
   {
    "duration": 190,
    "start_time": "2024-06-26T21:22:43.163Z"
   },
   {
    "duration": 24,
    "start_time": "2024-06-26T21:22:44.090Z"
   },
   {
    "duration": 32,
    "start_time": "2024-06-26T21:22:44.756Z"
   },
   {
    "duration": 135,
    "start_time": "2024-06-26T21:22:44.992Z"
   },
   {
    "duration": 24,
    "start_time": "2024-06-26T21:22:46.039Z"
   },
   {
    "duration": 13,
    "start_time": "2024-06-26T21:22:47.227Z"
   },
   {
    "duration": 116,
    "start_time": "2024-06-26T21:22:48.154Z"
   },
   {
    "duration": 149,
    "start_time": "2024-06-26T21:22:49.929Z"
   },
   {
    "duration": 249,
    "start_time": "2024-06-26T21:22:50.394Z"
   },
   {
    "duration": 11,
    "start_time": "2024-06-26T21:22:52.739Z"
   },
   {
    "duration": 2,
    "start_time": "2024-06-26T21:22:56.608Z"
   },
   {
    "duration": 79,
    "start_time": "2024-06-26T21:22:57.099Z"
   },
   {
    "duration": 368,
    "start_time": "2024-06-26T21:22:58.641Z"
   },
   {
    "duration": 18,
    "start_time": "2024-06-26T21:22:59.418Z"
   },
   {
    "duration": 73,
    "start_time": "2024-06-26T21:23:03.819Z"
   },
   {
    "duration": 8342,
    "start_time": "2024-06-26T21:23:08.198Z"
   },
   {
    "duration": 1038,
    "start_time": "2024-06-26T21:23:16.542Z"
   },
   {
    "duration": 297381,
    "start_time": "2024-06-26T21:23:18.055Z"
   },
   {
    "duration": 177116,
    "start_time": "2024-06-26T21:28:15.438Z"
   },
   {
    "duration": 3201,
    "start_time": "2024-06-26T21:42:56.134Z"
   },
   {
    "duration": 127,
    "start_time": "2024-06-26T21:44:58.882Z"
   },
   {
    "duration": 3372,
    "start_time": "2024-06-26T21:45:02.509Z"
   },
   {
    "duration": 956,
    "start_time": "2024-06-26T21:45:15.595Z"
   },
   {
    "duration": 5,
    "start_time": "2024-06-26T21:46:37.363Z"
   },
   {
    "duration": 50,
    "start_time": "2024-06-28T14:24:18.968Z"
   },
   {
    "duration": 1614,
    "start_time": "2024-06-28T14:25:55.650Z"
   },
   {
    "duration": 1184,
    "start_time": "2024-06-28T14:26:00.094Z"
   },
   {
    "duration": 69,
    "start_time": "2024-06-28T14:26:40.240Z"
   },
   {
    "duration": 594,
    "start_time": "2024-06-28T14:27:06.163Z"
   },
   {
    "duration": 418,
    "start_time": "2024-06-28T14:27:15.480Z"
   },
   {
    "duration": 463,
    "start_time": "2024-06-28T14:27:16.017Z"
   },
   {
    "duration": 23,
    "start_time": "2024-06-28T14:27:18.022Z"
   },
   {
    "duration": 66,
    "start_time": "2024-06-28T14:27:24.144Z"
   },
   {
    "duration": 72,
    "start_time": "2024-06-28T14:27:27.113Z"
   },
   {
    "duration": 124,
    "start_time": "2024-06-28T14:27:28.224Z"
   },
   {
    "duration": 152,
    "start_time": "2024-06-28T14:27:31.649Z"
   },
   {
    "duration": 547,
    "start_time": "2024-06-28T14:27:37.818Z"
   },
   {
    "duration": 38,
    "start_time": "2024-06-28T14:27:42.117Z"
   },
   {
    "duration": 143,
    "start_time": "2024-06-28T14:27:42.746Z"
   },
   {
    "duration": 37,
    "start_time": "2024-06-28T14:27:43.953Z"
   },
   {
    "duration": 34,
    "start_time": "2024-06-28T14:27:46.648Z"
   },
   {
    "duration": 181,
    "start_time": "2024-06-28T14:27:49.050Z"
   },
   {
    "duration": 33,
    "start_time": "2024-06-28T14:28:19.177Z"
   },
   {
    "duration": 20,
    "start_time": "2024-06-28T14:28:31.567Z"
   },
   {
    "duration": 28,
    "start_time": "2024-06-28T14:28:37.339Z"
   },
   {
    "duration": 27,
    "start_time": "2024-06-28T14:29:24.346Z"
   },
   {
    "duration": 120,
    "start_time": "2024-06-28T14:29:27.475Z"
   },
   {
    "duration": 21,
    "start_time": "2024-06-28T14:29:33.383Z"
   },
   {
    "duration": 25,
    "start_time": "2024-06-28T14:30:47.988Z"
   },
   {
    "duration": 13,
    "start_time": "2024-06-28T14:31:15.362Z"
   },
   {
    "duration": 97,
    "start_time": "2024-06-28T14:31:16.755Z"
   },
   {
    "duration": 122,
    "start_time": "2024-06-28T14:31:36.476Z"
   },
   {
    "duration": 233,
    "start_time": "2024-06-28T14:31:37.260Z"
   },
   {
    "duration": 114,
    "start_time": "2024-06-28T14:48:31.942Z"
   },
   {
    "duration": 8,
    "start_time": "2024-06-28T14:48:44.885Z"
   },
   {
    "duration": 115,
    "start_time": "2024-06-28T14:48:54.846Z"
   },
   {
    "duration": 221,
    "start_time": "2024-06-28T14:48:55.799Z"
   },
   {
    "duration": 11,
    "start_time": "2024-06-28T14:48:59.461Z"
   },
   {
    "duration": 2,
    "start_time": "2024-06-28T14:50:38.558Z"
   },
   {
    "duration": 65,
    "start_time": "2024-06-28T14:50:39.271Z"
   },
   {
    "duration": 2,
    "start_time": "2024-06-28T14:50:44.686Z"
   },
   {
    "duration": 341,
    "start_time": "2024-06-28T14:50:46.214Z"
   },
   {
    "duration": 285,
    "start_time": "2024-06-28T14:50:48.750Z"
   },
   {
    "duration": 35,
    "start_time": "2024-06-28T14:51:14.344Z"
   },
   {
    "duration": 61,
    "start_time": "2024-06-28T14:51:54.844Z"
   },
   {
    "duration": 259,
    "start_time": "2024-06-28T14:52:32.743Z"
   },
   {
    "duration": 42,
    "start_time": "2024-06-28T14:52:49.666Z"
   },
   {
    "duration": 18,
    "start_time": "2024-06-28T14:55:14.008Z"
   },
   {
    "duration": 1796,
    "start_time": "2024-06-28T14:57:42.384Z"
   },
   {
    "duration": 2,
    "start_time": "2024-06-28T15:03:19.139Z"
   },
   {
    "duration": 3,
    "start_time": "2024-06-28T15:04:23.259Z"
   },
   {
    "duration": 316,
    "start_time": "2024-06-28T15:04:25.771Z"
   },
   {
    "duration": 264,
    "start_time": "2024-06-28T15:04:26.364Z"
   },
   {
    "duration": 37,
    "start_time": "2024-06-28T15:04:27.177Z"
   },
   {
    "duration": 1836,
    "start_time": "2024-06-28T15:04:31.705Z"
   },
   {
    "duration": 4,
    "start_time": "2024-06-28T15:09:41.721Z"
   },
   {
    "duration": 23,
    "start_time": "2024-06-28T15:12:08.546Z"
   },
   {
    "duration": 1423,
    "start_time": "2024-06-28T15:16:04.313Z"
   },
   {
    "duration": 923,
    "start_time": "2024-06-28T15:16:07.903Z"
   },
   {
    "duration": 422,
    "start_time": "2024-06-28T15:16:09.066Z"
   },
   {
    "duration": 452,
    "start_time": "2024-06-28T15:16:09.490Z"
   },
   {
    "duration": 43,
    "start_time": "2024-06-28T15:16:10.321Z"
   },
   {
    "duration": 52,
    "start_time": "2024-06-28T15:16:11.688Z"
   },
   {
    "duration": 75,
    "start_time": "2024-06-28T15:16:13.265Z"
   },
   {
    "duration": 68,
    "start_time": "2024-06-28T15:16:13.904Z"
   },
   {
    "duration": 164,
    "start_time": "2024-06-28T15:16:14.634Z"
   },
   {
    "duration": 515,
    "start_time": "2024-06-28T15:16:16.440Z"
   },
   {
    "duration": 36,
    "start_time": "2024-06-28T15:16:16.957Z"
   },
   {
    "duration": 156,
    "start_time": "2024-06-28T15:16:17.240Z"
   },
   {
    "duration": 28,
    "start_time": "2024-06-28T15:16:18.008Z"
   },
   {
    "duration": 32,
    "start_time": "2024-06-28T15:16:19.377Z"
   },
   {
    "duration": 170,
    "start_time": "2024-06-28T15:16:19.672Z"
   },
   {
    "duration": 25,
    "start_time": "2024-06-28T15:16:20.390Z"
   },
   {
    "duration": 41,
    "start_time": "2024-06-28T15:16:22.289Z"
   },
   {
    "duration": 135,
    "start_time": "2024-06-28T15:16:22.733Z"
   },
   {
    "duration": 23,
    "start_time": "2024-06-28T15:16:23.577Z"
   },
   {
    "duration": 27,
    "start_time": "2024-06-28T15:16:26.008Z"
   },
   {
    "duration": 110,
    "start_time": "2024-06-28T15:16:26.738Z"
   },
   {
    "duration": 135,
    "start_time": "2024-06-28T15:16:29.131Z"
   },
   {
    "duration": 256,
    "start_time": "2024-06-28T15:16:29.681Z"
   },
   {
    "duration": 13,
    "start_time": "2024-06-28T15:16:33.131Z"
   },
   {
    "duration": 2,
    "start_time": "2024-06-28T15:16:36.240Z"
   },
   {
    "duration": 71,
    "start_time": "2024-06-28T15:16:36.637Z"
   },
   {
    "duration": 3,
    "start_time": "2024-06-28T15:16:42.305Z"
   },
   {
    "duration": 315,
    "start_time": "2024-06-28T15:16:47.060Z"
   },
   {
    "duration": 269,
    "start_time": "2024-06-28T15:16:48.379Z"
   },
   {
    "duration": 37,
    "start_time": "2024-06-28T15:16:48.898Z"
   },
   {
    "duration": 1809,
    "start_time": "2024-06-28T15:16:50.234Z"
   },
   {
    "duration": 63668,
    "start_time": "2024-06-28T15:16:52.999Z"
   },
   {
    "duration": 110358,
    "start_time": "2024-06-28T15:17:56.668Z"
   },
   {
    "duration": 262042,
    "start_time": "2024-06-28T15:19:47.028Z"
   },
   {
    "duration": 982258,
    "start_time": "2024-06-28T15:24:09.072Z"
   },
   {
    "duration": 993,
    "start_time": "2024-06-28T15:40:31.421Z"
   },
   {
    "duration": 0,
    "start_time": "2024-06-28T15:40:32.416Z"
   },
   {
    "duration": 3053,
    "start_time": "2024-06-28T18:02:31.501Z"
   },
   {
    "duration": 1356,
    "start_time": "2024-06-28T18:06:37.471Z"
   },
   {
    "duration": 869,
    "start_time": "2024-06-28T18:06:45.113Z"
   },
   {
    "duration": 379,
    "start_time": "2024-06-28T18:06:46.501Z"
   },
   {
    "duration": 429,
    "start_time": "2024-06-28T18:06:46.923Z"
   },
   {
    "duration": 24,
    "start_time": "2024-06-28T18:06:48.021Z"
   },
   {
    "duration": 57,
    "start_time": "2024-06-28T18:06:50.092Z"
   },
   {
    "duration": 70,
    "start_time": "2024-06-28T18:06:52.429Z"
   },
   {
    "duration": 80,
    "start_time": "2024-06-28T18:06:52.975Z"
   },
   {
    "duration": 149,
    "start_time": "2024-06-28T18:06:53.772Z"
   },
   {
    "duration": 533,
    "start_time": "2024-06-28T18:06:55.285Z"
   },
   {
    "duration": 36,
    "start_time": "2024-06-28T18:06:55.820Z"
   },
   {
    "duration": 137,
    "start_time": "2024-06-28T18:06:56.126Z"
   },
   {
    "duration": 32,
    "start_time": "2024-06-28T18:06:56.812Z"
   },
   {
    "duration": 25,
    "start_time": "2024-06-28T18:06:58.477Z"
   },
   {
    "duration": 176,
    "start_time": "2024-06-28T18:06:58.916Z"
   },
   {
    "duration": 22,
    "start_time": "2024-06-28T18:07:01.206Z"
   },
   {
    "duration": 27,
    "start_time": "2024-06-28T18:07:03.426Z"
   },
   {
    "duration": 134,
    "start_time": "2024-06-28T18:07:04.040Z"
   },
   {
    "duration": 22,
    "start_time": "2024-06-28T18:07:05.167Z"
   },
   {
    "duration": 12,
    "start_time": "2024-06-28T18:07:07.947Z"
   },
   {
    "duration": 99,
    "start_time": "2024-06-28T18:07:08.869Z"
   },
   {
    "duration": 121,
    "start_time": "2024-06-28T18:07:10.903Z"
   },
   {
    "duration": 248,
    "start_time": "2024-06-28T18:07:11.222Z"
   },
   {
    "duration": 11,
    "start_time": "2024-06-28T18:07:13.197Z"
   },
   {
    "duration": 2,
    "start_time": "2024-06-28T18:07:16.223Z"
   },
   {
    "duration": 62,
    "start_time": "2024-06-28T18:07:16.551Z"
   },
   {
    "duration": 2,
    "start_time": "2024-06-28T18:07:17.670Z"
   },
   {
    "duration": 312,
    "start_time": "2024-06-28T18:07:18.630Z"
   },
   {
    "duration": 274,
    "start_time": "2024-06-28T18:07:19.110Z"
   },
   {
    "duration": 36,
    "start_time": "2024-06-28T18:07:19.592Z"
   },
   {
    "duration": 1783,
    "start_time": "2024-06-28T18:07:20.318Z"
   },
   {
    "duration": 61814,
    "start_time": "2024-06-28T18:07:22.103Z"
   },
   {
    "duration": 66903,
    "start_time": "2024-06-28T18:08:23.920Z"
   },
   {
    "duration": 253012,
    "start_time": "2024-06-28T18:09:30.825Z"
   },
   {
    "duration": 194783,
    "start_time": "2024-06-28T18:13:43.839Z"
   },
   {
    "duration": 28,
    "start_time": "2024-06-28T18:16:58.624Z"
   },
   {
    "duration": 125,
    "start_time": "2024-06-28T18:16:58.654Z"
   },
   {
    "duration": 120,
    "start_time": "2024-06-28T18:16:58.780Z"
   },
   {
    "duration": 0,
    "start_time": "2024-06-28T18:16:58.901Z"
   },
   {
    "duration": 11,
    "start_time": "2024-06-28T19:20:44.854Z"
   },
   {
    "duration": 11,
    "start_time": "2024-06-28T19:23:27.270Z"
   },
   {
    "duration": 5579,
    "start_time": "2024-06-28T19:25:37.854Z"
   },
   {
    "duration": 1363,
    "start_time": "2024-06-28T19:26:32.944Z"
   },
   {
    "duration": 882,
    "start_time": "2024-06-28T19:26:35.517Z"
   },
   {
    "duration": 422,
    "start_time": "2024-06-28T19:26:36.404Z"
   },
   {
    "duration": 444,
    "start_time": "2024-06-28T19:26:36.828Z"
   },
   {
    "duration": 26,
    "start_time": "2024-06-28T19:26:37.450Z"
   },
   {
    "duration": 54,
    "start_time": "2024-06-28T19:26:38.661Z"
   },
   {
    "duration": 72,
    "start_time": "2024-06-28T19:26:40.428Z"
   },
   {
    "duration": 67,
    "start_time": "2024-06-28T19:26:40.965Z"
   },
   {
    "duration": 146,
    "start_time": "2024-06-28T19:26:41.525Z"
   },
   {
    "duration": 509,
    "start_time": "2024-06-28T19:26:42.853Z"
   },
   {
    "duration": 36,
    "start_time": "2024-06-28T19:26:43.363Z"
   },
   {
    "duration": 138,
    "start_time": "2024-06-28T19:26:43.434Z"
   },
   {
    "duration": 31,
    "start_time": "2024-06-28T19:26:43.957Z"
   },
   {
    "duration": 31,
    "start_time": "2024-06-28T19:26:44.925Z"
   },
   {
    "duration": 181,
    "start_time": "2024-06-28T19:26:45.174Z"
   },
   {
    "duration": 23,
    "start_time": "2024-06-28T19:26:45.733Z"
   },
   {
    "duration": 26,
    "start_time": "2024-06-28T19:26:47.262Z"
   },
   {
    "duration": 126,
    "start_time": "2024-06-28T19:26:47.690Z"
   },
   {
    "duration": 21,
    "start_time": "2024-06-28T19:26:48.776Z"
   },
   {
    "duration": 12,
    "start_time": "2024-06-28T19:26:50.717Z"
   },
   {
    "duration": 98,
    "start_time": "2024-06-28T19:26:51.670Z"
   },
   {
    "duration": 125,
    "start_time": "2024-06-28T19:26:54.365Z"
   },
   {
    "duration": 240,
    "start_time": "2024-06-28T19:26:55.510Z"
   },
   {
    "duration": 12,
    "start_time": "2024-06-28T19:27:03.124Z"
   },
   {
    "duration": 2,
    "start_time": "2024-06-28T19:27:06.198Z"
   },
   {
    "duration": 63,
    "start_time": "2024-06-28T19:27:06.607Z"
   },
   {
    "duration": 2,
    "start_time": "2024-06-28T19:27:07.902Z"
   },
   {
    "duration": 312,
    "start_time": "2024-06-28T19:27:09.181Z"
   },
   {
    "duration": 266,
    "start_time": "2024-06-28T19:27:09.758Z"
   },
   {
    "duration": 37,
    "start_time": "2024-06-28T19:27:10.422Z"
   },
   {
    "duration": 1792,
    "start_time": "2024-06-28T19:27:11.258Z"
   },
   {
    "duration": 61704,
    "start_time": "2024-06-28T19:27:13.052Z"
   },
   {
    "duration": 67970,
    "start_time": "2024-06-28T19:28:14.757Z"
   },
   {
    "duration": 252151,
    "start_time": "2024-06-28T19:29:22.729Z"
   },
   {
    "duration": 198547,
    "start_time": "2024-06-28T19:33:34.882Z"
   },
   {
    "duration": 111,
    "start_time": "2024-06-28T19:36:53.430Z"
   },
   {
    "duration": 98,
    "start_time": "2024-06-28T19:36:53.542Z"
   },
   {
    "duration": 5286,
    "start_time": "2024-06-28T19:36:53.641Z"
   },
   {
    "duration": 8536,
    "start_time": "2024-06-28T19:42:17.089Z"
   },
   {
    "duration": 801,
    "start_time": "2024-06-28T19:42:46.592Z"
   },
   {
    "duration": 5,
    "start_time": "2024-06-28T19:43:30.218Z"
   },
   {
    "duration": 1498,
    "start_time": "2024-07-01T20:51:09.090Z"
   },
   {
    "duration": 1061,
    "start_time": "2024-07-01T20:51:15.615Z"
   },
   {
    "duration": 388,
    "start_time": "2024-07-01T20:51:17.851Z"
   },
   {
    "duration": 383,
    "start_time": "2024-07-01T20:51:18.396Z"
   },
   {
    "duration": 21,
    "start_time": "2024-07-01T20:51:19.868Z"
   },
   {
    "duration": 51,
    "start_time": "2024-07-01T20:51:22.252Z"
   },
   {
    "duration": 65,
    "start_time": "2024-07-01T20:51:25.187Z"
   },
   {
    "duration": 62,
    "start_time": "2024-07-01T20:51:26.334Z"
   },
   {
    "duration": 138,
    "start_time": "2024-07-01T20:51:27.253Z"
   },
   {
    "duration": 479,
    "start_time": "2024-07-01T20:51:29.542Z"
   },
   {
    "duration": 34,
    "start_time": "2024-07-01T20:51:30.023Z"
   },
   {
    "duration": 128,
    "start_time": "2024-07-01T20:51:30.437Z"
   },
   {
    "duration": 23,
    "start_time": "2024-07-01T20:51:31.300Z"
   },
   {
    "duration": 24,
    "start_time": "2024-07-01T20:51:34.084Z"
   },
   {
    "duration": 151,
    "start_time": "2024-07-01T20:51:34.572Z"
   },
   {
    "duration": 21,
    "start_time": "2024-07-01T20:51:35.555Z"
   },
   {
    "duration": 31,
    "start_time": "2024-07-01T20:51:37.748Z"
   },
   {
    "duration": 116,
    "start_time": "2024-07-01T20:51:38.485Z"
   },
   {
    "duration": 23,
    "start_time": "2024-07-01T20:51:39.819Z"
   },
   {
    "duration": 11,
    "start_time": "2024-07-01T20:51:43.607Z"
   },
   {
    "duration": 91,
    "start_time": "2024-07-01T20:51:44.516Z"
   },
   {
    "duration": 119,
    "start_time": "2024-07-01T20:51:47.020Z"
   },
   {
    "duration": 217,
    "start_time": "2024-07-01T20:51:47.429Z"
   },
   {
    "duration": 11,
    "start_time": "2024-07-01T20:51:50.419Z"
   },
   {
    "duration": 2,
    "start_time": "2024-07-01T20:51:57.572Z"
   },
   {
    "duration": 3,
    "start_time": "2024-07-01T20:53:47.817Z"
   },
   {
    "duration": 2,
    "start_time": "2024-07-01T20:57:31.843Z"
   },
   {
    "duration": 54,
    "start_time": "2024-07-01T20:57:32.509Z"
   },
   {
    "duration": 2,
    "start_time": "2024-07-01T20:57:34.328Z"
   },
   {
    "duration": 292,
    "start_time": "2024-07-01T20:57:36.469Z"
   },
   {
    "duration": 6364,
    "start_time": "2024-07-01T20:57:37.109Z"
   },
   {
    "duration": 0,
    "start_time": "2024-07-01T20:57:43.475Z"
   },
   {
    "duration": 1520,
    "start_time": "2024-07-01T20:58:30.807Z"
   },
   {
    "duration": 133,
    "start_time": "2024-07-01T20:58:35.715Z"
   },
   {
    "duration": 1294,
    "start_time": "2024-07-01T21:00:43.536Z"
   },
   {
    "duration": 1063,
    "start_time": "2024-07-01T21:00:47.470Z"
   },
   {
    "duration": 352,
    "start_time": "2024-07-01T21:00:48.611Z"
   },
   {
    "duration": 421,
    "start_time": "2024-07-01T21:00:48.964Z"
   },
   {
    "duration": 23,
    "start_time": "2024-07-01T21:00:49.734Z"
   },
   {
    "duration": 52,
    "start_time": "2024-07-01T21:00:50.951Z"
   },
   {
    "duration": 66,
    "start_time": "2024-07-01T21:00:52.484Z"
   },
   {
    "duration": 63,
    "start_time": "2024-07-01T21:00:53.124Z"
   },
   {
    "duration": 136,
    "start_time": "2024-07-01T21:00:53.781Z"
   },
   {
    "duration": 475,
    "start_time": "2024-07-01T21:00:55.357Z"
   },
   {
    "duration": 39,
    "start_time": "2024-07-01T21:00:55.833Z"
   },
   {
    "duration": 133,
    "start_time": "2024-07-01T21:00:55.922Z"
   },
   {
    "duration": 23,
    "start_time": "2024-07-01T21:00:56.551Z"
   },
   {
    "duration": 23,
    "start_time": "2024-07-01T21:00:58.510Z"
   },
   {
    "duration": 159,
    "start_time": "2024-07-01T21:00:58.963Z"
   },
   {
    "duration": 22,
    "start_time": "2024-07-01T21:00:59.749Z"
   },
   {
    "duration": 25,
    "start_time": "2024-07-01T21:01:01.639Z"
   },
   {
    "duration": 116,
    "start_time": "2024-07-01T21:01:02.045Z"
   },
   {
    "duration": 22,
    "start_time": "2024-07-01T21:01:02.822Z"
   },
   {
    "duration": 11,
    "start_time": "2024-07-01T21:01:05.257Z"
   },
   {
    "duration": 89,
    "start_time": "2024-07-01T21:01:06.075Z"
   },
   {
    "duration": 118,
    "start_time": "2024-07-01T21:01:08.135Z"
   },
   {
    "duration": 235,
    "start_time": "2024-07-01T21:01:08.550Z"
   },
   {
    "duration": 10,
    "start_time": "2024-07-01T21:01:11.211Z"
   },
   {
    "duration": 2,
    "start_time": "2024-07-01T21:01:16.028Z"
   },
   {
    "duration": 53,
    "start_time": "2024-07-01T21:01:16.505Z"
   },
   {
    "duration": 2,
    "start_time": "2024-07-01T21:01:18.133Z"
   },
   {
    "duration": 323,
    "start_time": "2024-07-01T21:01:20.829Z"
   },
   {
    "duration": 1522,
    "start_time": "2024-07-01T21:01:21.341Z"
   },
   {
    "duration": 132,
    "start_time": "2024-07-01T21:01:22.864Z"
   },
   {
    "duration": 51,
    "start_time": "2024-07-01T21:01:42.070Z"
   },
   {
    "duration": 1243,
    "start_time": "2024-07-01T21:01:58.086Z"
   },
   {
    "duration": 1078,
    "start_time": "2024-07-01T21:02:02.214Z"
   },
   {
    "duration": 347,
    "start_time": "2024-07-01T21:02:03.295Z"
   },
   {
    "duration": 391,
    "start_time": "2024-07-01T21:02:03.644Z"
   },
   {
    "duration": 22,
    "start_time": "2024-07-01T21:02:04.542Z"
   },
   {
    "duration": 49,
    "start_time": "2024-07-01T21:02:06.262Z"
   },
   {
    "duration": 65,
    "start_time": "2024-07-01T21:02:07.812Z"
   },
   {
    "duration": 61,
    "start_time": "2024-07-01T21:02:08.263Z"
   },
   {
    "duration": 143,
    "start_time": "2024-07-01T21:02:08.754Z"
   },
   {
    "duration": 474,
    "start_time": "2024-07-01T21:02:09.959Z"
   },
   {
    "duration": 37,
    "start_time": "2024-07-01T21:02:10.435Z"
   },
   {
    "duration": 135,
    "start_time": "2024-07-01T21:02:10.474Z"
   },
   {
    "duration": 29,
    "start_time": "2024-07-01T21:02:10.998Z"
   },
   {
    "duration": 23,
    "start_time": "2024-07-01T21:02:12.390Z"
   },
   {
    "duration": 151,
    "start_time": "2024-07-01T21:02:12.741Z"
   },
   {
    "duration": 20,
    "start_time": "2024-07-01T21:02:13.183Z"
   },
   {
    "duration": 25,
    "start_time": "2024-07-01T21:02:14.599Z"
   },
   {
    "duration": 114,
    "start_time": "2024-07-01T21:02:14.928Z"
   },
   {
    "duration": 20,
    "start_time": "2024-07-01T21:02:15.743Z"
   },
   {
    "duration": 16,
    "start_time": "2024-07-01T21:02:18.231Z"
   },
   {
    "duration": 93,
    "start_time": "2024-07-01T21:02:19.016Z"
   },
   {
    "duration": 114,
    "start_time": "2024-07-01T21:02:20.800Z"
   },
   {
    "duration": 231,
    "start_time": "2024-07-01T21:02:21.166Z"
   },
   {
    "duration": 11,
    "start_time": "2024-07-01T21:02:23.829Z"
   },
   {
    "duration": 2,
    "start_time": "2024-07-01T21:02:27.900Z"
   },
   {
    "duration": 53,
    "start_time": "2024-07-01T21:02:28.204Z"
   },
   {
    "duration": 3,
    "start_time": "2024-07-01T21:02:30.078Z"
   },
   {
    "duration": 295,
    "start_time": "2024-07-01T21:02:31.254Z"
   },
   {
    "duration": 1652,
    "start_time": "2024-07-01T21:02:31.551Z"
   },
   {
    "duration": 132,
    "start_time": "2024-07-01T21:02:33.204Z"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
